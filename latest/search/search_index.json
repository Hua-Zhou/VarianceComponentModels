{
    "docs": [
        {
            "location": "/", 
            "text": "VarianceComponentModels.jl\n\n\nUtilities for fitting and testing variance component models\n\n\nVarianceComponentModels.jl implements computation routines for fitting and testing variance component model of form\n\n\n\n\n\n\\text{vec}(Y) \\sim \\text{Nomral}(X B, \\Sigma_1 \\otimes V_1 + \\cdots + \\Sigma_m \\otimes V_m),\n\n\n\n\n\nwhere $\\otimes$ is the \nKronecker product\n.\n\n\nIn this model, \ndata\n is represented by  \n\n\n\n\nY\n: \nn x d\n response matrix\n\n\nX\n: \nn x p\n covariate matrix\n\n\nV=(V1,...,Vm)\n: a tuple \nm\n \nn x n\n covariance matrices\n\n\n\n\nand \nparameters\n are  \n\n\n\n\nB\n: \np x d\n mean parameter matrix\n\n\n\u03a3=(\u03a31,...,\u03a3m)\n: a tuple of \nm\n \nd x d\n variance components\n\n\n\n\n\n\nPackage Features\n\n\n\n\nMaximum likelihood estimation (MLE) and restricted maximum likelihood estimation (REML) of mean parameters \nB\n and variance component parameters \n\u03a3\n\n\nAllow constrains in the mean parameters \nB\n\n\nChoice of optimization algorithms: \nFisher scoring\n and \nminorization-maximization algorithm\n\n\nHeritability Analysis\n in genetics\n\n\n\n\n\n\nInstallation\n\n\nUse the Julia package manager to install VarianceComponentModels.jl.\n\n\nPkg\n.\nclone\n(\nhttps://github.com/OpenMendel/VarianceComponentModels.jl.git\n)\n\n\n\n\n\n\nThis package supports Julia \n0.6\n.\n\n\n\n\nManual Outline\n\n\n\n\nMLE and REML\n\n\nDemo data\n\n\nMaximum likelihood estimation (MLE)\n\n\nRestricted maximum likelihood estimation (REML)\n\n\nOptimization algorithms\n\n\nStarting point\n\n\nConstrained estimation of \nB\n\n\n\n\n\n\nHeritability Analysis\n\n\nRead in binary SNP data\n\n\nSummary statistics of SNP data\n\n\nEmpirical kinship matrix\n\n\nPhenotypes\n\n\nPre-processing data for heritability analysis\n\n\nSave intermediate results\n\n\nHeritability of single traits\n\n\nPairwise traits\n\n\n3-trait analysis\n\n\n13-trait joint analysis\n\n\nSave analysis results", 
            "title": "Home"
        }, 
        {
            "location": "/#variancecomponentmodelsjl", 
            "text": "Utilities for fitting and testing variance component models  VarianceComponentModels.jl implements computation routines for fitting and testing variance component model of form   \n\\text{vec}(Y) \\sim \\text{Nomral}(X B, \\Sigma_1 \\otimes V_1 + \\cdots + \\Sigma_m \\otimes V_m),   where $\\otimes$ is the  Kronecker product .  In this model,  data  is represented by     Y :  n x d  response matrix  X :  n x p  covariate matrix  V=(V1,...,Vm) : a tuple  m   n x n  covariance matrices   and  parameters  are     B :  p x d  mean parameter matrix  \u03a3=(\u03a31,...,\u03a3m) : a tuple of  m   d x d  variance components", 
            "title": "VarianceComponentModels.jl"
        }, 
        {
            "location": "/#package-features", 
            "text": "Maximum likelihood estimation (MLE) and restricted maximum likelihood estimation (REML) of mean parameters  B  and variance component parameters  \u03a3  Allow constrains in the mean parameters  B  Choice of optimization algorithms:  Fisher scoring  and  minorization-maximization algorithm  Heritability Analysis  in genetics", 
            "title": "Package Features"
        }, 
        {
            "location": "/#installation", 
            "text": "Use the Julia package manager to install VarianceComponentModels.jl.  Pkg . clone ( https://github.com/OpenMendel/VarianceComponentModels.jl.git )   This package supports Julia  0.6 .", 
            "title": "Installation"
        }, 
        {
            "location": "/#manual-outline", 
            "text": "MLE and REML  Demo data  Maximum likelihood estimation (MLE)  Restricted maximum likelihood estimation (REML)  Optimization algorithms  Starting point  Constrained estimation of  B    Heritability Analysis  Read in binary SNP data  Summary statistics of SNP data  Empirical kinship matrix  Phenotypes  Pre-processing data for heritability analysis  Save intermediate results  Heritability of single traits  Pairwise traits  3-trait analysis  13-trait joint analysis  Save analysis results", 
            "title": "Manual Outline"
        }, 
        {
            "location": "/man/mle_reml/", 
            "text": "MLE and REML\n\n\nMachine information\n\n\nversioninfo\n()\n\n\n\n\n\n\nJulia Version 0.6.0\nCommit 903644385b (2017-06-19 13:05 UTC)\nPlatform Info:\n  OS: macOS (x86_64-apple-darwin13.4.0)\n  CPU: Intel(R) Core(TM) i7-4790K CPU @ 4.00GHz\n  WORD_SIZE: 64\n  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Haswell)\n  LAPACK: libopenblas64_\n  LIBM: libopenlibm\n  LLVM: libLLVM-3.9.1 (ORCJIT, haswell)\n\n\n\n\n\n\n\nDemo data\n\n\nFor demonstration, we generate a random data set.\n\n\n# generate data from a d-variate response variane component model\n\n\nsrand\n(\n123\n)\n\n\nn\n \n=\n \n1000\n   \n# no. observations\n\n\nd\n \n=\n \n2\n      \n# dimension of responses\n\n\nm\n \n=\n \n2\n      \n# no. variance components\n\n\np\n \n=\n \n2\n      \n# no. covariates\n\n\n# n-by-p design matrix\n\n\nX\n \n=\n \nrandn\n(\nn\n,\n \np\n)\n\n\n# p-by-d mean component regression coefficient\n\n\nB\n \n=\n \nones\n(\np\n,\n \nd\n)\n  \n\n# a tuple of m covariance matrices\n\n\nV\n \n=\n \nntuple\n(\nx\n \n-\n \nzeros\n(\nn\n,\n \nn\n),\n \nm\n)\n \n\nfor\n \ni\n \n=\n \n1\n:\nm\n-\n1\n\n  \nVi\n \n=\n \nrandn\n(\nn\n,\n \n50\n)\n\n  \ncopy!\n(\nV\n[\ni\n],\n \nVi\n \n*\n \nVi\n)\n\n\nend\n\n\ncopy!\n(\nV\n[\nm\n],\n \neye\n(\nn\n))\n \n# last covarianec matrix is idendity\n\n\n# a tuple of m d-by-d variance component parameters\n\n\n\u03a3\n \n=\n \nntuple\n(\nx\n \n-\n \nzeros\n(\nd\n,\n \nd\n),\n \nm\n)\n \n\nfor\n \ni\n \nin\n \n1\n:\nm\n\n  \n\u03a3i\n \n=\n \nrandn\n(\nd\n,\n \nd\n)\n\n  \ncopy!\n(\n\u03a3\n[\ni\n],\n \n\u03a3i\n \n*\n \n\u03a3i\n)\n\n\nend\n\n\n# form overall nd-by-nd covariance matrix \u03a9\n\n\n\u03a9\n \n=\n \nzeros\n(\nn\n \n*\n \nd\n,\n \nn\n \n*\n \nd\n)\n\n\nfor\n \ni\n \n=\n \n1\n:\nm\n\n  \n\u03a9\n \n+=\n \nkron\n(\n\u03a3\n[\ni\n],\n \nV\n[\ni\n])\n\n\nend\n\n\n\u03a9chol\n \n=\n \ncholfact\n(\n\u03a9\n)\n\n\n# n-by-d responses\n\n\nY\n \n=\n \nX\n \n*\n \nB\n \n+\n \nreshape\n(\n\u03a9chol\n[\n:\nL\n]\n \n*\n \nrandn\n(\nn\n*\nd\n),\n \nn\n,\n \nd\n);\n\n\n\n\n\n\n\n\nMaximum likelihood estimation (MLE)\n\n\nTo find the MLE of parameters $(B,\\Sigma_1,\\ldots,\\Sigma_m)$, we take 3 steps:  \n\n\nStep 1 (Construct data)\n. Construct an instance of \nVarianceComponentVariate\n, which consists fields  \n\n\n\n\nY\n: $n$-by-$d$ responses\n\n\nX\n: $n$-by-$p$ covariate matrix\n\n\nV=(V[1],...,V[m])\n: a tuple of $n$-by-$n$ covariance matrices. The last covariance matrix must be positive definite and usually is the identity matrix.\n\n\n\n\nusing\n \nVarianceComponentModels\n\n\nvcdata\n \n=\n \nVarianceComponentVariate\n(\nY\n,\n \nX\n,\n \nV\n)\n\n\nfieldnames\n(\nvcdata\n)\n\n\n\n\n\n\n3-element Array{Symbol,1}:\n :Y\n :X\n :V\n\n\n\n\n\nIn the absence of covariates $X$, we can simply initialize by \nvcdata = VarianceComponentVariate(Y, V)\n.\n\n\nStep 2 (Construct a model)\n. Construct an instance of \nVarianceComponentModel\n, which consists of fields  \n\n\n\n\nB\n: $n$-by-$p$ mean regression coefficients\n\n\n\u03a3=(\u03a3[1],...,\u03a3[m])\n: variane component parameters respectively.\n\n\n\n\nWhen constructed from a \nVarianceComponentVariate\n instance, the mean parameters $B$ are initialized to be zero and the tuple of variance component parameters $\\Sigma$ to be \n(eye(d),...,eye(d))\n.\n\n\nvcmodel\n \n=\n \nVarianceComponentModel\n(\nvcdata\n)\n\n\nfieldnames\n(\nvcmodel\n)\n\n\n\n\n\n\n7-element Array{Symbol,1}:\n :B    \n :\u03a3    \n :A    \n :sense\n :b    \n :lb   \n :ub\n\n\n\n\n\nvcmodel\n\n\n\n\n\n\nVarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}([0.0 0.0; 0.0 0.0], ([1.0 0.0; 0.0 1.0], [1.0 0.0; 0.0 1.0]), Array{Float64}(0,4), Char[], Float64[], -Inf, Inf)\n\n\n\n\n\nThe remaining fields \nA\n, \nsense\n, \nb\n, \nlb\n, \nub\n specify (optional) constraints on the mean parameters \nB\n:\n\n\n\n\n\nA * \\text{vec}(B) \\,\\, =(\\text{or } \\ge \\text{or } \\le) \\,\\, b\n\n\n\n\n\n\n\n\nlb \\le \\text{vec}(B) \\le ub\n\n\n\n\n\nA\n is an constraint matrix with $pd$ columns, \nsense\n is a vector of charaters taking values \n'\n'\n, \n'='\n or \n'\n'\n, and \nlb\n and \nub\n are the lower and upper bounds for \nvec(B)\n. By default, \nA\n, \nsense\n, \nb\n are empty, \nlb\n is \n-Inf\n, and \nub\n is \nInf\n. If any constraits are non-trivial, final estimates of \nB\n are enforced to satisfy them.\n\n\nWhen a better initial guess is available, we can initialize by calling \nvcmodel=VarianceComponentModel(B0, \u03a30)\n directly.\n\n\nStep 3 (Fit model)\n. Call optmization routine \nfit_mle!\n. The keywork \nalgo\n dictates the optimization algorithm: \n:MM\n (minorization-maximization algorithm) or \n:FS\n (Fisher scoring algorithm).\n\n\nvcmodel_mle\n \n=\n \ndeepcopy\n(\nvcmodel\n)\n\n\n@time\n \nlogl\n,\n \nvcmodel_mle\n,\n \n\u03a3se\n,\n \n\u03a3cov\n,\n \nBse\n,\n \nBcov\n \n=\n \nfit_mle!\n(\nvcmodel_mle\n,\n \nvcdata\n;\n \nalgo\n \n=\n \n:\nMM\n);\n\n\n\n\n\n\n     MM Algorithm\n  Iter      Objective  \n--------  -------------\n       0  -6.253551e+03\n       1  -3.881454e+03\n       2  -3.853179e+03\n       3  -3.846525e+03\n       4  -3.844906e+03\n       5  -3.844506e+03\n       6  -3.844406e+03\n       7  -3.844381e+03\n       8  -3.844375e+03\n       9  -3.844374e+03\n      10  -3.844373e+03\n\n  0.290970 seconds (10.45 k allocations: 24.036 MiB, 4.73% gc time)\n\n\n\n\n\nThe output of \nfit_mle!\n contains  \n\n\n\n\nfinal log-likelihood\n\n\n\n\nlogl\n\n\n\n\n\n\n-3844.3731814180805\n\n\n\n\n\n\n\nfitted model\n\n\n\n\nfieldnames\n(\nvcmodel_mle\n)\n\n\n\n\n\n\n7-element Array{Symbol,1}:\n :B    \n :\u03a3    \n :A    \n :sense\n :b    \n :lb   \n :ub\n\n\n\n\n\nvcmodel_mle\n\n\n\n\n\n\nVarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}([1.092 1.04727; 0.955346 1.01632], ([0.380637 -0.305465; -0.305465 4.51938], [1.84009 0.265569; 0.265569 2.17275]), Array{Float64}(0,4), Char[], Float64[], -Inf, Inf)\n\n\n\n\n\n\n\nstandard errors of the estimated varianec component parameters\n\n\n\n\n\u03a3se\n\n\n\n\n\n\n([0.0765136 0.263047; 0.263047 0.904332], [0.0844292 0.0917441; 0.0917441 0.0996927])\n\n\n\n\n\n\n\ncovariance matrix of the variance component parameters estimates\n\n\n\n\n\u03a3cov\n\n\n\n\n\n\n8\u00d78 Array{Float64,2}:\n  0.00585433  -0.00467019  -0.00467019  \u2026  -1.07903e-6   -1.557e-7   \n -0.00467019   0.0691937    0.00372555     -1.557e-7     -1.27444e-6 \n -0.00467019   0.00372555   0.0691937      -8.83212e-6   -1.27444e-6 \n  0.00372555  -0.055198    -0.055198       -1.27444e-6   -1.04316e-5 \n -7.4779e-6   -1.07903e-6  -1.07903e-6      0.00102878    0.000148477\n -1.07903e-6  -8.83212e-6  -1.557e-7    \u2026   0.000148477   0.00121477 \n -1.07903e-6  -1.557e-7    -8.83212e-6      0.00841698    0.00121477 \n -1.557e-7    -1.27444e-6  -1.27444e-6      0.00121477    0.00993864\n\n\n\n\n\n\n\nstandard errors of the estimated mean parameters\n\n\n\n\nBse\n\n\n\n\n\n\n2\u00d72 Array{Float64,2}:\n 0.042559   0.0487086\n 0.0430588  0.049178\n\n\n\n\n\n\n\ncovariance matrix of the mean parameter estimates\n\n\n\n\nBcov\n\n\n\n\n\n\n4\u00d74 Array{Float64,2}:\n  0.00181127   -1.98035e-5    0.000240705  -2.59506e-6 \n -1.98035e-5    0.00185406   -2.59506e-6    0.000247285\n  0.000240705  -2.59506e-6    0.00237252   -2.63542e-5 \n -2.59506e-6    0.000247285  -2.63542e-5    0.00241848\n\n\n\n\n\n\n\nRestricted maximum likelihood estimation (REML)\n\n\nREML (restricted maximum likelihood estimation)\n is a popular alternative to the MLE. To find the REML of a variane component model, we replace the above step 3 by  \n\n\nStep 3\n. Call optmization routine \nfit_reml!\n.\n\n\nvcmodel_reml\n \n=\n \ndeepcopy\n(\nvcmodel\n)\n\n\n@time\n \nlogl\n,\n \nvcmodel_reml\n,\n \n\u03a3se\n,\n \n\u03a3cov\n,\n \nBse\n,\n \nBcov\n \n=\n \nfit_reml!\n(\nvcmodel_reml\n,\n \nvcdata\n;\n \nalgo\n \n=\n \n:\nMM\n);\n\n\n\n\n\n\n     MM Algorithm\n  Iter      Objective  \n--------  -------------\n       0  -4.215053e+03\n       1  -3.925799e+03\n       2  -3.865114e+03\n       3  -3.851105e+03\n       4  -3.847732e+03\n       5  -3.846903e+03\n       6  -3.846698e+03\n       7  -3.846647e+03\n       8  -3.846634e+03\n       9  -3.846631e+03\n      10  -3.846630e+03\n\n  0\n\n\n\n\n\nThe output of \nfit_reml!\n contains\n\n\n\n\nthe final log-likelihood at REML estimate\n\n\n\n\nlogl\n\n\n\n\n\n\n-3844.3777179025096\n\n\n\n\n\n\n\nREML estimates\n\n\n\n\nfieldnames\n(\nvcmodel_reml\n)\n\n\n\n\n\n\n7-element Array{Symbol,1}:\n :B    \n :\u03a3    \n :A    \n :sense\n :b    \n :lb   \n :ub\n\n\n\n\n\nvcmodel_reml\n\n\n\n\n\n\nVarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}([1.092 1.04727; 0.955345 1.01632], ([0.380594 -0.305485; -0.305485 4.51994], [1.84285 0.261963; 0.261963 2.17842]), Array{Float64}(0,4), Char[], Float64[], -Inf, Inf)\n\n\n\n\n\n\n\nstandard errors of the estimated varianec component parameters\n\n\n\n\n\u03a3se\n\n\n\n\n\n\n([0.0765055 0.26305; 0.26305 0.904446], [0.0845559 0.0919325; 0.0919325 0.0999526])\n\n\n\n\n\n\n\ncovariance matrix of the variance component parameters estimates\n\n\n\n\n\u03a3cov\n\n\n\n\n\n\n8\u00d78 Array{Float64,2}:\n  0.0058531   -0.00467005  -0.00467005  \u2026  -1.06597e-6   -1.51499e-7 \n -0.00467005   0.0691951    0.00372613     -1.51499e-7   -1.26041e-6 \n -0.00467005   0.00372613   0.0691951      -8.86843e-6   -1.26041e-6 \n  0.00372613  -0.0552092   -0.0552092      -1.26041e-6   -1.0486e-5  \n -7.50035e-6  -1.06597e-6  -1.06597e-6      0.00101633    0.000144472\n -1.06597e-6  -8.86843e-6  -1.51499e-7  \u2026   0.000144472   0.0012014  \n -1.06597e-6  -1.51499e-7  -8.86843e-6      0.00845158    0.0012014  \n -1.51499e-7  -1.26041e-6  -1.26041e-6      0.0012014     0.00999052\n\n\n\n\n\n\n\nstandard errors of the estimated mean parameters\n\n\n\n\nBse\n\n\n\n\n\n\n2\u00d72 Array{Float64,2}:\n 0.0425909  0.0487744\n 0.043091   0.0492444\n\n\n\n\n\n\n\ncovariance matrix of the mean parameter estimates\n\n\n\n\nBcov\n\n\n\n\n\n\n4\u00d74 Array{Float64,2}:\n  0.00181398   -1.98331e-5    0.000237127  -2.55589e-6 \n -1.98331e-5    0.00185683   -2.55589e-6    0.000243624\n  0.000237127  -2.55589e-6    0.00237894   -2.6426e-5  \n -2.55589e-6    0.000243624  -2.6426e-5     0.00242501\n\n\n\n\n\n\n\nOptimization algorithms\n\n\nFinding the MLE or REML of variance component models is a non-trivial nonlinear optimization problem. The main complications are the non-convexity of objective function and the positive semi-definiteness constraint of variane component parameters $\\Sigma_1,\\ldots,\\Sigma_m$. In specific applications, users should try different algorithms with different starting points in order to find a better solution. Here are some tips for efficient computation. \n\n\nIn general the optimization algorithm needs to invert the $nd$ by $nd$ overall covariance matrix $\\Omega = \\Sigma_1 \\otimes V_1 + \\cdots + \\Sigma_m \\otimes V_m$ in each iteration. Inverting a matrix is an expensive operation with $O(n^3 d^3)$ floating operations. When there are only \ntwo\n varianec components ($m=2$), this tedious task can be avoided by taking one (generalized) eigendecomposion of $(V_1, V_2)$ and rotating data $(Y, X)$ by the eigen-vectors. \n\n\nvcdatarot\n \n=\n \nTwoVarCompVariateRotate\n(\nvcdata\n)\n\n\nfieldnames\n(\nvcdatarot\n)\n\n\n\n\n\n\n5-element Array{Symbol,1}:\n :Yrot    \n :Xrot    \n :eigval  \n :eigvec  \n :logdetV2\n\n\n\n\n\nTwo optimization algorithms are implemented: \nFisher scoring\n (\nmle_fs!\n) and the \nminorization-maximization (MM) algorithm\n (\nmle_mm!\n). Both take the rotated data as input. These two functions give finer control of the optimization algorithms. Generally speaking, MM algorithm is more stable while Fisher scoring (if it converges) yields more accurate answer.\n\n\nvcmodel_mm\n \n=\n \ndeepcopy\n(\nvcmodel\n)\n\n\n@time\n \nmle_mm!\n(\nvcmodel_mm\n,\n \nvcdatarot\n;\n \nmaxiter\n=\n10000\n,\n \nfuntol\n=\n1e-8\n,\n \nverbose\n \n=\n \ntrue\n);\n\n\n\n\n\n\n     MM Algorithm\n  Iter      Objective  \n--------  -------------\n       0  -6.253551e+03\n       1  -3.881454e+03\n       2  -3.853179e+03\n       3  -3.846525e+03\n       4  -3.844906e+03\n       5  -3.844506e+03\n       6  -3.844406e+03\n       7  -3.844381e+03\n       8  -3.844375e+03\n       9  -3.844374e+03\n      10  -3.844373e+03\n\n  0.018754 seconds (9.15 k allocations: 680.172 KiB)\n\n\n\n\n\n# MM estimates\n\n\nvcmodel_mm\n.\nB\n\n\n\n\n\n\n2\u00d72 Array{Float64,2}:\n 1.092     1.04727\n 0.955346  1.01632\n\n\n\n\n\n# MM estimates\n\n\nvcmodel_mm\n.\n\u03a3\n\n\n\n\n\n\n([0.380637 -0.305465; -0.305465 4.51938], [1.84009 0.265569; 0.265569 2.17275])\n\n\n\n\n\nFisher scoring (\nmle_fs!\n) uses either \nIpopt.jl\n (keyword \nsolver=:Ipopt\n) or \nKNITRO.jl\n (keyword \nsolver=:Knitro\n) as the backend solver. Ipopt is open source and installation of \nIpopt.jl\n package alone is sufficient.\n\n\n# Fisher scoring using Ipopt\n\n\nvcmodel_ipopt\n \n=\n \ndeepcopy\n(\nvcmodel\n)\n\n\n@time\n \nmle_fs!\n(\nvcmodel_ipopt\n,\n \nvcdatarot\n;\n \nsolver\n=:\nIpopt\n,\n \nmaxiter\n=\n1000\n,\n \nverbose\n=\ntrue\n);\n\n\n\n\n\n\nThis is Ipopt version 3.12.4, running with linear solver mumps.\nNOTE: Other linear solvers might be more efficient (see Ipopt documentation).\n\nNumber of nonzeros in equality constraint Jacobian...:        0\nNumber of nonzeros in inequality constraint Jacobian.:        0\nNumber of nonzeros in Lagrangian Hessian.............:       21\n\nTotal number of variables............................:        6\n                     variables with only lower bounds:        0\n                variables with lower and upper bounds:        0\n                     variables with only upper bounds:        0\nTotal number of equality constraints.................:        0\nTotal number of inequality constraints...............:        0\n        inequality constraints with only lower bounds:        0\n   inequality constraints with lower and upper bounds:        0\n        inequality constraints with only upper bounds:        0\n\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n   0  4.2109423e+03 0.00e+00 1.00e+02   0.0 0.00e+00    -  0.00e+00 0.00e+00   0 \n   5  3.8445586e+03 0.00e+00 7.87e-01 -11.0 4.94e-02    -  1.00e+00 1.00e+00f  1 MaxS\n  10  3.8443870e+03 0.00e+00 2.25e-01 -11.0 1.38e-02    -  1.00e+00 1.00e+00f  1 MaxS\n  15  3.8443742e+03 0.00e+00 6.23e-02 -11.0 3.78e-03    -  1.00e+00 1.00e+00f  1 MaxS\n  20  3.8443733e+03 0.00e+00 1.70e-02 -11.0 1.03e-03    -  1.00e+00 1.00e+00f  1 MaxS\n  25  3.8443732e+03 0.00e+00 4.61e-03 -11.0 2.79e-04    -  1.00e+00 1.00e+00f  1 MaxS\n  30  3.8443732e+03 0.00e+00 1.25e-03 -11.0 7.56e-05    -  1.00e+00 1.00e+00f  1 MaxS\n  35  3.8443732e+03 0.00e+00 3.39e-04 -11.0 2.05e-05    -  1.00e+00 1.00e+00f  1 MaxS\n  40  3.8443732e+03 0.00e+00 9.19e-05 -11.0 5.55e-06    -  1.00e+00 1.00e+00f  1 MaxS\n  45  3.8443732e+03 0.00e+00 2.49e-05 -11.0 1.51e-06    -  1.00e+00 1.00e+00f  1 MaxS\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n  50  3.8443732e+03 0.00e+00 6.76e-06 -11.0 4.08e-07    -  1.00e+00 1.00e+00f  1 MaxSA\n  55  3.8443732e+03 0.00e+00 1.83e-06 -11.0 1.11e-07    -  1.00e+00 1.00e+00f  1 MaxSA\n  60  3.8443732e+03 0.00e+00 4.97e-07 -11.0 3.00e-08    -  1.00e+00 1.00e+00f  1 MaxSA\n\nNumber of Iterations....: 63\n\n                                   (scaled)                 (unscaled)\nObjective...............:   3.4496886481728075e+02    3.8443731733053696e+03\nDual infeasibility......:   2.2693631692678575e-07    2.5290047242499938e-06\nConstraint violation....:   0.0000000000000000e+00    0.0000000000000000e+00\nComplementarity.........:   0.0000000000000000e+00    0.0000000000000000e+00\nOverall NLP error.......:   2.2693631692678575e-07    2.5290047242499938e-06\n\n\nNumber of objective function evaluations             = 64\nNumber of objective gradient evaluations             = 64\nNumber of equality constraint evaluations            = 0\nNumber of inequality constraint evaluations          = 0\nNumber of equality constraint Jacobian evaluations   = 0\nNumber of inequality constraint Jacobian evaluations = 0\nNumber of Lagrangian Hessian evaluations             = 63\nTotal CPU secs in IPOPT (w/o function evaluations)   =      0.020\nTotal CPU secs in NLP function evaluations           =      0.256\n\nEXIT: Solved To Acceptable Level.\n\n\n\n\n\n# Ipopt estimates\n\n\nvcmodel_ipopt\n.\nB\n\n\n\n\n\n\n2\u00d72 Array{Float64,2}:\n 1.092     1.04727\n 0.955346  1.01632\n\n\n\n\n\n# Ipopt estimates\n\n\nvcmodel_ipopt\n.\n\u03a3\n\n\n\n\n\n\n([0.380552 -0.305594; -0.305594 4.52106], [1.84008 0.265385; 0.265385 2.17287])\n\n\n\n\n\nKnitro is a commercial software and users need to follow instructions at \nKNITRO.jl\n for proper functioning. Following code invokes Knitro as the backend optimization solver.\n\n\nusing\n \nKNITRO\n\n\n\n# Fisher scoring using Knitro\n\n\nvcmodel_knitro\n \n=\n \ndeepcopy\n(\nvcmodel\n)\n\n\n@time\n \nmle_fs!\n(\nvcmodel_knitro\n,\n \nvcdatarot\n;\n \nsolver\n=:\nKnitro\n,\n \nmaxiter\n=\n1000\n,\n \nverbose\n=\ntrue\n);\n\n\n\n# Knitro estimates\n\n\nvcmodel_knitro\n.\nB\n\n\n\n# Knitro estimates\n\n\nvcmodel_knitro\n.\n\u03a3\n\n\n\n\n\n\n\n\nStarting point\n\n\nHere are a few strategies for successful optimization. \n\n\n\n\nFor $d\n1$ (multivariate response), initialize $B, \\Sigma$ from univariate estimates.\n\n\nUse REML estimate as starting point for MLE.\n\n\nWhen there are only $m=2$ variance components, pre-compute \nTwoVarCompVariateRotate\n and use it for optimization.\n\n\n\n\n\n\nConstrained estimation of \nB\n\n\nMany applications invoke constraints on the mean parameters \nB\n. For demonstration, we enforce \nB[1,1]=B[1,2]\n and all entries of \nB\n are within [0, 2].\n\n\n# set up constraints on B\n\n\nvcmodel_constr\n \n=\n \ndeepcopy\n(\nvcmodel\n)\n\n\nvcmodel_constr\n.\nA\n \n=\n \n[\n1.0\n \n0.0\n \n-\n1.0\n \n0.0\n]\n\n\nvcmodel_constr\n.\nsense\n \n=\n \n=\n\n\nvcmodel_constr\n.\nb\n \n=\n \n0.0\n\n\nvcmodel_constr\n.\nlb\n \n=\n \n0.0\n\n\nvcmodel_constr\n.\nub\n \n=\n \n2.0\n\n\nvcmodel_constr\n\n\n\n\n\n\nVarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}([0.0 0.0; 0.0 0.0], ([1.0 0.0; 0.0 1.0], [1.0 0.0; 0.0 1.0]), [1.0 0.0 -1.0 0.0], \n=\n, 0.0, 0.0, 2.0)\n\n\n\n\n\nWe first try the MM algorithm.\n\n\n# MM algorithm for constrained estimation of B\n\n\n@time\n \nmle_mm!\n(\nvcmodel_constr\n,\n \nvcdatarot\n;\n \nmaxiter\n=\n10000\n,\n \nfuntol\n=\n1e-8\n,\n \nverbose\n \n=\n \ntrue\n);\n\n\n\n\n\n\n     MM Algorithm\n  Iter      Objective  \n--------  -------------\n       0  -6.253551e+03\n       1  -3.881820e+03\n       2  -3.853477e+03\n       3  -3.846807e+03\n       4  -3.845184e+03\n       5  -3.844783e+03\n       6  -3.844683e+03\n       7  -3.844658e+03\n       8  -3.844652e+03\n       9  -3.844650e+03\n      10  -3.844650e+03\n\n  0.031954 seconds (10.70 k allocations: 781.828 KiB)\n\n\n\n\n\nfieldnames\n(\nvcmodel_constr\n)\n\n\n\n\n\n\n7-element Array{Symbol,1}:\n :B    \n :\u03a3    \n :A    \n :sense\n :b    \n :lb   \n :ub\n\n\n\n\n\nvcmodel_constr\n.\nB\n\n\n\n\n\n\n2\u00d72 Array{Float64,2}:\n 1.07177   1.07177\n 0.955683  1.01591\n\n\n\n\n\nvcmodel_constr\n.\n\u03a3\n\n\n\n\n\n\n([0.380624 -0.305498; -0.305498 4.51948], [1.84051 0.265065; 0.265065 2.17336])\n\n\n\n\n\nNow let's try Fisher scoring.\n\n\n# Fisher scoring using Ipopt for constrained estimation of B\n\n\nvcmodel_constr\n \n=\n \ndeepcopy\n(\nvcmodel\n)\n\n\nvcmodel_constr\n.\nA\n \n=\n \n[\n1.0\n \n0.0\n \n-\n1.0\n \n0.0\n]\n\n\nvcmodel_constr\n.\nsense\n \n=\n \n=\n\n\nvcmodel_constr\n.\nb\n \n=\n \n0.0\n\n\nvcmodel_constr\n.\nlb\n \n=\n \n0.0\n\n\nvcmodel_constr\n.\nub\n \n=\n \n2.0\n\n\nvcmodel_constr\n\n\n@time\n \nmle_fs!\n(\nvcmodel_constr\n,\n \nvcdatarot\n;\n \nsolver\n=:\nIpopt\n,\n \nmaxiter\n=\n1000\n,\n \nverbose\n=\ntrue\n);\n\n\n\n\n\n\nThis is Ipopt version 3.12.4, running with linear solver mumps.\nNOTE: Other linear solvers might be more efficient (see Ipopt documentation).\n\nNumber of nonzeros in equality constraint Jacobian...:        0\nNumber of nonzeros in inequality constraint Jacobian.:        0\nNumber of nonzeros in Lagrangian Hessian.............:       21\n\nTotal number of variables............................:        6\n                     variables with only lower bounds:        0\n                variables with lower and upper bounds:        0\n                     variables with only upper bounds:        0\nTotal number of equality constraints.................:        0\nTotal number of inequality constraints...............:        0\n        inequality constraints with only lower bounds:        0\n   inequality constraints with lower and upper bounds:        0\n        inequality constraints with only upper bounds:        0\n\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n   0  4.2114270e+03 0.00e+00 1.00e+02   0.0 0.00e+00    -  0.00e+00 0.00e+00   0 \n   5  3.8448353e+03 0.00e+00 7.87e-01 -11.0 4.94e-02    -  1.00e+00 1.00e+00f  1 MaxS\n  10  3.8446636e+03 0.00e+00 2.25e-01 -11.0 1.38e-02    -  1.00e+00 1.00e+00f  1 MaxS\n  15  3.8446509e+03 0.00e+00 6.23e-02 -11.0 3.78e-03    -  1.00e+00 1.00e+00f  1 MaxS\n  20  3.8446499e+03 0.00e+00 1.70e-02 -11.0 1.03e-03    -  1.00e+00 1.00e+00f  1 MaxS\n  25  3.8446498e+03 0.00e+00 4.61e-03 -11.0 2.79e-04    -  1.00e+00 1.00e+00f  1 MaxS\n  30  3.8446498e+03 0.00e+00 1.25e-03 -11.0 7.56e-05    -  1.00e+00 1.00e+00f  1 MaxS\n  35  3.8446498e+03 0.00e+00 3.39e-04 -11.0 2.05e-05    -  1.00e+00 1.00e+00f  1 MaxS\n  40  3.8446498e+03 0.00e+00 9.19e-05 -11.0 5.56e-06    -  1.00e+00 1.00e+00f  1 MaxS\n  45  3.8446498e+03 0.00e+00 2.49e-05 -11.0 1.51e-06    -  1.00e+00 1.00e+00f  1 MaxS\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n  50  3.8446498e+03 0.00e+00 6.76e-06 -11.0 4.08e-07    -  1.00e+00 1.00e+00f  1 MaxSA\n  55  3.8446498e+03 0.00e+00 1.83e-06 -11.0 1.11e-07    -  1.00e+00 1.00e+00f  1 MaxSA\n  60  3.8446498e+03 0.00e+00 4.97e-07 -11.0 3.00e-08    -  1.00e+00 1.00e+00f  1 MaxSA\n\nNumber of Iterations....: 63\n\n                                   (scaled)                 (unscaled)\nObjective...............:   3.4484507551949008e+02    3.8446498170293403e+03\nDual infeasibility......:   2.2694405349430929e-07    2.5301808715939735e-06\nConstraint violation....:   0.0000000000000000e+00    0.0000000000000000e+00\nComplementarity.........:   0.0000000000000000e+00    0.0000000000000000e+00\nOverall NLP error.......:   2.2694405349430929e-07    2.5301808715939735e-06\n\n\nNumber of objective function evaluations             = 64\nNumber of objective gradient evaluations             = 64\nNumber of equality constraint evaluations            = 0\nNumber of inequality constraint evaluations          = 0\nNumber of equality constraint Jacobian evaluations   = 0\nNumber of inequality constraint Jacobian evaluations = 0\nNumber of Lagrangian Hessian evaluations             = 63\nTotal CPU secs in IPOPT (w/o function evaluations)   =      0.016\nTotal CPU secs in NLP function evaluations           =      0.417\n\nEXIT: Solved To Acceptable Level.\n\n\n\n\n\nvcmodel_constr\n.\nB\n\n\n\n\n\n\n2\u00d72 Array{Float64,2}:\n 1.07177   1.07177\n 0.955683  1.01591\n\n\n\n\n\nvcmodel_constr\n.\n\u03a3\n\n\n\n\n\n\n([0.380539 -0.305626; -0.305626 4.52116], [1.8405 0.264881; 0.264881 2.17348])", 
            "title": "MLE/REML"
        }, 
        {
            "location": "/man/mle_reml/#mle-and-reml", 
            "text": "Machine information  versioninfo ()   Julia Version 0.6.0\nCommit 903644385b (2017-06-19 13:05 UTC)\nPlatform Info:\n  OS: macOS (x86_64-apple-darwin13.4.0)\n  CPU: Intel(R) Core(TM) i7-4790K CPU @ 4.00GHz\n  WORD_SIZE: 64\n  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Haswell)\n  LAPACK: libopenblas64_\n  LIBM: libopenlibm\n  LLVM: libLLVM-3.9.1 (ORCJIT, haswell)", 
            "title": "MLE and REML"
        }, 
        {
            "location": "/man/mle_reml/#demo-data", 
            "text": "For demonstration, we generate a random data set.  # generate data from a d-variate response variane component model  srand ( 123 )  n   =   1000     # no. observations  d   =   2        # dimension of responses  m   =   2        # no. variance components  p   =   2        # no. covariates  # n-by-p design matrix  X   =   randn ( n ,   p )  # p-by-d mean component regression coefficient  B   =   ones ( p ,   d )    # a tuple of m covariance matrices  V   =   ntuple ( x   -   zeros ( n ,   n ),   m )   for   i   =   1 : m - 1 \n   Vi   =   randn ( n ,   50 ) \n   copy! ( V [ i ],   Vi   *   Vi )  end  copy! ( V [ m ],   eye ( n ))   # last covarianec matrix is idendity  # a tuple of m d-by-d variance component parameters  \u03a3   =   ntuple ( x   -   zeros ( d ,   d ),   m )   for   i   in   1 : m \n   \u03a3i   =   randn ( d ,   d ) \n   copy! ( \u03a3 [ i ],   \u03a3i   *   \u03a3i )  end  # form overall nd-by-nd covariance matrix \u03a9  \u03a9   =   zeros ( n   *   d ,   n   *   d )  for   i   =   1 : m \n   \u03a9   +=   kron ( \u03a3 [ i ],   V [ i ])  end  \u03a9chol   =   cholfact ( \u03a9 )  # n-by-d responses  Y   =   X   *   B   +   reshape ( \u03a9chol [ : L ]   *   randn ( n * d ),   n ,   d );", 
            "title": "Demo data"
        }, 
        {
            "location": "/man/mle_reml/#maximum-likelihood-estimation-mle", 
            "text": "To find the MLE of parameters $(B,\\Sigma_1,\\ldots,\\Sigma_m)$, we take 3 steps:    Step 1 (Construct data) . Construct an instance of  VarianceComponentVariate , which consists fields     Y : $n$-by-$d$ responses  X : $n$-by-$p$ covariate matrix  V=(V[1],...,V[m]) : a tuple of $n$-by-$n$ covariance matrices. The last covariance matrix must be positive definite and usually is the identity matrix.   using   VarianceComponentModels  vcdata   =   VarianceComponentVariate ( Y ,   X ,   V )  fieldnames ( vcdata )   3-element Array{Symbol,1}:\n :Y\n :X\n :V  In the absence of covariates $X$, we can simply initialize by  vcdata = VarianceComponentVariate(Y, V) .  Step 2 (Construct a model) . Construct an instance of  VarianceComponentModel , which consists of fields     B : $n$-by-$p$ mean regression coefficients  \u03a3=(\u03a3[1],...,\u03a3[m]) : variane component parameters respectively.   When constructed from a  VarianceComponentVariate  instance, the mean parameters $B$ are initialized to be zero and the tuple of variance component parameters $\\Sigma$ to be  (eye(d),...,eye(d)) .  vcmodel   =   VarianceComponentModel ( vcdata )  fieldnames ( vcmodel )   7-element Array{Symbol,1}:\n :B    \n :\u03a3    \n :A    \n :sense\n :b    \n :lb   \n :ub  vcmodel   VarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}([0.0 0.0; 0.0 0.0], ([1.0 0.0; 0.0 1.0], [1.0 0.0; 0.0 1.0]), Array{Float64}(0,4), Char[], Float64[], -Inf, Inf)  The remaining fields  A ,  sense ,  b ,  lb ,  ub  specify (optional) constraints on the mean parameters  B :   \nA * \\text{vec}(B) \\,\\, =(\\text{or } \\ge \\text{or } \\le) \\,\\, b    \nlb \\le \\text{vec}(B) \\le ub   A  is an constraint matrix with $pd$ columns,  sense  is a vector of charaters taking values  ' ' ,  '='  or  ' ' , and  lb  and  ub  are the lower and upper bounds for  vec(B) . By default,  A ,  sense ,  b  are empty,  lb  is  -Inf , and  ub  is  Inf . If any constraits are non-trivial, final estimates of  B  are enforced to satisfy them.  When a better initial guess is available, we can initialize by calling  vcmodel=VarianceComponentModel(B0, \u03a30)  directly.  Step 3 (Fit model) . Call optmization routine  fit_mle! . The keywork  algo  dictates the optimization algorithm:  :MM  (minorization-maximization algorithm) or  :FS  (Fisher scoring algorithm).  vcmodel_mle   =   deepcopy ( vcmodel )  @time   logl ,   vcmodel_mle ,   \u03a3se ,   \u03a3cov ,   Bse ,   Bcov   =   fit_mle! ( vcmodel_mle ,   vcdata ;   algo   =   : MM );        MM Algorithm\n  Iter      Objective  \n--------  -------------\n       0  -6.253551e+03\n       1  -3.881454e+03\n       2  -3.853179e+03\n       3  -3.846525e+03\n       4  -3.844906e+03\n       5  -3.844506e+03\n       6  -3.844406e+03\n       7  -3.844381e+03\n       8  -3.844375e+03\n       9  -3.844374e+03\n      10  -3.844373e+03\n\n  0.290970 seconds (10.45 k allocations: 24.036 MiB, 4.73% gc time)  The output of  fit_mle!  contains     final log-likelihood   logl   -3844.3731814180805   fitted model   fieldnames ( vcmodel_mle )   7-element Array{Symbol,1}:\n :B    \n :\u03a3    \n :A    \n :sense\n :b    \n :lb   \n :ub  vcmodel_mle   VarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}([1.092 1.04727; 0.955346 1.01632], ([0.380637 -0.305465; -0.305465 4.51938], [1.84009 0.265569; 0.265569 2.17275]), Array{Float64}(0,4), Char[], Float64[], -Inf, Inf)   standard errors of the estimated varianec component parameters   \u03a3se   ([0.0765136 0.263047; 0.263047 0.904332], [0.0844292 0.0917441; 0.0917441 0.0996927])   covariance matrix of the variance component parameters estimates   \u03a3cov   8\u00d78 Array{Float64,2}:\n  0.00585433  -0.00467019  -0.00467019  \u2026  -1.07903e-6   -1.557e-7   \n -0.00467019   0.0691937    0.00372555     -1.557e-7     -1.27444e-6 \n -0.00467019   0.00372555   0.0691937      -8.83212e-6   -1.27444e-6 \n  0.00372555  -0.055198    -0.055198       -1.27444e-6   -1.04316e-5 \n -7.4779e-6   -1.07903e-6  -1.07903e-6      0.00102878    0.000148477\n -1.07903e-6  -8.83212e-6  -1.557e-7    \u2026   0.000148477   0.00121477 \n -1.07903e-6  -1.557e-7    -8.83212e-6      0.00841698    0.00121477 \n -1.557e-7    -1.27444e-6  -1.27444e-6      0.00121477    0.00993864   standard errors of the estimated mean parameters   Bse   2\u00d72 Array{Float64,2}:\n 0.042559   0.0487086\n 0.0430588  0.049178   covariance matrix of the mean parameter estimates   Bcov   4\u00d74 Array{Float64,2}:\n  0.00181127   -1.98035e-5    0.000240705  -2.59506e-6 \n -1.98035e-5    0.00185406   -2.59506e-6    0.000247285\n  0.000240705  -2.59506e-6    0.00237252   -2.63542e-5 \n -2.59506e-6    0.000247285  -2.63542e-5    0.00241848", 
            "title": "Maximum likelihood estimation (MLE)"
        }, 
        {
            "location": "/man/mle_reml/#restricted-maximum-likelihood-estimation-reml", 
            "text": "REML (restricted maximum likelihood estimation)  is a popular alternative to the MLE. To find the REML of a variane component model, we replace the above step 3 by    Step 3 . Call optmization routine  fit_reml! .  vcmodel_reml   =   deepcopy ( vcmodel )  @time   logl ,   vcmodel_reml ,   \u03a3se ,   \u03a3cov ,   Bse ,   Bcov   =   fit_reml! ( vcmodel_reml ,   vcdata ;   algo   =   : MM );        MM Algorithm\n  Iter      Objective  \n--------  -------------\n       0  -4.215053e+03\n       1  -3.925799e+03\n       2  -3.865114e+03\n       3  -3.851105e+03\n       4  -3.847732e+03\n       5  -3.846903e+03\n       6  -3.846698e+03\n       7  -3.846647e+03\n       8  -3.846634e+03\n       9  -3.846631e+03\n      10  -3.846630e+03\n\n  0  The output of  fit_reml!  contains   the final log-likelihood at REML estimate   logl   -3844.3777179025096   REML estimates   fieldnames ( vcmodel_reml )   7-element Array{Symbol,1}:\n :B    \n :\u03a3    \n :A    \n :sense\n :b    \n :lb   \n :ub  vcmodel_reml   VarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}([1.092 1.04727; 0.955345 1.01632], ([0.380594 -0.305485; -0.305485 4.51994], [1.84285 0.261963; 0.261963 2.17842]), Array{Float64}(0,4), Char[], Float64[], -Inf, Inf)   standard errors of the estimated varianec component parameters   \u03a3se   ([0.0765055 0.26305; 0.26305 0.904446], [0.0845559 0.0919325; 0.0919325 0.0999526])   covariance matrix of the variance component parameters estimates   \u03a3cov   8\u00d78 Array{Float64,2}:\n  0.0058531   -0.00467005  -0.00467005  \u2026  -1.06597e-6   -1.51499e-7 \n -0.00467005   0.0691951    0.00372613     -1.51499e-7   -1.26041e-6 \n -0.00467005   0.00372613   0.0691951      -8.86843e-6   -1.26041e-6 \n  0.00372613  -0.0552092   -0.0552092      -1.26041e-6   -1.0486e-5  \n -7.50035e-6  -1.06597e-6  -1.06597e-6      0.00101633    0.000144472\n -1.06597e-6  -8.86843e-6  -1.51499e-7  \u2026   0.000144472   0.0012014  \n -1.06597e-6  -1.51499e-7  -8.86843e-6      0.00845158    0.0012014  \n -1.51499e-7  -1.26041e-6  -1.26041e-6      0.0012014     0.00999052   standard errors of the estimated mean parameters   Bse   2\u00d72 Array{Float64,2}:\n 0.0425909  0.0487744\n 0.043091   0.0492444   covariance matrix of the mean parameter estimates   Bcov   4\u00d74 Array{Float64,2}:\n  0.00181398   -1.98331e-5    0.000237127  -2.55589e-6 \n -1.98331e-5    0.00185683   -2.55589e-6    0.000243624\n  0.000237127  -2.55589e-6    0.00237894   -2.6426e-5  \n -2.55589e-6    0.000243624  -2.6426e-5     0.00242501", 
            "title": "Restricted maximum likelihood estimation (REML)"
        }, 
        {
            "location": "/man/mle_reml/#optimization-algorithms", 
            "text": "Finding the MLE or REML of variance component models is a non-trivial nonlinear optimization problem. The main complications are the non-convexity of objective function and the positive semi-definiteness constraint of variane component parameters $\\Sigma_1,\\ldots,\\Sigma_m$. In specific applications, users should try different algorithms with different starting points in order to find a better solution. Here are some tips for efficient computation.   In general the optimization algorithm needs to invert the $nd$ by $nd$ overall covariance matrix $\\Omega = \\Sigma_1 \\otimes V_1 + \\cdots + \\Sigma_m \\otimes V_m$ in each iteration. Inverting a matrix is an expensive operation with $O(n^3 d^3)$ floating operations. When there are only  two  varianec components ($m=2$), this tedious task can be avoided by taking one (generalized) eigendecomposion of $(V_1, V_2)$ and rotating data $(Y, X)$ by the eigen-vectors.   vcdatarot   =   TwoVarCompVariateRotate ( vcdata )  fieldnames ( vcdatarot )   5-element Array{Symbol,1}:\n :Yrot    \n :Xrot    \n :eigval  \n :eigvec  \n :logdetV2  Two optimization algorithms are implemented:  Fisher scoring  ( mle_fs! ) and the  minorization-maximization (MM) algorithm  ( mle_mm! ). Both take the rotated data as input. These two functions give finer control of the optimization algorithms. Generally speaking, MM algorithm is more stable while Fisher scoring (if it converges) yields more accurate answer.  vcmodel_mm   =   deepcopy ( vcmodel )  @time   mle_mm! ( vcmodel_mm ,   vcdatarot ;   maxiter = 10000 ,   funtol = 1e-8 ,   verbose   =   true );        MM Algorithm\n  Iter      Objective  \n--------  -------------\n       0  -6.253551e+03\n       1  -3.881454e+03\n       2  -3.853179e+03\n       3  -3.846525e+03\n       4  -3.844906e+03\n       5  -3.844506e+03\n       6  -3.844406e+03\n       7  -3.844381e+03\n       8  -3.844375e+03\n       9  -3.844374e+03\n      10  -3.844373e+03\n\n  0.018754 seconds (9.15 k allocations: 680.172 KiB)  # MM estimates  vcmodel_mm . B   2\u00d72 Array{Float64,2}:\n 1.092     1.04727\n 0.955346  1.01632  # MM estimates  vcmodel_mm . \u03a3   ([0.380637 -0.305465; -0.305465 4.51938], [1.84009 0.265569; 0.265569 2.17275])  Fisher scoring ( mle_fs! ) uses either  Ipopt.jl  (keyword  solver=:Ipopt ) or  KNITRO.jl  (keyword  solver=:Knitro ) as the backend solver. Ipopt is open source and installation of  Ipopt.jl  package alone is sufficient.  # Fisher scoring using Ipopt  vcmodel_ipopt   =   deepcopy ( vcmodel )  @time   mle_fs! ( vcmodel_ipopt ,   vcdatarot ;   solver =: Ipopt ,   maxiter = 1000 ,   verbose = true );   This is Ipopt version 3.12.4, running with linear solver mumps.\nNOTE: Other linear solvers might be more efficient (see Ipopt documentation).\n\nNumber of nonzeros in equality constraint Jacobian...:        0\nNumber of nonzeros in inequality constraint Jacobian.:        0\nNumber of nonzeros in Lagrangian Hessian.............:       21\n\nTotal number of variables............................:        6\n                     variables with only lower bounds:        0\n                variables with lower and upper bounds:        0\n                     variables with only upper bounds:        0\nTotal number of equality constraints.................:        0\nTotal number of inequality constraints...............:        0\n        inequality constraints with only lower bounds:        0\n   inequality constraints with lower and upper bounds:        0\n        inequality constraints with only upper bounds:        0\n\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n   0  4.2109423e+03 0.00e+00 1.00e+02   0.0 0.00e+00    -  0.00e+00 0.00e+00   0 \n   5  3.8445586e+03 0.00e+00 7.87e-01 -11.0 4.94e-02    -  1.00e+00 1.00e+00f  1 MaxS\n  10  3.8443870e+03 0.00e+00 2.25e-01 -11.0 1.38e-02    -  1.00e+00 1.00e+00f  1 MaxS\n  15  3.8443742e+03 0.00e+00 6.23e-02 -11.0 3.78e-03    -  1.00e+00 1.00e+00f  1 MaxS\n  20  3.8443733e+03 0.00e+00 1.70e-02 -11.0 1.03e-03    -  1.00e+00 1.00e+00f  1 MaxS\n  25  3.8443732e+03 0.00e+00 4.61e-03 -11.0 2.79e-04    -  1.00e+00 1.00e+00f  1 MaxS\n  30  3.8443732e+03 0.00e+00 1.25e-03 -11.0 7.56e-05    -  1.00e+00 1.00e+00f  1 MaxS\n  35  3.8443732e+03 0.00e+00 3.39e-04 -11.0 2.05e-05    -  1.00e+00 1.00e+00f  1 MaxS\n  40  3.8443732e+03 0.00e+00 9.19e-05 -11.0 5.55e-06    -  1.00e+00 1.00e+00f  1 MaxS\n  45  3.8443732e+03 0.00e+00 2.49e-05 -11.0 1.51e-06    -  1.00e+00 1.00e+00f  1 MaxS\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n  50  3.8443732e+03 0.00e+00 6.76e-06 -11.0 4.08e-07    -  1.00e+00 1.00e+00f  1 MaxSA\n  55  3.8443732e+03 0.00e+00 1.83e-06 -11.0 1.11e-07    -  1.00e+00 1.00e+00f  1 MaxSA\n  60  3.8443732e+03 0.00e+00 4.97e-07 -11.0 3.00e-08    -  1.00e+00 1.00e+00f  1 MaxSA\n\nNumber of Iterations....: 63\n\n                                   (scaled)                 (unscaled)\nObjective...............:   3.4496886481728075e+02    3.8443731733053696e+03\nDual infeasibility......:   2.2693631692678575e-07    2.5290047242499938e-06\nConstraint violation....:   0.0000000000000000e+00    0.0000000000000000e+00\nComplementarity.........:   0.0000000000000000e+00    0.0000000000000000e+00\nOverall NLP error.......:   2.2693631692678575e-07    2.5290047242499938e-06\n\n\nNumber of objective function evaluations             = 64\nNumber of objective gradient evaluations             = 64\nNumber of equality constraint evaluations            = 0\nNumber of inequality constraint evaluations          = 0\nNumber of equality constraint Jacobian evaluations   = 0\nNumber of inequality constraint Jacobian evaluations = 0\nNumber of Lagrangian Hessian evaluations             = 63\nTotal CPU secs in IPOPT (w/o function evaluations)   =      0.020\nTotal CPU secs in NLP function evaluations           =      0.256\n\nEXIT: Solved To Acceptable Level.  # Ipopt estimates  vcmodel_ipopt . B   2\u00d72 Array{Float64,2}:\n 1.092     1.04727\n 0.955346  1.01632  # Ipopt estimates  vcmodel_ipopt . \u03a3   ([0.380552 -0.305594; -0.305594 4.52106], [1.84008 0.265385; 0.265385 2.17287])  Knitro is a commercial software and users need to follow instructions at  KNITRO.jl  for proper functioning. Following code invokes Knitro as the backend optimization solver.  using   KNITRO  # Fisher scoring using Knitro  vcmodel_knitro   =   deepcopy ( vcmodel )  @time   mle_fs! ( vcmodel_knitro ,   vcdatarot ;   solver =: Knitro ,   maxiter = 1000 ,   verbose = true );  # Knitro estimates  vcmodel_knitro . B  # Knitro estimates  vcmodel_knitro . \u03a3", 
            "title": "Optimization algorithms"
        }, 
        {
            "location": "/man/mle_reml/#starting-point", 
            "text": "Here are a few strategies for successful optimization.    For $d 1$ (multivariate response), initialize $B, \\Sigma$ from univariate estimates.  Use REML estimate as starting point for MLE.  When there are only $m=2$ variance components, pre-compute  TwoVarCompVariateRotate  and use it for optimization.", 
            "title": "Starting point"
        }, 
        {
            "location": "/man/mle_reml/#constrained-estimation-of-b", 
            "text": "Many applications invoke constraints on the mean parameters  B . For demonstration, we enforce  B[1,1]=B[1,2]  and all entries of  B  are within [0, 2].  # set up constraints on B  vcmodel_constr   =   deepcopy ( vcmodel )  vcmodel_constr . A   =   [ 1.0   0.0   - 1.0   0.0 ]  vcmodel_constr . sense   =   =  vcmodel_constr . b   =   0.0  vcmodel_constr . lb   =   0.0  vcmodel_constr . ub   =   2.0  vcmodel_constr   VarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}([0.0 0.0; 0.0 0.0], ([1.0 0.0; 0.0 1.0], [1.0 0.0; 0.0 1.0]), [1.0 0.0 -1.0 0.0],  = , 0.0, 0.0, 2.0)  We first try the MM algorithm.  # MM algorithm for constrained estimation of B  @time   mle_mm! ( vcmodel_constr ,   vcdatarot ;   maxiter = 10000 ,   funtol = 1e-8 ,   verbose   =   true );        MM Algorithm\n  Iter      Objective  \n--------  -------------\n       0  -6.253551e+03\n       1  -3.881820e+03\n       2  -3.853477e+03\n       3  -3.846807e+03\n       4  -3.845184e+03\n       5  -3.844783e+03\n       6  -3.844683e+03\n       7  -3.844658e+03\n       8  -3.844652e+03\n       9  -3.844650e+03\n      10  -3.844650e+03\n\n  0.031954 seconds (10.70 k allocations: 781.828 KiB)  fieldnames ( vcmodel_constr )   7-element Array{Symbol,1}:\n :B    \n :\u03a3    \n :A    \n :sense\n :b    \n :lb   \n :ub  vcmodel_constr . B   2\u00d72 Array{Float64,2}:\n 1.07177   1.07177\n 0.955683  1.01591  vcmodel_constr . \u03a3   ([0.380624 -0.305498; -0.305498 4.51948], [1.84051 0.265065; 0.265065 2.17336])  Now let's try Fisher scoring.  # Fisher scoring using Ipopt for constrained estimation of B  vcmodel_constr   =   deepcopy ( vcmodel )  vcmodel_constr . A   =   [ 1.0   0.0   - 1.0   0.0 ]  vcmodel_constr . sense   =   =  vcmodel_constr . b   =   0.0  vcmodel_constr . lb   =   0.0  vcmodel_constr . ub   =   2.0  vcmodel_constr  @time   mle_fs! ( vcmodel_constr ,   vcdatarot ;   solver =: Ipopt ,   maxiter = 1000 ,   verbose = true );   This is Ipopt version 3.12.4, running with linear solver mumps.\nNOTE: Other linear solvers might be more efficient (see Ipopt documentation).\n\nNumber of nonzeros in equality constraint Jacobian...:        0\nNumber of nonzeros in inequality constraint Jacobian.:        0\nNumber of nonzeros in Lagrangian Hessian.............:       21\n\nTotal number of variables............................:        6\n                     variables with only lower bounds:        0\n                variables with lower and upper bounds:        0\n                     variables with only upper bounds:        0\nTotal number of equality constraints.................:        0\nTotal number of inequality constraints...............:        0\n        inequality constraints with only lower bounds:        0\n   inequality constraints with lower and upper bounds:        0\n        inequality constraints with only upper bounds:        0\n\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n   0  4.2114270e+03 0.00e+00 1.00e+02   0.0 0.00e+00    -  0.00e+00 0.00e+00   0 \n   5  3.8448353e+03 0.00e+00 7.87e-01 -11.0 4.94e-02    -  1.00e+00 1.00e+00f  1 MaxS\n  10  3.8446636e+03 0.00e+00 2.25e-01 -11.0 1.38e-02    -  1.00e+00 1.00e+00f  1 MaxS\n  15  3.8446509e+03 0.00e+00 6.23e-02 -11.0 3.78e-03    -  1.00e+00 1.00e+00f  1 MaxS\n  20  3.8446499e+03 0.00e+00 1.70e-02 -11.0 1.03e-03    -  1.00e+00 1.00e+00f  1 MaxS\n  25  3.8446498e+03 0.00e+00 4.61e-03 -11.0 2.79e-04    -  1.00e+00 1.00e+00f  1 MaxS\n  30  3.8446498e+03 0.00e+00 1.25e-03 -11.0 7.56e-05    -  1.00e+00 1.00e+00f  1 MaxS\n  35  3.8446498e+03 0.00e+00 3.39e-04 -11.0 2.05e-05    -  1.00e+00 1.00e+00f  1 MaxS\n  40  3.8446498e+03 0.00e+00 9.19e-05 -11.0 5.56e-06    -  1.00e+00 1.00e+00f  1 MaxS\n  45  3.8446498e+03 0.00e+00 2.49e-05 -11.0 1.51e-06    -  1.00e+00 1.00e+00f  1 MaxS\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n  50  3.8446498e+03 0.00e+00 6.76e-06 -11.0 4.08e-07    -  1.00e+00 1.00e+00f  1 MaxSA\n  55  3.8446498e+03 0.00e+00 1.83e-06 -11.0 1.11e-07    -  1.00e+00 1.00e+00f  1 MaxSA\n  60  3.8446498e+03 0.00e+00 4.97e-07 -11.0 3.00e-08    -  1.00e+00 1.00e+00f  1 MaxSA\n\nNumber of Iterations....: 63\n\n                                   (scaled)                 (unscaled)\nObjective...............:   3.4484507551949008e+02    3.8446498170293403e+03\nDual infeasibility......:   2.2694405349430929e-07    2.5301808715939735e-06\nConstraint violation....:   0.0000000000000000e+00    0.0000000000000000e+00\nComplementarity.........:   0.0000000000000000e+00    0.0000000000000000e+00\nOverall NLP error.......:   2.2694405349430929e-07    2.5301808715939735e-06\n\n\nNumber of objective function evaluations             = 64\nNumber of objective gradient evaluations             = 64\nNumber of equality constraint evaluations            = 0\nNumber of inequality constraint evaluations          = 0\nNumber of equality constraint Jacobian evaluations   = 0\nNumber of inequality constraint Jacobian evaluations = 0\nNumber of Lagrangian Hessian evaluations             = 63\nTotal CPU secs in IPOPT (w/o function evaluations)   =      0.016\nTotal CPU secs in NLP function evaluations           =      0.417\n\nEXIT: Solved To Acceptable Level.  vcmodel_constr . B   2\u00d72 Array{Float64,2}:\n 1.07177   1.07177\n 0.955683  1.01591  vcmodel_constr . \u03a3   ([0.380539 -0.305626; -0.305626 4.52116], [1.8405 0.264881; 0.264881 2.17348])", 
            "title": "Constrained estimation of B"
        }, 
        {
            "location": "/man/heritability/", 
            "text": "Heritability Analysis\n\n\nAs an application of the variance component model, this note demonstrates the workflow for heritability analysis in genetics, using a sample data set \ncg10k\n with \n6,670\n individuals and \n630,860\n SNPs. Person IDs and phenotype names are masked for privacy. \ncg10k.bed\n, \ncg10k.bim\n, and \ncg10k.fam\n is a set of Plink files in binary format. \ncg10k_traits.txt\n contains 13 phenotypes of the 6,670 individuals.\n\n\n;\nls\n \ncg10k\n.\nbed\n \ncg10k\n.\nbim\n \ncg10k\n.\nfam\n \ncg10k_traits\n.\ntxt\n\n\n\n\n\n\ncg10k.bed\ncg10k.bim\ncg10k.fam\ncg10k_traits.txt\n\n\n\n\n\nMachine information:\n\n\nversioninfo\n()\n\n\n\n\n\n\nJulia Version 0.6.0\nCommit 903644385b (2017-06-19 13:05 UTC)\nPlatform Info:\n  OS: macOS (x86_64-apple-darwin13.4.0)\n  CPU: Intel(R) Core(TM) i7-4790K CPU @ 4.00GHz\n  WORD_SIZE: 64\n  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Haswell)\n  LAPACK: libopenblas64_\n  LIBM: libopenlibm\n  LLVM: libLLVM-3.9.1 (ORCJIT, haswell)\n\n\n\n\n\n\n\nRead in binary SNP data\n\n\nWe will use the \nSnpArrays.jl\n package to read in binary SNP data and compute the empirical kinship matrix. Issue \n\n\nPkg\n.\nclone\n(\nhttps://github.com/OpenMendel/SnpArrays.jl.git\n)\n\n\n\n\n\n\nwithin \nJulia\n to install the \nSnpArrays\n package.\n\n\nusing\n \nSnpArrays\n\n\n\n\n\n\n# read in genotype data from Plink binary file (~50 secs on my laptop)\n\n\n@time\n \ncg10k\n \n=\n \nSnpArray\n(\ncg10k\n)\n\n\n\n\n\n\n 22.902730 seconds (51.62 k allocations: 1005.845 MiB, 0.11% gc time)\n\n\n\n\n\n6670\u00d7630860 SnpArrays.SnpArray{2}:\n (false, true)   (false, true)   \u2026  (true, true)    (true, true) \n (true, true)    (true, true)       (false, true)   (true, false)\n (true, true)    (true, true)       (true, true)    (true, true) \n (true, true)    (true, true)       (false, true)   (true, true) \n (true, true)    (true, true)       (true, true)    (false, true)\n (false, true)   (false, true)   \u2026  (true, true)    (true, true) \n (false, false)  (false, false)     (true, true)    (true, true) \n (true, true)    (true, true)       (true, true)    (false, true)\n (true, true)    (true, true)       (true, true)    (true, true) \n (true, true)    (true, true)       (false, true)   (true, true) \n (true, true)    (true, true)    \u2026  (true, true)    (true, true) \n (false, true)   (false, true)      (true, true)    (false, true)\n (true, true)    (true, true)       (true, true)    (false, true)\n \u22ee                               \u22f1                               \n (false, true)   (false, true)      (false, true)   (false, true)\n (false, true)   (false, true)      (false, true)   (true, true) \n (true, true)    (true, true)    \u2026  (false, true)   (true, true) \n (false, true)   (false, true)      (true, true)    (false, true)\n (true, true)    (true, true)       (false, true)   (true, true) \n (true, true)    (true, true)       (false, false)  (false, true)\n (true, true)    (true, true)       (true, true)    (false, true)\n (true, true)    (true, true)    \u2026  (true, true)    (true, true) \n (true, true)    (true, true)       (false, true)   (true, true) \n (true, true)    (true, true)       (true, true)    (false, true)\n (false, true)   (false, true)      (true, true)    (true, true) \n (true, true)    (true, true)       (true, true)    (true, true)\n\n\n\n\n\n\n\nSummary statistics of SNP data\n\n\npeople\n,\n \nsnps\n \n=\n \nsize\n(\ncg10k\n)\n\n\n\n\n\n\n(6670, 630860)\n\n\n\n\n\n# summary statistics (~50 secs on my laptop)\n\n\n@time\n \nmaf\n,\n \n_\n,\n \nmissings_by_snp\n,\n \n=\n \nsummarize\n(\ncg10k\n);\n\n\n\n\n\n\n 24\n\n\n\n\n\n# 5 number summary and average MAF (minor allele frequencies)\n\n\nquantile\n(\nmaf\n,\n \n[\n0.0\n \n.\n25\n \n.\n5\n \n.\n75\n \n1.0\n]),\n \nmean\n(\nmaf\n)\n\n\n\n\n\n\n([0.00841726 0.124063 \u2026 0.364253 0.5], 0.24536516625042462)\n\n\n\n\n\n# Pkg.add(\nPlots\n)\n\n\n# Pkg.add(\nPyPlot\n)\n\n\nusing\n \nPlots\n\n\npyplot\n()\n\n\n\nhistogram\n(\nmaf\n,\n \nxlab\n \n=\n \nMinor Allele Frequency (MAF)\n,\n \nlabel\n \n=\n \nMAF\n)\n\n\n\n\n\n\n\n\n# proportion of missing genotypes\n\n\nsum\n(\nmissings_by_snp\n)\n \n/\n \nlength\n(\ncg10k\n)\n\n\n\n\n\n\n0.0013128198764010824\n\n\n\n\n\n# proportion of rare SNPs with maf \n 0.05\n\n\ncountnz\n(\nmaf\n \n.\n \n0.05\n)\n \n/\n \nlength\n(\nmaf\n)\n\n\n\n\n\n\n0.07228069619249913\n\n\n\n\n\n\n\nEmpirical kinship matrix\n\n\nWe estimate empirical kinship based on all SNPs by the genetic relation matrix (GRM). Missing genotypes are imputed on the fly by drawing according to the minor allele frequencies.\n\n\n# GRM using SNPs with maf \n 0.01 (default) (~10 mins on my laptop)\n\n\nsrand\n(\n123\n)\n\n\n@time\n \n\u03a6grm\n \n=\n \ngrm\n(\ncg10k\n;\n \nmethod\n \n=\n \n:\nGRM\n)\n\n\n\n\n\n\n396.943890 seconds (8.43 G allocations: 127.378 GiB, 4.38% gc time)\n\n\n\n\n\n6670\u00d76670 Array{Float64,2}:\n  0.503024      0.00335505   -0.000120075  \u2026  -5.45185e-5   -0.00278072 \n  0.00335505    0.498958     -0.00195952       0.000868471   0.0034285  \n -0.000120075  -0.00195952    0.493828         0.000174648  -0.000381467\n  0.000923828  -0.00329169   -0.00194166      -0.00223595   -0.00123508 \n -8.39649e-5   -0.00353358    0.0018709        0.00222858   -0.00171176 \n  0.00204208    0.000572952   0.00254025   \u2026   0.000861385   2.99785e-5 \n  0.000569323   0.0024786    -0.00185743       0.00117649   -0.00118027 \n -0.000642144   0.00317992   -0.00099777       0.00354182   -0.000260645\n -0.00102913   -0.00123475   -0.00061138       0.00173885    0.00177727 \n -0.00139442    0.00208423    0.000124525     -0.00145156   -0.001011   \n -0.00204555    0.00011055   -0.000419398  \u2026  -0.000198235  -0.00110353 \n  0.000947587   0.00167346    0.00184451      -0.000690143  -0.00304087 \n  0.000322759  -0.000899805   0.00303981       0.000739331  -0.00118835 \n  \u22ee                                        \u22f1                            \n  0.00298012    0.00130003    0.000998861      4.18454e-6    0.00303991 \n -0.00207748    0.00274717   -0.00191741      -0.00107073    0.00368267 \n  0.000545569  -0.00244439   -0.00299578   \u2026  -0.000669885   0.00221027 \n -0.00423186   -0.00208514   -0.00108833      -0.000622127  -0.000567483\n -0.00325644   -0.000781353   0.0030423        0.000501423  -0.00010267 \n  0.00041055   -0.00200772    0.00274867      -0.00624933   -0.00521365 \n  0.00210519    0.000879889  -0.00107817      -0.000797878  -0.000557352\n -0.00230058   -0.000119132   0.000116817  \u2026   0.000867087  -0.00233512 \n -0.0020119     0.00230772   -0.00128837       0.00194798   -0.00048733 \n -0.000944942  -0.000928073  -0.000175096      0.00126911   -0.00303766 \n -5.45185e-5    0.000868471   0.000174648      0.500829      0.000469478\n -0.00278072    0.0034285    -0.000381467      0.000469478   0.500627\n\n\n\n\n\n\n\nPhenotypes\n\n\nRead in the phenotype data and compute descriptive statistics.\n\n\n# Pkg.add(\nDataFrames\n)\n\n\nusing\n \nDataFrames\n\n\n\ncg10k_trait\n \n=\n \nreadtable\n(\n\n    \ncg10k_traits.txt\n;\n \n    \nseparator\n \n=\n \n \n,\n\n    \nnames\n \n=\n \n[\n:\nFID\n;\n \n:\nIID\n;\n \n:\nTrait1\n;\n \n:\nTrait2\n;\n \n:\nTrait3\n;\n \n:\nTrait4\n;\n \n:\nTrait5\n;\n \n:\nTrait6\n;\n \n             \n:\nTrait7\n;\n \n:\nTrait8\n;\n \n:\nTrait9\n;\n \n:\nTrait10\n;\n \n:\nTrait11\n;\n \n:\nTrait12\n;\n \n:\nTrait13\n],\n  \n    \neltypes\n \n=\n \n[\nString\n;\n \nString\n;\n \nFloat64\n;\n \nFloat64\n;\n \nFloat64\n;\n \nFloat64\n;\n \nFloat64\n;\n \n               \nFloat64\n;\n \nFloat64\n;\n \nFloat64\n;\n \nFloat64\n;\n \nFloat64\n;\n \nFloat64\n;\n \nFloat64\n;\n \nFloat64\n]\n\n    \n)\n\n\n# do not display FID and IID for privacy\n\n\ncg10k_trait\n[\n:\n,\n \n3\n:\nend\n]\n\n\n\n\n\n\nTrait1\nTrait2\nTrait3\nTrait4\nTrait5\nTrait6\nTrait7\nTrait8\nTrait9\nTrait10\nTrait11\nTrait12\nTrait13\n1\n-1.81573145026234\n-0.94615046147283\n1.11363077580442\n-2.09867121119159\n0.744416614111748\n0.00139171884080131\n0.934732480409667\n-1.22677315418103\n1.1160784277875\n-0.4436280335029\n0.824465656443384\n-1.02852542216546\n-0.394049201727681\n2\n-1.24440094378729\n0.109659992547179\n0.467119394241789\n-1.62131304097589\n1.0566758355683\n0.978946979419181\n1.00014633946047\n0.32487427140228\n1.16232175219696\n2.6922706948705\n3.08263672461047\n1.09064954786013\n0.0256616415357438\n3\n1.45566914502305\n1.53866932923243\n1.09402959376555\n0.586655272226893\n-0.32796454430367\n-0.30337709778827\n-0.0334354881314741\n-0.464463064285437\n-0.3319396273436\n-0.486839089635991\n-1.10648681564373\n-1.42015780427231\n-0.687463456644413\n4\n-0.768809276698548\n0.513490885514249\n0.244263028382142\n-1.31740254475691\n1.19393774326845\n1.17344127734288\n1.08737426675232\n0.536022583732261\n0.802759240762068\n0.234159411749815\n0.394174866891074\n-0.767365892476029\n0.0635385761884935\n5\n-0.264415132547719\n-0.348240421825694\n-0.0239065083413606\n0.00473915802244948\n1.25619191712193\n1.2038883667631\n1.29800739042627\n0.310113660247311\n0.626159861059352\n0.899289129831224\n0.54996783350812\n0.540687809542048\n0.179675416046033\n6\n-1.37617270917293\n-1.47191967744564\n0.291179894254146\n-0.803110740704731\n-0.264239977442213\n-0.260573027836772\n-0.165372266287781\n-0.219257294118362\n1.04702422290318\n-0.0985815534616482\n0.947393438068448\n0.594014812031438\n0.245407436348479\n7\n0.1009416296374\n-0.191615722103455\n-0.567421321596677\n0.378571487240382\n-0.246656179817904\n-0.608810750053858\n0.189081058215596\n-1.27077787326519\n-0.452476199143965\n0.702562877297724\n0.332636218957179\n0.0026916503626181\n0.317117176705358\n8\n-0.319818276367464\n1.35774480657283\n0.818689545938528\n-1.15565531644352\n0.63448368102259\n0.291461908634679\n0.933323714954726\n-0.741083289682492\n0.647477683507572\n-0.970877627077966\n0.220861165411304\n0.852512250237764\n-0.225904624283945\n9\n-0.288334173342032\n0.566082538090752\n0.254958336116175\n-0.652578302869714\n0.668921559277347\n0.978309199170558\n0.122862966041938\n1.4790926378214\n0.0672132424173449\n0.0795903917527827\n0.167532455243232\n0.246915579442139\n0.539932616458363\n10\n-1.15759732583991\n-0.781198583545165\n-0.595807759833517\n-1.00554980260402\n0.789828885933321\n0.571058413379044\n0.951304176233755\n-0.295962982984816\n0.99042002479707\n0.561309366988983\n0.733100030623233\n-1.73467772245684\n-1.35278484330654\n11\n0.740569150459031\n1.40873846755415\n0.734689999440088\n0.0208322841295094\n-0.337440968561619\n-0.458304040611395\n-0.142582512772326\n-0.580392297464107\n-0.684684998101516\n-0.00785381461893456\n-0.712244337518008\n-0.313345561230878\n-0.345419463162219\n12\n-0.675892486454995\n0.279892613829682\n0.267915996308248\n-1.04103665392985\n0.910741715645888\n0.866027618513171\n1.07414431702005\n0.0381751003538302\n0.766355377018601\n-0.340118016143495\n-0.809013958505059\n0.548521663785885\n-0.0201828675962336\n13\n-0.795410435603455\n-0.699989939762738\n0.3991295030063\n-0.510476261900736\n1.51552245416844\n1.28743032939467\n1.53772393250903\n0.133989160117702\n1.02025736886037\n0.499018733899186\n-0.36948273277931\n-1.10153460436318\n-0.598132438886619\n14\n-0.193483122930324\n-0.286021160323518\n-0.691494225262995\n0.0131581678700699\n1.52337470686782\n1.4010638072262\n1.53114620451896\n0.333066483478075\n1.04372480381099\n0.163206783570466\n-0.422883765001728\n-0.383527976713573\n-0.489221907788158\n15\n0.151246203379718\n2.09185108993614\n2.03800472474384\n-1.12474717143531\n1.66557024390713\n1.62535675109576\n1.58751070483655\n0.635852186043776\n0.842577784605979\n0.450761870778952\n-1.39479033623028\n-0.560984107567768\n0.289349776549287\n16\n-0.464608740812712\n0.36127694772303\n1.2327673928287\n-0.826033731086383\n1.43475224709983\n1.74451823818846\n0.211096887484638\n2.64816425140548\n1.02511433146096\n0.11975731603184\n0.0596832073448267\n-0.631231612661616\n-0.207878671782927\n17\n-0.732977488012215\n-0.526223425889779\n0.61657871336593\n-0.55447974332593\n0.947484859025104\n0.936833214138173\n0.972516806335524\n0.290251013865227\n1.01285359725723\n0.516207422283291\n-0.0300689171988194\n0.8787322524583\n0.450254629309513\n18\n-0.167326459622119\n0.175327165487237\n0.287467725892572\n-0.402652532084246\n0.551181509418056\n0.522204743290975\n0.436837660094653\n0.299564933845579\n0.583109520896067\n-0.704415820005353\n-0.730810367994577\n-1.95140580379896\n-0.933504665700164\n19\n1.41159485787418\n1.78722407901017\n0.84397639585364\n0.481278083772991\n-0.0887673728508268\n-0.49957757426858\n0.304195897924847\n-1.23884208383369\n-0.153475724036624\n-0.870486102788329\n0.0955473331150403\n-0.983708050882817\n-0.3563445644514\n20\n-1.42997091652825\n-0.490147045034213\n0.272730237607695\n-1.61029992954153\n0.990787817197748\n0.711687532608184\n1.1885836012715\n-0.371229188075638\n1.24703459239952\n-0.0389162332271516\n0.883495749072872\n2.58988026321017\n3.33539552370368\n21\n-0.147247288176765\n0.12328430415652\n0.617549051912237\n-0.18713077178262\n0.256438107586694\n0.17794983735083\n0.412611806463263\n-0.244809124559737\n0.0947624806136492\n0.723017223849532\n-0.683948354633436\n0.0873751276309269\n-0.262209652750371\n22\n-0.187112676773894\n-0.270777264595619\n-1.01556818551606\n0.0602850568600233\n0.272419757757978\n0.869133161879197\n-0.657519461414234\n2.32388522018189\n-0.999936011525034\n1.44671844178306\n0.971157886040772\n-0.358747904241515\n-0.439657942096136\n23\n-1.82434047163768\n-0.933480446068067\n1.29474003766977\n-1.94545221151036\n0.33584651189654\n0.359201654302844\n0.513652924365886\n-0.073197696696958\n1.57139042812005\n1.53329371326728\n1.82076821859528\n2.22740301867829\n1.50063347195857\n24\n-2.29344084351335\n-2.49161842344418\n0.40383988742336\n-2.36488074752948\n1.4105254831956\n1.42244117147792\n1.17024166272172\n0.84476650176855\n1.79026875432495\n0.648181858970515\n-0.0857231057403538\n-1.02789535292617\n0.491288088952859\n25\n-0.434135932888305\n0.740881989034652\n0.699576357578518\n-1.02405543187775\n0.759529223983713\n0.956656110895288\n0.633299568656589\n0.770733932268516\n0.824988511714526\n1.84287437634769\n1.91045942063443\n-0.502317207869366\n0.132670133448219\n26\n-2.1920969546557\n-2.49465664272271\n0.354854763893431\n-1.93155848635714\n0.941979400289938\n0.978917101414106\n0.894860097289736\n0.463239402831873\n1.12537133317163\n1.70528446191955\n0.717792714479123\n0.645888049108261\n0.783968250169388\n27\n-1.46602269088422\n-1.24921677101897\n0.307977693653039\n-1.55097364660989\n0.618908494474798\n0.662508171662042\n0.475957173906078\n0.484718674597707\n0.401564892028249\n0.55987973254026\n-0.376938143754217\n-0.933982629228218\n0.390013151672955\n28\n-1.83317744236881\n-1.53268787828701\n2.55674262685865\n-1.51827745783835\n0.789409601746455\n0.908747799728588\n0.649971922941479\n0.668373649931667\n1.20058303519903\n0.277963256075637\n1.2504953198275\n3.31370445071638\n2.22035828885342\n29\n-0.784546628243178\n0.276582579543931\n3.01104958800057\n-1.11978843206758\n0.920823858422707\n0.750217689886151\n1.26153730009639\n-0.403363882922417\n0.400667296857811\n-0.217597941303479\n-0.724669537565068\n-0.391945338467193\n-0.650023936358253\n30\n0.464455916345135\n1.3326356122229\n-1.23059563374303\n-0.357975958937414\n1.18249746977104\n1.54315938069757\n-0.60339041154062\n3.38308845958422\n0.823740765148641\n-0.129951318508883\n-0.657979878422938\n-0.499534924074273\n-0.414476569095651\n\n\n\ndescribe\n(\ncg10k_trait\n[\n:\n,\n \n3\n:\nend\n])\n\n\n\n\n\n\nTrait1\nSummary Stats:\nMean:           0.002211\nMinimum:        -3.204128\n1st Quartile:   -0.645771\nMedian:         0.125010\n3rd Quartile:   0.723315\nMaximum:        3.479398\nLength:         6670\nType:           Float64\nNumber Missing: 0\n% Missing:      0.000000\n\nTrait2\nSummary Stats:\nMean:           0.001353\nMinimum:        -3.511659\n1st Quartile:   -0.642621\nMedian:         0.033517\n3rd Quartile:   0.657467\nMaximum:        4.913423\nLength:         6670\nType:           Float64\nNumber Missing: 0\n% Missing:      0.000000\n\nTrait3\nSummary Stats:\nMean:           -0.001296\nMinimum:        -3.938436\n1st Quartile:   -0.640907\nMedian:         -0.000782\n3rd Quartile:   0.637108\nMaximum:        7.916299\nLength:         6670\nType:           Float64\nNumber Missing: 0\n% Missing:      0.000000\n\nTrait4\nSummary Stats:\nMean:           0.002309\nMinimum:        -3.608403\n1st Quartile:   -0.546086\nMedian:         0.228165\n3rd Quartile:   0.715291\nMaximum:        3.127688\nLength:         6670\nType:           Float64\nNumber Missing: 0\n% Missing:      0.000000\n\nTrait5\nSummary Stats:\nMean:           -0.001790\nMinimum:        -4.148749\n1st Quartile:   -0.690765\nMedian:         0.031034\n3rd Quartile:   0.734916\nMaximum:        2.717184\nLength:         6670\nType:           Float64\nNumber Missing: 0\n% Missing:      0.000000\n\nTrait6\nSummary Stats:\nMean:           -0.001196\nMinimum:        -3.824792\n1st Quartile:   -0.662796\nMedian:         0.036242\n3rd Quartile:   0.741176\nMaximum:        2.589728\nLength:         6670\nType:           Float64\nNumber Missing: 0\n% Missing:      0.000000\n\nTrait7\nSummary Stats:\nMean:           -0.001989\nMinimum:        -4.272455\n1st Quartile:   -0.638923\nMedian:         0.069801\n3rd Quartile:   0.710423\nMaximum:        2.653779\nLength:         6670\nType:           Float64\nNumber Missing: 0\n% Missing:      0.000000\n\nTrait8\nSummary Stats:\nMean:           0.000614\nMinimum:        -5.625488\n1st Quartile:   -0.601575\nMedian:         -0.038630\n3rd Quartile:   0.527342\nMaximum:        5.805702\nLength:         6670\nType:           Float64\nNumber Missing: 0\n% Missing:      0.000000\n\nTrait9\nSummary Stats:\nMean:           -0.001810\nMinimum:        -5.381968\n1st Quartile:   -0.601429\nMedian:         0.106571\n3rd Quartile:   0.698567\nMaximum:        2.571936\nLength:         6670\nType:           Float64\nNumber Missing: 0\n% Missing:      0.000000\n\nTrait10\nSummary Stats:\nMean:           -0.000437\nMinimum:        -3.548506\n1st Quartile:   -0.633641\nMedian:         -0.096651\n3rd Quartile:   0.498610\nMaximum:        6.537820\nLength:         6670\nType:           Float64\nNumber Missing: 0\n% Missing:      0.000000\n\nTrait11\nSummary Stats:\nMean:           -0.000616\nMinimum:        -3.264910\n1st Quartile:   -0.673685\nMedian:         -0.068044\n3rd Quartile:   0.655486\nMaximum:        4.262410\nLength:         6670\nType:           Float64\nNumber Missing: 0\n% Missing:      0.000000\n\nTrait12\nSummary Stats:\nMean:           -0.000589\nMinimum:        -8.851909\n1st Quartile:   -0.539686\nMedian:         -0.141099\n3rd Quartile:   0.350779\nMaximum:        13.211402\nLength:         6670\nType:           Float64\nNumber Missing: 0\n% Missing:      0.000000\n\nTrait13\nSummary Stats:\nMean:           -0.000151\nMinimum:        -5.592104\n1st Quartile:   -0.492289\nMedian:         -0.141022\n3rd Quartile:   0.324804\nMaximum:        24.174436\nLength:         6670\nType:           Float64\nNumber Missing: 0\n% Missing:      0.000000\n\n\n\n\n\nY\n \n=\n \nconvert\n(\nMatrix\n{\nFloat64\n},\n \ncg10k_trait\n[\n:\n,\n \n3\n:\n15\n])\n\n\nhistogram\n(\nY\n,\n \nlayout\n \n=\n \n13\n)\n\n\n\n\n\n\n\n\n\n\nPre-processing data for heritability analysis\n\n\nTo prepare variance component model fitting, we form an instance of \nVarianceComponentVariate\n. The two variance components are $(2\\Phi, I)$.\n\n\nusing\n \nVarianceComponentModels\n\n\n\n# form data as VarianceComponentVariate\n\n\ncg10kdata\n \n=\n \nVarianceComponentVariate\n(\nY\n,\n \n(\n2\n\u03a6grm\n,\n \neye\n(\nsize\n(\nY\n,\n \n1\n))))\n\n\nfieldnames\n(\ncg10kdata\n)\n\n\n\n\n\n\n3-element Array{Symbol,1}:\n :Y\n :X\n :V\n\n\n\n\n\ncg10kdata\n\n\n\n\n\n\nVarianceComponentModels.VarianceComponentVariate{Float64,2,Array{Float64,2},Array{Float64,2},Array{Float64,2}}([-1.81573 -0.94615 \u2026 -1.02853 -0.394049; -1.2444 0.10966 \u2026 1.09065 0.0256616; \u2026 ; 0.886626 0.487408 \u2026 -0.636874 -0.439825; -1.24394 0.213697 \u2026 0.299931 0.392809], Array{Float64}(6670,0), ([1.00605 0.00671009 \u2026 -0.000109037 -0.00556144; 0.00671009 0.997916 \u2026 0.00173694 0.00685701; \u2026 ; -0.000109037 0.00173694 \u2026 1.00166 0.000938955; -0.00556144 0.00685701 \u2026 0.000938955 1.00125], [1.0 0.0 \u2026 0.0 0.0; 0.0 1.0 \u2026 0.0 0.0; \u2026 ; 0.0 0.0 \u2026 1.0 0.0; 0.0 0.0 \u2026 0.0 1.0]))\n\n\n\n\n\nBefore fitting the variance component model, we pre-compute the eigen-decomposition of $2\\Phi_{\\text{GRM}}$, the rotated responses, and the constant part in log-likelihood, and store them as a \nTwoVarCompVariateRotate\n instance, which is re-used in various variane component estimation procedures.\n\n\n# pre-compute eigen-decomposition (~50 secs on my laptop)\n\n\n@time\n \ncg10kdata_rotated\n \n=\n \nTwoVarCompVariateRotate\n(\ncg10kdata\n)\n\n\nfieldnames\n(\ncg10kdata_rotated\n)\n\n\n\n\n\n\n 48.837361 seconds (39 allocations: 1021.427 MiB, 0.57% gc time)\n\n\n\n\n\n5-element Array{Symbol,1}:\n :Yrot    \n :Xrot    \n :eigval  \n :eigvec  \n :logdetV2\n\n\n\n\n\n\n\nSave intermediate results\n\n\nWe don't want to re-compute SnpArray and empirical kinship matrices again and again for heritibility analysis.\n\n\n# # Pkg.add(\nJLD\n)\n\n\n# using JLD\n\n\n# @save \ncg10k.jld\n\n\n# whos()\n\n\n\n\n\n\nTo load workspace\n\n\nusing\n \nSnpArrays\n,\n \nJLD\n,\n \nDataFrames\n,\n \nVarianceComponentModels\n,\n \nPlots\n\n\npyplot\n()\n\n\n@load\n \ncg10k.jld\n\n\nwhos\n()\n\n\n\n\n\n\n                          Base               Module\n                       BinDeps  41348 KB     Module\n                         Blosc  41202 KB     Module\n                    ColorTypes  41457 KB     Module\n                        Colors  41480 KB     Module\n                        Compat  41196 KB     Module\n                         Conda  41205 KB     Module\n                          Core               Module\n                    DataArrays  41456 KB     Module\n                    DataFrames  41684 KB     Module\n                DataStructures  41356 KB     Module\n                        FileIO  41310 KB     Module\n             FixedPointNumbers  41695 KB     Module\n                          GZip  41181 KB     Module\n                          HDF5  41403 KB     Module\n                        IJulia 4185781 KB     Module\n                         Ipopt  41172 KB     Module\n                           JLD  41376 KB     Module\n                          JSON  41245 KB     Module\n                  LaTeXStrings   4058 bytes  Module\n                 LegacyStrings  41212 KB     Module\n                    LinearMaps     22 KB     Module\n                    MacroTools  41606 KB     Module\n                          Main               Module\n                  MathProgBase  41353 KB     Module\n                       MbedTLS  41269 KB     Module\n                      Measures  41175 KB     Module\n                       NaNMath  41200 KB     Module\n                    PlotThemes  41167 KB     Module\n                     PlotUtils  41332 KB     Module\n                         Plots  42960 KB     Module\n                        PyCall  41711 KB     Module\n                        PyPlot  41771 KB     Module\n                   RecipesBase  41283 KB     Module\n                      Reexport  41160 KB     Module\n                      Requires  41172 KB     Module\n                           SHA     62 KB     Module\n                       Showoff  41163 KB     Module\n                     SnpArrays  41218 KB     Module\n             SortingAlgorithms  41178 KB     Module\n              SpecialFunctions  41252 KB     Module\n                  StaticArrays  41744 KB     Module\n                     StatsBase  41810 KB     Module\n                     URIParser  41171 KB     Module\n       VarianceComponentModels  41278 KB     Module\n                             Y    677 KB     6670\u00d713 Array{Float64,2}\n                           ZMQ  41223 KB     Module\n                             _     77 KB     630860-element BitArray{1}\n                         cg10k 1027303 KB     6670\u00d7630860 SnpArrays.SnpArray{2}\n                   cg10k_trait    978 KB     6670\u00d715 DataFrames.DataFrame\n                     cg10kdata 695816 KB     VarianceComponentModels.VarianceCo\u2026\n             cg10kdata_rotated 348299 KB     VarianceComponentModels.TwoVarComp\u2026\n                             h     24 bytes  3-element Array{Float64,1}\n                           hST    104 bytes  13-element Array{Float64,1}\n                        hST_se    104 bytes  13-element Array{Float64,1}\n                           hse     24 bytes  3-element Array{Float64,1}\n                           maf   4928 KB     630860-element Array{Float64,1}\n               missings_by_snp   4928 KB     630860-element Array{Int64,1}\n                        people      8 bytes  Int64\n                          snps      8 bytes  Int64\n                  trait57_data 347778 KB     VarianceComponentModels.TwoVarComp\u2026\n                 trait57_model    232 bytes  VarianceComponentModels.VarianceCo\u2026\n                traitall_model   2792 bytes  VarianceComponentModels.VarianceCo\u2026\n                      traitidx     16 bytes  3-element UnitRange{Int64}\n                            \u03a3a   3848 bytes  13\u00d713 Array{Array{Float64,2},2}\n                          \u03a3cov   2592 bytes  18\u00d718 Array{Float64,2}\n                            \u03a3e   3848 bytes  13\u00d713 Array{Array{Float64,2},2}\n                          \u03a6grm 347569 KB     6670\u00d76670 Array{Float64,2}\n                           \u03c32a    104 bytes  13-element Array{Float64,1}\n                           \u03c32e    104 bytes  13-element Array{Float64,1}\n\n\n\n\n\n\n\nHeritability of single traits\n\n\nWe use Fisher scoring algorithm to fit variance component model for each single trait.\n\n\n# heritability from single trait analysis\n\n\nhST\n \n=\n \nzeros\n(\n13\n)\n\n\n# standard errors of estimated heritability\n\n\nhST_se\n \n=\n \nzeros\n(\n13\n)\n\n\n# additive genetic effects\n\n\n\u03c32a\n \n=\n \nzeros\n(\n13\n)\n\n\n# enviromental effects\n\n\n\u03c32e\n \n=\n \nzeros\n(\n13\n)\n\n\n\ntic\n()\n\n\nfor\n \ntrait\n \nin\n \n1\n:\n13\n\n    \nprintln\n(\nnames\n(\ncg10k_trait\n)[\ntrait\n \n+\n \n2\n])\n\n    \n# form data set for trait j\n\n    \ntraitj_data\n \n=\n \nTwoVarCompVariateRotate\n(\ncg10kdata_rotated\n.\nYrot\n[\n:\n,\n \ntrait\n],\n \ncg10kdata_rotated\n.\nXrot\n,\n \n        \ncg10kdata_rotated\n.\neigval\n,\n \ncg10kdata_rotated\n.\neigvec\n,\n \ncg10kdata_rotated\n.\nlogdetV2\n)\n\n    \n# initialize model parameters\n\n    \ntraitj_model\n \n=\n \nVarianceComponentModel\n(\ntraitj_data\n)\n\n    \n# estimate variance components\n\n    \n_\n,\n \n_\n,\n \n_\n,\n \n\u03a3cov\n,\n \n_\n,\n \n_\n \n=\n \nmle_fs!\n(\ntraitj_model\n,\n \ntraitj_data\n;\n \nsolver\n=:\nIpopt\n,\n \nverbose\n=\nfalse\n)\n\n    \n\u03c32a\n[\ntrait\n]\n \n=\n \ntraitj_model\n.\n\u03a3\n[\n1\n][\n1\n]\n\n    \n\u03c32e\n[\ntrait\n]\n \n=\n \ntraitj_model\n.\n\u03a3\n[\n2\n][\n1\n]\n\n    \n@show\n \n\u03c32a\n[\ntrait\n],\n \n\u03c32e\n[\ntrait\n]\n\n    \nh\n,\n \nhse\n \n=\n \nheritability\n(\ntraitj_model\n.\n\u03a3\n,\n \n\u03a3cov\n)\n\n    \nhST\n[\ntrait\n]\n \n=\n \nh\n[\n1\n]\n\n    \nhST_se\n[\ntrait\n]\n \n=\n \nhse\n[\n1\n]\n\n\nend\n\n\ntoc\n()\n\n\n\n\n\n\nTrait1\n(\u03c32a[trait], \u03c32e[trait]) = (0.25978160614793233, 0.7369535197912689)\nTrait2\n(\u03c32a[trait], \u03c32e[trait]) = (0.18647130348299173, 0.8129591079735827)\nTrait3\n(\u03c32a[trait], \u03c32e[trait]) = (0.3188368159422607, 0.6798809726936244)\nTrait4\n(\u03c32a[trait], \u03c32e[trait]) = (0.2651357653143703, 0.7308007669086968)\nTrait5\n(\u03c32a[trait], \u03c32e[trait]) = (0.28083388108246, 0.7172036435586534)\nTrait6\n(\u03c32a[trait], \u03c32e[trait]) = (0.2824159905728832, 0.7170988773569172)\nTrait7\n(\u03c32a[trait], \u03c32e[trait]) = (0.2155274336968625, 0.7815346282986375)\nTrait8\n(\u03c32a[trait], \u03c32e[trait]) = (0.194687807263945, 0.8049690651320599)\nTrait9\n(\u03c32a[trait], \u03c32e[trait]) = (0.24706855916591713, 0.7512942998567308)\nTrait10\n(\u03c32a[trait], \u03c32e[trait]) = (0.098712236297271, 0.9011756660217387)\nTrait11\n(\u03c32a[trait], \u03c32e[trait]) = (0.1664264642608195, 0.8322427413046204)\nTrait12\n(\u03c32a[trait], \u03c32e[trait]) = (0.0834296761650666, 0.9153609794266608)\nTrait13\n(\u03c32a[trait], \u03c32e[trait]) = (0.05893968504298988, 0.940270012443928)\nelapsed time: 0.160999612 seconds\n\n\n\n\n\n0.160999612\n\n\n\n\n\n# heritability and standard errors\n\n\n[\nhST\n;\n \nhST_se\n]\n\n\n\n\n\n\n2\u00d713 Array{Float64,2}:\n 0.260633   0.186578   0.319246   \u2026  0.166648  0.0835307  0.0589863\n 0.0799732  0.0869002  0.0741007     0.08862   0.0944407  0.0953238\n\n\n\n\n\n\n\nPairwise traits\n\n\nJoint analysis of multiple traits is subject to intensive research recently. Following code snippet does joint analysis of all pairs of traits, a total of 78 bivariate variane component models.\n\n\n# additive genetic effects (2x2 psd matrices) from bavariate trait analysis;\n\n\n\u03a3a\n \n=\n \nArray\n{\nMatrix\n{\nFloat64\n}}(\n13\n,\n \n13\n)\n\n\n# environmental effects (2x2 psd matrices) from bavariate trait analysis;\n\n\n\u03a3e\n \n=\n \nArray\n{\nMatrix\n{\nFloat64\n}}(\n13\n,\n \n13\n)\n\n\n\ntic\n()\n\n\nfor\n \ni\n \nin\n \n1\n:\n13\n\n    \nfor\n \nj\n \nin\n \n(\ni\n+\n1\n)\n:\n13\n\n        \nprintln\n(\nnames\n(\ncg10k_trait\n)[\ni\n \n+\n \n2\n],\n \nnames\n(\ncg10k_trait\n)[\nj\n \n+\n \n2\n])\n\n        \n# form data set for (trait1, trait2)\n\n        \ntraitij_data\n \n=\n \nTwoVarCompVariateRotate\n(\ncg10kdata_rotated\n.\nYrot\n[\n:\n,\n \n[\ni\n;\nj\n]],\n \ncg10kdata_rotated\n.\nXrot\n,\n \n            \ncg10kdata_rotated\n.\neigval\n,\n \ncg10kdata_rotated\n.\neigvec\n,\n \ncg10kdata_rotated\n.\nlogdetV2\n)\n\n        \n# initialize model parameters\n\n        \ntraitij_model\n \n=\n \nVarianceComponentModel\n(\ntraitij_data\n)\n\n        \n# estimate variance components\n\n        \nmle_fs!\n(\ntraitij_model\n,\n \ntraitij_data\n;\n \nsolver\n=:\nIpopt\n,\n \nverbose\n=\nfalse\n)\n\n        \n\u03a3a\n[\ni\n,\n \nj\n]\n \n=\n \ntraitij_model\n.\n\u03a3\n[\n1\n]\n\n        \n\u03a3e\n[\ni\n,\n \nj\n]\n \n=\n \ntraitij_model\n.\n\u03a3\n[\n2\n]\n\n        \n@show\n \n\u03a3a\n[\ni\n,\n \nj\n],\n \n\u03a3e\n[\ni\n,\n \nj\n]\n\n    \nend\n\n\nend\n\n\ntoc\n()\n\n\n\n\n\n\nTrait1Trait2\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.258822 0.174358; 0.174358 0.185108], [0.737892 0.585751; 0.585751 0.814301])\nTrait1Trait3\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.260236 -0.0144726; -0.0144726 0.319245], [0.736512 -0.11979; -0.11979 0.679488])\nTrait1Trait4\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.259615 0.222203; 0.222203 0.265149], [0.737116 0.599854; 0.599854 0.730788])\nTrait1Trait5\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.259574 -0.146827; -0.146827 0.28153], [0.737153 -0.254777; -0.254777 0.71653])\nTrait1Trait6\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.259476 -0.129115; -0.129115 0.282688], [0.73725 -0.23161; -0.23161 0.716837])\nTrait1Trait7\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.259115 -0.140455; -0.140455 0.215297], [0.737606 -0.197616; -0.197616 0.781774])\nTrait1Trait8\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.259778 -0.0327756; -0.0327756 0.194698], [0.736957 -0.127026; -0.127026 0.804959])\nTrait1Trait9\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.261858 -0.204589; -0.204589 0.246027], [0.734961 -0.307734; -0.307734 0.75232])\nTrait1Trait10\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.259649 -0.0994858; -0.0994858 0.0956585], [0.737083 -0.303942; -0.303942 0.904218])\nTrait1Trait11\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.25947 -0.138603; -0.138603 0.164709], [0.737257 -0.359557; -0.359557 0.83395])\nTrait1Trait12\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.261779 -0.145414; -0.145414 0.0807748], [0.735076 -0.041823; -0.041823 0.9181])\nTrait1Trait13\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.261125 -0.108774; -0.108774 0.0538214], [0.735674 -0.114123; -0.114123 0.945416])\nTrait2Trait3\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.186541 0.144056; 0.144056 0.320627], [0.812888 0.0995944; 0.0995944 0.678167])\nTrait2Trait4\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.186131 0.0746032; 0.0746032 0.265122], [0.813293 0.221109; 0.221109 0.730814])\nTrait2Trait5\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.186442 -0.0118093; -0.0118093 0.280842], [0.812987 -0.0365191; -0.0365191 0.717195])\nTrait2Trait6\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.18649 -0.00366533; -0.00366533 0.282471], [0.812941 -0.0206271; -0.0206271 0.717046])\nTrait2Trait7\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.186104 -0.030665; -0.030665 0.215304], [0.81332 -0.000667009; -0.000667009 0.781755])\nTrait2Trait8\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.187023 0.0331783; 0.0331783 0.195259], [0.812421 -0.0326343; -0.0326343 0.804415])\nTrait2Trait9\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.185032 -0.085334; -0.085334 0.245909], [0.814386 -0.0809638; -0.0809638 0.752433])\nTrait2Trait10\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.186587 -0.123303; -0.123303 0.0987387], [0.812872 -0.273083; -0.273083 0.901229])\nTrait2Trait11\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.185484 -0.117256; -0.117256 0.167776], [0.81393 -0.296772; -0.296772 0.830934])\nTrait2Trait12\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.185907 -0.0909104; -0.0909104 0.0827171], [0.813555 0.0457924; 0.0457924 0.916135])\nTrait2Trait13\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.185979 -0.0720811; -0.0720811 0.0568238], [0.8135 0.0751703; 0.0751703 0.942424])\nTrait3Trait4\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.3188 -0.154562; -0.154562 0.264323], [0.679917 -0.303223; -0.303223 0.731591])\nTrait3Trait5\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.319216 0.183527; 0.183527 0.282063], [0.679514 0.33724; 0.33724 0.716008])\nTrait3Trait6\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.319776 0.165672; 0.165672 0.284448], [0.678972 0.298667; 0.298667 0.715124])\nTrait3Trait7\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.318838 0.166283; 0.166283 0.215261], [0.67988 0.347706; 0.347706 0.781796])\nTrait3Trait8\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.320718 0.0566397; 0.0566397 0.197764], [0.678063 0.0451569; 0.0451569 0.801956])\nTrait3Trait9\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.319001 0.137699; 0.137699 0.246142], [0.679722 0.266704; 0.266704 0.752197])\nTrait3Trait10\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.31908 -0.076513; -0.076513 0.0996001], [0.679646 -0.142905; -0.142905 0.900298])\nTrait3Trait11\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.318094 -0.0177494; -0.0177494 0.16629], [0.6806 -0.1144; -0.1144 0.832376])\nTrait3Trait12\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.321164 0.0843842; 0.0843842 0.0874609], [0.677639 0.0341558; 0.0341558 0.911368])\nTrait3Trait13\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.323273 0.109443; 0.109443 0.0634295], [0.675635 -0.0060525; -0.0060525 0.935819])\nTrait4Trait5\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.26525 -0.215125; -0.215125 0.282572], [0.73068 -0.377406; -0.377406 0.715518])\nTrait4Trait6\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.265715 -0.199714; -0.199714 0.283942], [0.730231 -0.347732; -0.347732 0.715619])\nTrait4Trait7\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.26407 -0.18238; -0.18238 0.214324], [0.731843 -0.32655; -0.32655 0.782733])\nTrait4Trait8\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.266229 -0.0965381; -0.0965381 0.196655], [0.729739 -0.151461; -0.151461 0.803044])\nTrait4Trait9\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.269627 -0.226931; -0.226931 0.247265], [0.726443 -0.416085; -0.416085 0.751086])\nTrait4Trait10\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.265098 -0.0352926; -0.0352926 0.0981462], [0.730847 -0.226248; -0.226248 0.901736])\nTrait4Trait11\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.265178 -0.0970634; -0.0970634 0.164885], [0.73076 -0.272291; -0.272291 0.833762])\nTrait4Trait12\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.267732 -0.140985; -0.140985 0.081029], [0.728323 -0.0834791; -0.0834791 0.917815])\nTrait4Trait13\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.265695 -0.0970238; -0.0970238 0.0564809], [0.730259 -0.226115; -0.226115 0.942736])\nTrait5Trait6\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.281198 0.280259; 0.280259 0.281764], [0.716855 0.661013; 0.661013 0.717735])\nTrait5Trait7\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.280442 0.231918; 0.231918 0.211837], [0.717597 0.674491; 0.674491 0.785172])\nTrait5Trait8\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.280958 0.163168; 0.163168 0.193315], [0.717089 0.221817; 0.221817 0.806314])\nTrait5Trait9\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.283544 0.243884; 0.243884 0.240564], [0.714585 0.509072; 0.509072 0.757631])\nTrait5Trait10\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.281378 -0.0454427; -0.0454427 0.100081], [0.716678 -0.0579778; -0.0579778 0.899822])\nTrait5Trait11\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.280066 0.0195669; 0.0195669 0.165607], [0.71795 -0.0345589; -0.0345589 0.833047])\nTrait5Trait12\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.28101 0.0592641; 0.0592641 0.0831831], [0.717036 0.0552788; 0.0552788 0.915608])\nTrait5Trait13\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.281854 0.0680641; 0.0680641 0.0591899], [0.716223 0.0551992; 0.0551992 0.940027])\nTrait6Trait7\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.282435 0.220236; 0.220236 0.213997], [0.71708 0.581507; 0.581507 0.783041])\nTrait6Trait8\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.282435 0.18375; 0.18375 0.192999], [0.717081 0.436932; 0.436932 0.80663])\nTrait6Trait9\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.284516 0.233768; 0.233768 0.242478], [0.715071 0.477502; 0.477502 0.755765])\nTrait6Trait10\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.283087 -0.0427658; -0.0427658 0.100634], [0.716449 -0.0599491; -0.0599491 0.899275])\nTrait6Trait11\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.281046 0.0272144; 0.0272144 0.165044], [0.71843 -0.0516242; -0.0516242 0.833601])\nTrait6Trait12\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.28256 0.0548537; 0.0548537 0.083133], [0.716961 0.0502064; 0.0502064 0.915658])\nTrait6Trait13\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.283231 0.0585667; 0.0585667 0.0592752], [0.716314 0.055827; 0.055827 0.939942])\nTrait7Trait8\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.213998 0.0875641; 0.0875641 0.192993], [0.78304 -0.055939; -0.055939 0.806635])\nTrait7Trait9\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.219039 0.216925; 0.216925 0.243338], [0.778156 0.463024; 0.463024 0.754935])\nTrait7Trait10\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.216296 -0.0412106; -0.0412106 0.100663], [0.780785 -0.0868086; -0.0868086 0.899246])\nTrait7Trait11\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.2142 0.0204227; 0.0204227 0.165077], [0.782833 -0.0478727; -0.0478727 0.833569])\nTrait7Trait12\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.215054 0.0738562; 0.0738562 0.0814228], [0.782012 0.0366272; 0.0366272 0.917365])\nTrait7Trait13\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.216093 0.0728515; 0.0728515 0.0570272], [0.781006 0.0409945; 0.0409945 0.942189])\nTrait8Trait9\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.195154 0.111756; 0.111756 0.246453], [0.804528 0.185842; 0.185842 0.751896])\nTrait8Trait10\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.195015 -0.015506; -0.015506 0.0990776], [0.804651 0.0118538; 0.0118538 0.900815])\nTrait8Trait11\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.194421 0.0215044; 0.0215044 0.166226], [0.805231 -0.026247; -0.026247 0.83244])\nTrait8Trait12\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.194491 -0.00425152; -0.00425152 0.0832711], [0.805162 0.0349872; 0.0349872 0.915518])\nTrait8Trait13\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.19448 0.00235501; 0.00235501 0.0589351], [0.805173 0.0396048; 0.0396048 0.940275])\nTrait9Trait10\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.246455 -0.00257997; -0.00257997 0.0984563], [0.751895 0.0743439; 0.0743439 0.901429])\nTrait9Trait11\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.247001 0.0303415; 0.0303415 0.166421], [0.75136 0.153765; 0.153765 0.832248])\nTrait9Trait12\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.249421 0.0829968; 0.0829968 0.0890874], [0.749007 0.109331; 0.109331 0.909778])\nTrait9Trait13\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.24861 0.0916799; 0.0916799 0.0602352], [0.749811 0.100027; 0.100027 0.939032])\nTrait10Trait11\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.0914658 0.100613; 0.100613 0.166501], [0.908376 0.473847; 0.473847 0.83217])\nTrait10Trait12\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.0951392 0.0588424; 0.0588424 0.0796744], [0.904735 0.0828862; 0.0828862 0.919115])\nTrait10Trait13\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.0995192 -0.0257171; -0.0257171 0.0598595], [0.900397 0.163778; 0.163778 0.939368])\nTrait11Trait12\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.165386 0.0579914; 0.0579914 0.0796005], [0.83327 0.144637; 0.144637 0.919166])\nTrait11Trait13\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.166417 -0.000985185; -0.000985185 0.0595681], [0.832265 0.200012; 0.200012 0.939646])\nTrait12Trait13\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.085082 0.0696185; 0.0696185 0.0569655], [0.913729 0.572041; 0.572041 0.942247])\nelapsed time: 3.587337102 seconds\n\n\n\n\n\n3.587337102\n\n\n\n\n\n\n\n3-trait analysis\n\n\nResearchers want to jointly analyze traits 5-7. Our strategy is to try both Fisher scoring and MM algorithm with different starting point, and choose the best local optimum. We first form the data set and run Fisher scoring, which yields a final objective value -1.4700991+04.\n\n\ntraitidx\n \n=\n \n5\n:\n7\n\n\n# form data set\n\n\ntrait57_data\n \n=\n \nTwoVarCompVariateRotate\n(\ncg10kdata_rotated\n.\nYrot\n[\n:\n,\n \ntraitidx\n],\n \ncg10kdata_rotated\n.\nXrot\n,\n \n    \ncg10kdata_rotated\n.\neigval\n,\n \ncg10kdata_rotated\n.\neigvec\n,\n \ncg10kdata_rotated\n.\nlogdetV2\n)\n\n\n# initialize model parameters\n\n\ntrait57_model\n \n=\n \nVarianceComponentModel\n(\ntrait57_data\n)\n\n\n# estimate variance components\n\n\n@time\n \nmle_fs!\n(\ntrait57_model\n,\n \ntrait57_data\n;\n \nsolver\n=:\nIpopt\n,\n \nverbose\n=\ntrue\n)\n\n\ntrait57_model\n\n\n\n\n\n\nThis is Ipopt version 3.12.4, running with linear solver mumps.\nNOTE: Other linear solvers might be more efficient (see Ipopt documentation).\n\nNumber of nonzeros in equality constraint Jacobian...:        0\nNumber of nonzeros in inequality constraint Jacobian.:        0\nNumber of nonzeros in Lagrangian Hessian.............:       78\n\nTotal number of variables............................:       12\n                     variables with only lower bounds:        0\n                variables with lower and upper bounds:        0\n                     variables with only upper bounds:        0\nTotal number of equality constraints.................:        0\nTotal number of inequality constraints...............:        0\n        inequality constraints with only lower bounds:        0\n   inequality constraints with lower and upper bounds:        0\n        inequality constraints with only upper bounds:        0\n\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n   0  3.0247565e+04 0.00e+00 1.00e+02   0.0 0.00e+00    -  0.00e+00 0.00e+00   0 \n   5  1.6835078e+04 0.00e+00 4.08e+02 -11.0 3.64e-01    -  1.00e+00 1.00e+00f  1 MaxS\n  10  1.4742941e+04 0.00e+00 1.10e+02 -11.0 2.35e-01    -  1.00e+00 1.00e+00f  1 MaxS\n  15  1.4701394e+04 0.00e+00 1.16e+01 -11.0 7.78e-02  -4.5 1.00e+00 1.00e+00f  1 MaxS\n  20  1.4701019e+04 0.00e+00 5.75e-01 -11.0 1.51e-04  -6.9 1.00e+00 1.00e+00f  1 MaxS\n  25  1.4701018e+04 0.00e+00 2.40e-02 -11.0 6.38e-06  -9.2 1.00e+00 1.00e+00f  1 MaxS\n  30  1.4701018e+04 0.00e+00 9.98e-04 -11.0 2.66e-07 -11.6 1.00e+00 1.00e+00f  1 MaxS\n  35  1.4701018e+04 0.00e+00 4.15e-05 -11.0 1.10e-08 -14.0 1.00e+00 1.00e+00h  1 MaxS\n  40  1.4701018e+04 0.00e+00 1.72e-06 -11.0 4.59e-10 -16.4 1.00e+00 1.00e+00f  1 MaxSA\n  45  1.4701018e+04 0.00e+00 7.17e-08 -11.0 1.91e-11 -18.8 1.00e+00 1.00e+00h  1 MaxSA\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n\nNumber of Iterations....: 49\n\n                                   (scaled)                 (unscaled)\nObjective...............:   4.4720359684330265e+02    1.4701017692082147e+04\nDual infeasibility......:   5.6081357364421780e-09    1.8435742302386474e-07\nConstraint violation....:   0.0000000000000000e+00    0.0000000000000000e+00\nComplementarity.........:   0.0000000000000000e+00    0.0000000000000000e+00\nOverall NLP error.......:   5.6081357364421780e-09    1.8435742302386474e-07\n\n\nNumber of objective function evaluations             = 50\nNumber of objective gradient evaluations             = 50\nNumber of equality constraint evaluations            = 0\nNumber of inequality constraint evaluations          = 0\nNumber of equality constraint Jacobian evaluations   = 0\nNumber of inequality constraint Jacobian evaluations = 0\nNumber of Lagrangian Hessian evaluations             = 49\nTotal CPU secs in IPOPT (w/o function evaluations)   =      0.014\nTotal CPU secs in NLP function evaluations           =      0.076\n\nEXIT: Optimal Solution Found.\n  0.097955 seconds (55.15 k allocations: 5.632 MiB)\n\n\n\n\n\nVarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}(Array{Float64}(0,3), ([0.280777 0.279441 0.232208; 0.279441 0.28422 0.219831; 0.232208 0.219831 0.212832], [0.717266 0.66183 0.674206; 0.66183 0.715287 0.581891; 0.674206 0.581891 0.784183]), Array{Float64}(0,0), Char[], Float64[], -Inf, Inf)\n\n\n\n\n\nWe then run the MM algorithm, starting from the Fisher scoring answer. MM finds an improved solution with objective value 8.955397e+03.\n\n\n# trait59_model contains the fitted model by Fisher scoring now\n\n\n@time\n \nmle_mm!\n(\ntrait57_model\n,\n \ntrait57_data\n;\n \nverbose\n=\ntrue\n)\n\n\ntrait57_model\n\n\n\n\n\n\n     MM Algorithm\n  Iter      Objective  \n--------  -------------\n       0  -1.470102e+04\n       1  -1.470102e+04\n\n  0.003006 seconds (21.01 k allocations: 1.551 MiB)\n\n\n\n\n\nVarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}(Array{Float64}(0,3), ([0.280777 0.279441 0.232208; 0.279441 0.28422 0.219831; 0.232208 0.219831 0.212832], [0.717266 0.66183 0.674206; 0.66183 0.715287 0.581891; 0.674206 0.581891 0.784183]), Array{Float64}(0,0), Char[], Float64[], -Inf, Inf)\n\n\n\n\n\nDo another run of MM algorithm from default starting point. It leads to a slightly better local optimum -1.470104e+04, slighly worse than the Fisher scoring result. Follow up anlaysis should use the Fisher scoring result.\n\n\n# default starting point\n\n\ntrait57_model\n \n=\n \nVarianceComponentModel\n(\ntrait57_data\n)\n\n\n@time\n \n_\n,\n \n_\n,\n \n_\n,\n \n\u03a3cov\n,\n \n=\n \nmle_mm!\n(\ntrait57_model\n,\n \ntrait57_data\n;\n \nverbose\n=\ntrue\n)\n\n\ntrait57_model\n\n\n\n\n\n\n     MM Algorithm\n  Iter      Objective  \n--------  -------------\n       0  -3.024757e+04\n       1  -2.040300e+04\n       2  -1.656070e+04\n       3  -1.528529e+04\n       4  -1.490986e+04\n       5  -1.480638e+04\n       6  -1.477811e+04\n       7  -1.476968e+04\n       8  -1.476639e+04\n       9  -1.476444e+04\n      10  -1.476286e+04\n      20  -1.475000e+04\n      30  -1.474011e+04\n      40  -1.473248e+04\n      50  -1.472658e+04\n      60  -1.472200e+04\n      70  -1.471840e+04\n      80  -1.471555e+04\n      90  -1.471328e+04\n     100  -1.471145e+04\n     110  -1.470997e+04\n     120  -1.470875e+04\n     130  -1.470775e+04\n     140  -1.470691e+04\n     150  -1.470621e+04\n     160  -1.470562e+04\n     170  -1.470511e+04\n     180  -1.470469e+04\n     190  -1.470432e+04\n     200  -1.470400e+04\n     210  -1.470372e+04\n     220  -1.470348e+04\n     230  -1.470326e+04\n     240  -1.470308e+04\n     250  -1.470291e+04\n     260  -1.470276e+04\n     270  -1.470263e+04\n     280  -1.470251e+04\n     290  -1.470241e+04\n     300  -1.470231e+04\n     310  -1.470223e+04\n     320  -1.470215e+04\n     330  -1.470208e+04\n     340  -1.470201e+04\n     350  -1.470195e+04\n     360  -1.470190e+04\n     370  -1.470185e+04\n     380  -1.470180e+04\n     390  -1.470176e+04\n     400  -1.470172e+04\n     410  -1.470168e+04\n     420  -1.470165e+04\n     430  -1.470162e+04\n     440  -1.470159e+04\n     450  -1.470156e+04\n     460  -1.470153e+04\n     470  -1.470151e+04\n     480  -1.470149e+04\n     490  -1.470147e+04\n     500  -1.470145e+04\n     510  -1.470143e+04\n     520  -1.470141e+04\n     530  -1.470139e+04\n     540  -1.470138e+04\n     550  -1.470136e+04\n     560  -1.470135e+04\n     570  -1.470133e+04\n     580  -1.470132e+04\n     590  -1.470131e+04\n     600  -1.470130e+04\n     610  -1.470129e+04\n     620  -1.470128e+04\n     630  -1.470127e+04\n     640  -1.470126e+04\n     650  -1.470125e+04\n     660  -1.470124e+04\n     670  -1.470123e+04\n     680  -1.470122e+04\n     690  -1.470122e+04\n     700  -1.470121e+04\n     710  -1.470120e+04\n     720  -1.470120e+04\n     730  -1.470119e+04\n     740  -1.470118e+04\n     750  -1.470118e+04\n     760  -1.470117e+04\n     770  -1.470117e+04\n     780  -1.470116e+04\n     790  -1.470116e+04\n     800  -1.470115e+04\n     810  -1.470115e+04\n     820  -1.470114e+04\n     830  -1.470114e+04\n     840  -1.470114e+04\n     850  -1.470113e+04\n     860  -1.470113e+04\n     870  -1.470112e+04\n     880  -1.470112e+04\n     890  -1.470112e+04\n     900  -1.470111e+04\n     910  -1.470111e+04\n     920  -1.470111e+04\n     930  -1.470111e+04\n     940  -1.470110e+04\n     950  -1.470110e+04\n     960  -1.470110e+04\n     970  -1.470109e+04\n     980  -1.470109e+04\n     990  -1.470109e+04\n    1000  -1.470109e+04\n    1010  -1.470109e+04\n    1020  -1.470108e+04\n    1030  -1.470108e+04\n    1040  -1.470108e+04\n    1050  -1.470108e+04\n    1060  -1.470108e+04\n    1070  -1.470107e+04\n    1080  -1.470107e+04\n    1090  -1.470107e+04\n    1100  -1.470107e+04\n    1110  -1.470107e+04\n    1120  -1.470107e+04\n\n  0.794377 seconds (168.12 k allocations: 15.640 MiB, 0.80% gc time)\n\n\n\n\n\nVarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}(Array{Float64}(0,3), ([0.2808 0.279454 0.232256; 0.279454 0.284312 0.219977; 0.232256 0.219977 0.213052], [0.717243 0.661816 0.674158; 0.661816 0.715193 0.581746; 0.674158 0.581746 0.783965]), Array{Float64}(0,0), Char[], Float64[], -Inf, Inf)\n\n\n\n\n\nHeritability from 3-variate estimate and their standard errors.\n\n\nh\n,\n \nhse\n \n=\n \nheritability\n(\ntrait57_model\n.\n\u03a3\n,\n \n\u03a3cov\n)\n\n\n[\nh\n;\n \nhse\n]\n\n\n\n\n\n\n2\u00d73 Array{Float64,2}:\n 0.281351   0.284453  0.213689\n 0.0778252  0.077378  0.084084\n\n\n\n\n\n\n\n13-trait joint analysis\n\n\nIn some situations, such as studying the genetic covariance, we need to jointly analyze 13 traits. We first try the \nFisher scoring algorithm\n.\n\n\n# initialize model parameters\n\n\ntraitall_model\n \n=\n \nVarianceComponentModel\n(\ncg10kdata_rotated\n)\n\n\n# estimate variance components using Fisher scoring algorithm\n\n\n@time\n \nmle_fs!\n(\ntraitall_model\n,\n \ncg10kdata_rotated\n;\n \nsolver\n=:\nIpopt\n,\n \nverbose\n=\ntrue\n)\n\n\n\n\n\n\nThis is Ipopt version 3.12.4, running with linear solver mumps.\nNOTE: Other linear solvers might be more efficient (see Ipopt documentation).\n\nNumber of nonzeros in equality constraint Jacobian...:        0\nNumber of nonzeros in inequality constraint Jacobian.:        0\nNumber of nonzeros in Lagrangian Hessian.............:    16653\n\nTotal number of variables............................:      182\n                     variables with only lower bounds:        0\n                variables with lower and upper bounds:        0\n                     variables with only upper bounds:        0\nTotal number of equality constraints.................:        0\nTotal number of inequality constraints...............:        0\n        inequality constraints with only lower bounds:        0\n   inequality constraints with lower and upper bounds:        0\n        inequality constraints with only upper bounds:        0\n\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n   0  1.3113371e+05 0.00e+00 1.00e+02   0.0 0.00e+00    -  0.00e+00 0.00e+00   0 \n   5  8.2233766e+04 0.00e+00 6.03e+02 -11.0 2.32e+00    -  1.00e+00 1.00e+00f  1 MaxS\n  10  1.1960260e+05 0.00e+00 8.76e+02 -11.0 6.20e+01  -5.4 1.00e+00 1.00e+00h  1 MaxS\n  15  2.4416551e+05 0.00e+00 2.50e+02 -11.0 8.69e+02  -7.8 1.00e+00 1.00e+00f  1 MaxS\n\n\n\nDomainError:\nlog will only return a complex result if called with a complex argument. Try log(complex(x)).\n\n\n\nStacktrace:\n\n [1] nan_dom_err at ./math.jl:300 [inlined]\n\n [2] log at ./math.jl:419 [inlined]\n\n [3] logdet(::Array{Float64,2}) at ./linalg/generic.jl:1244\n\n [4] VarianceComponentModels.TwoVarCompModelRotate(::VarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}) at /Users/huazhou/.julia/v0.6/VarianceComponentModels/src/VarianceComponentModels.jl:127\n\n [5] eval_f(::VarianceComponentModels.TwoVarCompOptProb{VarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}},VarianceComponentModels.TwoVarCompVariateRotate{Float64,Array{Float64,2},Array{Float64,2}},Array{Float64,2},Array{Float64,1},VarianceComponentModels.VarianceComponentAuxData{Array{Float64,2},Array{Float64,1}}}, ::Array{Float64,1}) at /Users/huazhou/.julia/v0.6/VarianceComponentModels/src/two_variance_component.jl:683\n\n [6] (::Ipopt.#eval_f_cb#4{VarianceComponentModels.TwoVarCompOptProb{VarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}},VarianceComponentModels.TwoVarCompVariateRotate{Float64,Array{Float64,2},Array{Float64,2}},Array{Float64,2},Array{Float64,1},VarianceComponentModels.VarianceComponentAuxData{Array{Float64,2},Array{Float64,1}}}})(::Array{Float64,1}) at /Users/huazhou/.julia/v0.6/Ipopt/src/IpoptSolverInterface.jl:53\n\n [7] eval_f_wrapper(::Int32, ::Ptr{Float64}, ::Int32, ::Ptr{Float64}, ::Ptr{Void}) at /Users/huazhou/.julia/v0.6/Ipopt/src/Ipopt.jl:89\n\n [8] solveProblem(::Ipopt.IpoptProblem) at /Users/huazhou/.julia/v0.6/Ipopt/src/Ipopt.jl:304\n\n [9] optimize!(::Ipopt.IpoptMathProgModel) at /Users/huazhou/.julia/v0.6/Ipopt/src/IpoptSolverInterface.jl:120\n\n [10] #mle_fs!#29(::Int64, ::Symbol, ::Symbol, ::Bool, ::Function, ::VarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}, ::VarianceComponentModels.TwoVarCompVariateRotate{Float64,Array{Float64,2},Array{Float64,2}}) at /Users/huazhou/.julia/v0.6/VarianceComponentModels/src/two_variance_component.jl:893\n\n [11] (::VarianceComponentModels.#kw##mle_fs!)(::Array{Any,1}, ::VarianceComponentModels.#mle_fs!, ::VarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}, ::VarianceComponentModels.TwoVarCompVariateRotate{Float64,Array{Float64,2},Array{Float64,2}}) at ./\nmissing\n:0\n\n [12] include_string(::String, ::String) at ./loading.jl:515\n\n\n\n\n\nFrom the output we can see the Fisher scoring algorithm ran into some numerical issues. Let's try the \nMM algorithm\n.\n\n\n# reset model parameters\n\n\ntraitall_model\n \n=\n \nVarianceComponentModel\n(\ncg10kdata_rotated\n)\n\n\n# estimate variance components using Fisher scoring algorithm\n\n\n@time\n \nmle_mm!\n(\ntraitall_model\n,\n \ncg10kdata_rotated\n;\n \nverbose\n=\ntrue\n)\n\n\n\n\n\n\n     MM Algorithm\n  Iter      Objective  \n--------  -------------\n       0  -1.311337e+05\n       1  -8.002108e+04\n       2  -5.806935e+04\n       3  -4.926111e+04\n       4  -4.611059e+04\n       5  -4.511606e+04\n       6  -4.482679e+04\n       7  -4.474294e+04\n       8  -4.471496e+04\n       9  -4.470174e+04\n      10  -4.469246e+04\n      20  -4.462243e+04\n      30  -4.456888e+04\n      40  -4.452774e+04\n      50  -4.449601e+04\n      60  -4.447134e+04\n      70  -4.445199e+04\n      80  -4.443665e+04\n      90  -4.442436e+04\n     100  -4.441442e+04\n     110  -4.440630e+04\n     120  -4.439961e+04\n     130  -4.439405e+04\n     140  -4.438938e+04\n     150  -4.438544e+04\n     160  -4.438210e+04\n     170  -4.437923e+04\n     180  -4.437676e+04\n     190  -4.437463e+04\n     200  -4.437277e+04\n     210  -4.437115e+04\n     220  -4.436972e+04\n     230  -4.436846e+04\n     240  -4.436735e+04\n     250  -4.436636e+04\n     260  -4.436548e+04\n     270  -4.436469e+04\n     280  -4.436399e+04\n     290  -4.436335e+04\n     300  -4.436278e+04\n     310  -4.436226e+04\n     320  -4.436179e+04\n     330  -4.436137e+04\n     340  -4.436098e+04\n     350  -4.436063e+04\n     360  -4.436030e+04\n     370  -4.436001e+04\n     380  -4.435974e+04\n     390  -4.435949e+04\n     400  -4.435926e+04\n     410  -4.435905e+04\n     420  -4.435886e+04\n     430  -4.435868e+04\n     440  -4.435851e+04\n     450  -4.435836e+04\n     460  -4.435822e+04\n     470  -4.435809e+04\n     480  -4.435797e+04\n     490  -4.435785e+04\n     500  -4.435775e+04\n     510  -4.435765e+04\n     520  -4.435756e+04\n     530  -4.435747e+04\n     540  -4.435739e+04\n     550  -4.435732e+04\n     560  -4.435725e+04\n     570  -4.435718e+04\n     580  -4.435712e+04\n     590  -4.435706e+04\n     600  -4.435701e+04\n     610  -4.435696e+04\n     620  -4.435691e+04\n     630  -4.435687e+04\n     640  -4.435683e+04\n     650  -4.435679e+04\n     660  -4.435675e+04\n     670  -4.435671e+04\n     680  -4.435668e+04\n     690  -4.435665e+04\n     700  -4.435662e+04\n     710  -4.435659e+04\n     720  -4.435657e+04\n     730  -4.435654e+04\n     740  -4.435652e+04\n     750  -4.435649e+04\n     760  -4.435647e+04\n     770  -4.435645e+04\n     780  -4.435643e+04\n     790  -4.435642e+04\n     800  -4.435640e+04\n     810  -4.435638e+04\n     820  -4.435637e+04\n     830  -4.435635e+04\n     840  -4.435634e+04\n     850  -4.435633e+04\n     860  -4.435631e+04\n     870  -4.435630e+04\n     880  -4.435629e+04\n     890  -4.435628e+04\n     900  -4.435627e+04\n     910  -4.435626e+04\n     920  -4.435625e+04\n     930  -4.435624e+04\n     940  -4.435623e+04\n     950  -4.435622e+04\n     960  -4.435621e+04\n     970  -4.435621e+04\n     980  -4.435620e+04\n     990  -4.435619e+04\n    1000  -4.435619e+04\n    1010  -4.435618e+04\n    1020  -4.435617e+04\n    1030  -4.435617e+04\n    1040  -4.435616e+04\n    1050  -4.435616e+04\n    1060  -4.435615e+04\n    1070  -4.435615e+04\n    1080  -4.435614e+04\n    1090  -4.435614e+04\n\n  3.551301 seconds (178.42 k allocations: 70.115 MiB, 0.42% gc time)\n\n\n\n\n\n(-44356.138529861186, VarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}(Array{Float64}(0,13), ([0.272384 0.190358 \u2026 -0.128222 -0.0980655; 0.190358 0.21692 \u2026 -0.0689912 -0.0444349; \u2026 ; -0.128222 -0.0689912 \u2026 0.118227 0.0909188; -0.0980655 -0.0444349 \u2026 0.0909188 0.107456], [0.724562 0.56992 \u2026 -0.0590518 -0.124939; 0.56992 0.782639 \u2026 0.0238629 0.0475408; \u2026 ; -0.0590518 0.0238629 \u2026 0.880671 0.550889; -0.124939 0.0475408 \u2026 0.550889 0.891929]), Array{Float64}(0,0), Char[], Float64[], -Inf, Inf), ([0.0111619 0.0131088 \u2026 0.0128956 0.0127641; 0.0131091 0.0151759 \u2026 0.017162 0.0171466; \u2026 ; 0.0128956 0.017162 \u2026 0.0173994 0.0182002; 0.0127643 0.0171461 \u2026 0.0182003 0.0187848], [0.0112235 0.0133094 \u2026 0.0130111 0.0127861; 0.01331 0.0158262 \u2026 0.017867 0.017798; \u2026 ; 0.013011 0.0178666 \u2026 0.0179487 0.0187579; 0.012786 0.0177975 \u2026 0.0187578 0.0193328]), [0.000124587 7.24074e-5 \u2026 -3.35716e-7 -1.40982e-5; 7.24411e-5 0.000171849 \u2026 -2.05381e-5 -3.17975e-6; \u2026 ; -3.60221e-7 -2.05683e-5 \u2026 0.000351859 -1.5168e-5; -1.40799e-5 -3.16738e-6 \u2026 -1.51641e-5 0.000373756], Array{Float64}(0,13), Array{Float64}(0,0))\n\n\n\n\n\nIt converges after ~1000 iterations.\n\n\n\n\nSave analysis results\n\n\n#using JLD\n\n\n#@save \ncopd.jld\n\n\n#whos()", 
            "title": "Heritability"
        }, 
        {
            "location": "/man/heritability/#heritability-analysis", 
            "text": "As an application of the variance component model, this note demonstrates the workflow for heritability analysis in genetics, using a sample data set  cg10k  with  6,670  individuals and  630,860  SNPs. Person IDs and phenotype names are masked for privacy.  cg10k.bed ,  cg10k.bim , and  cg10k.fam  is a set of Plink files in binary format.  cg10k_traits.txt  contains 13 phenotypes of the 6,670 individuals.  ; ls   cg10k . bed   cg10k . bim   cg10k . fam   cg10k_traits . txt   cg10k.bed\ncg10k.bim\ncg10k.fam\ncg10k_traits.txt  Machine information:  versioninfo ()   Julia Version 0.6.0\nCommit 903644385b (2017-06-19 13:05 UTC)\nPlatform Info:\n  OS: macOS (x86_64-apple-darwin13.4.0)\n  CPU: Intel(R) Core(TM) i7-4790K CPU @ 4.00GHz\n  WORD_SIZE: 64\n  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Haswell)\n  LAPACK: libopenblas64_\n  LIBM: libopenlibm\n  LLVM: libLLVM-3.9.1 (ORCJIT, haswell)", 
            "title": "Heritability Analysis"
        }, 
        {
            "location": "/man/heritability/#read-in-binary-snp-data", 
            "text": "We will use the  SnpArrays.jl  package to read in binary SNP data and compute the empirical kinship matrix. Issue   Pkg . clone ( https://github.com/OpenMendel/SnpArrays.jl.git )   within  Julia  to install the  SnpArrays  package.  using   SnpArrays   # read in genotype data from Plink binary file (~50 secs on my laptop)  @time   cg10k   =   SnpArray ( cg10k )    22.902730 seconds (51.62 k allocations: 1005.845 MiB, 0.11% gc time)\n\n\n\n\n\n6670\u00d7630860 SnpArrays.SnpArray{2}:\n (false, true)   (false, true)   \u2026  (true, true)    (true, true) \n (true, true)    (true, true)       (false, true)   (true, false)\n (true, true)    (true, true)       (true, true)    (true, true) \n (true, true)    (true, true)       (false, true)   (true, true) \n (true, true)    (true, true)       (true, true)    (false, true)\n (false, true)   (false, true)   \u2026  (true, true)    (true, true) \n (false, false)  (false, false)     (true, true)    (true, true) \n (true, true)    (true, true)       (true, true)    (false, true)\n (true, true)    (true, true)       (true, true)    (true, true) \n (true, true)    (true, true)       (false, true)   (true, true) \n (true, true)    (true, true)    \u2026  (true, true)    (true, true) \n (false, true)   (false, true)      (true, true)    (false, true)\n (true, true)    (true, true)       (true, true)    (false, true)\n \u22ee                               \u22f1                               \n (false, true)   (false, true)      (false, true)   (false, true)\n (false, true)   (false, true)      (false, true)   (true, true) \n (true, true)    (true, true)    \u2026  (false, true)   (true, true) \n (false, true)   (false, true)      (true, true)    (false, true)\n (true, true)    (true, true)       (false, true)   (true, true) \n (true, true)    (true, true)       (false, false)  (false, true)\n (true, true)    (true, true)       (true, true)    (false, true)\n (true, true)    (true, true)    \u2026  (true, true)    (true, true) \n (true, true)    (true, true)       (false, true)   (true, true) \n (true, true)    (true, true)       (true, true)    (false, true)\n (false, true)   (false, true)      (true, true)    (true, true) \n (true, true)    (true, true)       (true, true)    (true, true)", 
            "title": "Read in binary SNP data"
        }, 
        {
            "location": "/man/heritability/#summary-statistics-of-snp-data", 
            "text": "people ,   snps   =   size ( cg10k )   (6670, 630860)  # summary statistics (~50 secs on my laptop)  @time   maf ,   _ ,   missings_by_snp ,   =   summarize ( cg10k );    24  # 5 number summary and average MAF (minor allele frequencies)  quantile ( maf ,   [ 0.0   . 25   . 5   . 75   1.0 ]),   mean ( maf )   ([0.00841726 0.124063 \u2026 0.364253 0.5], 0.24536516625042462)  # Pkg.add( Plots )  # Pkg.add( PyPlot )  using   Plots  pyplot ()  histogram ( maf ,   xlab   =   Minor Allele Frequency (MAF) ,   label   =   MAF )    # proportion of missing genotypes  sum ( missings_by_snp )   /   length ( cg10k )   0.0013128198764010824  # proportion of rare SNPs with maf   0.05  countnz ( maf   .   0.05 )   /   length ( maf )   0.07228069619249913", 
            "title": "Summary statistics of SNP data"
        }, 
        {
            "location": "/man/heritability/#empirical-kinship-matrix", 
            "text": "We estimate empirical kinship based on all SNPs by the genetic relation matrix (GRM). Missing genotypes are imputed on the fly by drawing according to the minor allele frequencies.  # GRM using SNPs with maf   0.01 (default) (~10 mins on my laptop)  srand ( 123 )  @time   \u03a6grm   =   grm ( cg10k ;   method   =   : GRM )   396.943890 seconds (8.43 G allocations: 127.378 GiB, 4.38% gc time)\n\n\n\n\n\n6670\u00d76670 Array{Float64,2}:\n  0.503024      0.00335505   -0.000120075  \u2026  -5.45185e-5   -0.00278072 \n  0.00335505    0.498958     -0.00195952       0.000868471   0.0034285  \n -0.000120075  -0.00195952    0.493828         0.000174648  -0.000381467\n  0.000923828  -0.00329169   -0.00194166      -0.00223595   -0.00123508 \n -8.39649e-5   -0.00353358    0.0018709        0.00222858   -0.00171176 \n  0.00204208    0.000572952   0.00254025   \u2026   0.000861385   2.99785e-5 \n  0.000569323   0.0024786    -0.00185743       0.00117649   -0.00118027 \n -0.000642144   0.00317992   -0.00099777       0.00354182   -0.000260645\n -0.00102913   -0.00123475   -0.00061138       0.00173885    0.00177727 \n -0.00139442    0.00208423    0.000124525     -0.00145156   -0.001011   \n -0.00204555    0.00011055   -0.000419398  \u2026  -0.000198235  -0.00110353 \n  0.000947587   0.00167346    0.00184451      -0.000690143  -0.00304087 \n  0.000322759  -0.000899805   0.00303981       0.000739331  -0.00118835 \n  \u22ee                                        \u22f1                            \n  0.00298012    0.00130003    0.000998861      4.18454e-6    0.00303991 \n -0.00207748    0.00274717   -0.00191741      -0.00107073    0.00368267 \n  0.000545569  -0.00244439   -0.00299578   \u2026  -0.000669885   0.00221027 \n -0.00423186   -0.00208514   -0.00108833      -0.000622127  -0.000567483\n -0.00325644   -0.000781353   0.0030423        0.000501423  -0.00010267 \n  0.00041055   -0.00200772    0.00274867      -0.00624933   -0.00521365 \n  0.00210519    0.000879889  -0.00107817      -0.000797878  -0.000557352\n -0.00230058   -0.000119132   0.000116817  \u2026   0.000867087  -0.00233512 \n -0.0020119     0.00230772   -0.00128837       0.00194798   -0.00048733 \n -0.000944942  -0.000928073  -0.000175096      0.00126911   -0.00303766 \n -5.45185e-5    0.000868471   0.000174648      0.500829      0.000469478\n -0.00278072    0.0034285    -0.000381467      0.000469478   0.500627", 
            "title": "Empirical kinship matrix"
        }, 
        {
            "location": "/man/heritability/#phenotypes", 
            "text": "Read in the phenotype data and compute descriptive statistics.  # Pkg.add( DataFrames )  using   DataFrames  cg10k_trait   =   readtable ( \n     cg10k_traits.txt ;  \n     separator   =     , \n     names   =   [ : FID ;   : IID ;   : Trait1 ;   : Trait2 ;   : Trait3 ;   : Trait4 ;   : Trait5 ;   : Trait6 ;  \n              : Trait7 ;   : Trait8 ;   : Trait9 ;   : Trait10 ;   : Trait11 ;   : Trait12 ;   : Trait13 ],   \n     eltypes   =   [ String ;   String ;   Float64 ;   Float64 ;   Float64 ;   Float64 ;   Float64 ;  \n                Float64 ;   Float64 ;   Float64 ;   Float64 ;   Float64 ;   Float64 ;   Float64 ;   Float64 ] \n     )  # do not display FID and IID for privacy  cg10k_trait [ : ,   3 : end ]   Trait1 Trait2 Trait3 Trait4 Trait5 Trait6 Trait7 Trait8 Trait9 Trait10 Trait11 Trait12 Trait13 1 -1.81573145026234 -0.94615046147283 1.11363077580442 -2.09867121119159 0.744416614111748 0.00139171884080131 0.934732480409667 -1.22677315418103 1.1160784277875 -0.4436280335029 0.824465656443384 -1.02852542216546 -0.394049201727681 2 -1.24440094378729 0.109659992547179 0.467119394241789 -1.62131304097589 1.0566758355683 0.978946979419181 1.00014633946047 0.32487427140228 1.16232175219696 2.6922706948705 3.08263672461047 1.09064954786013 0.0256616415357438 3 1.45566914502305 1.53866932923243 1.09402959376555 0.586655272226893 -0.32796454430367 -0.30337709778827 -0.0334354881314741 -0.464463064285437 -0.3319396273436 -0.486839089635991 -1.10648681564373 -1.42015780427231 -0.687463456644413 4 -0.768809276698548 0.513490885514249 0.244263028382142 -1.31740254475691 1.19393774326845 1.17344127734288 1.08737426675232 0.536022583732261 0.802759240762068 0.234159411749815 0.394174866891074 -0.767365892476029 0.0635385761884935 5 -0.264415132547719 -0.348240421825694 -0.0239065083413606 0.00473915802244948 1.25619191712193 1.2038883667631 1.29800739042627 0.310113660247311 0.626159861059352 0.899289129831224 0.54996783350812 0.540687809542048 0.179675416046033 6 -1.37617270917293 -1.47191967744564 0.291179894254146 -0.803110740704731 -0.264239977442213 -0.260573027836772 -0.165372266287781 -0.219257294118362 1.04702422290318 -0.0985815534616482 0.947393438068448 0.594014812031438 0.245407436348479 7 0.1009416296374 -0.191615722103455 -0.567421321596677 0.378571487240382 -0.246656179817904 -0.608810750053858 0.189081058215596 -1.27077787326519 -0.452476199143965 0.702562877297724 0.332636218957179 0.0026916503626181 0.317117176705358 8 -0.319818276367464 1.35774480657283 0.818689545938528 -1.15565531644352 0.63448368102259 0.291461908634679 0.933323714954726 -0.741083289682492 0.647477683507572 -0.970877627077966 0.220861165411304 0.852512250237764 -0.225904624283945 9 -0.288334173342032 0.566082538090752 0.254958336116175 -0.652578302869714 0.668921559277347 0.978309199170558 0.122862966041938 1.4790926378214 0.0672132424173449 0.0795903917527827 0.167532455243232 0.246915579442139 0.539932616458363 10 -1.15759732583991 -0.781198583545165 -0.595807759833517 -1.00554980260402 0.789828885933321 0.571058413379044 0.951304176233755 -0.295962982984816 0.99042002479707 0.561309366988983 0.733100030623233 -1.73467772245684 -1.35278484330654 11 0.740569150459031 1.40873846755415 0.734689999440088 0.0208322841295094 -0.337440968561619 -0.458304040611395 -0.142582512772326 -0.580392297464107 -0.684684998101516 -0.00785381461893456 -0.712244337518008 -0.313345561230878 -0.345419463162219 12 -0.675892486454995 0.279892613829682 0.267915996308248 -1.04103665392985 0.910741715645888 0.866027618513171 1.07414431702005 0.0381751003538302 0.766355377018601 -0.340118016143495 -0.809013958505059 0.548521663785885 -0.0201828675962336 13 -0.795410435603455 -0.699989939762738 0.3991295030063 -0.510476261900736 1.51552245416844 1.28743032939467 1.53772393250903 0.133989160117702 1.02025736886037 0.499018733899186 -0.36948273277931 -1.10153460436318 -0.598132438886619 14 -0.193483122930324 -0.286021160323518 -0.691494225262995 0.0131581678700699 1.52337470686782 1.4010638072262 1.53114620451896 0.333066483478075 1.04372480381099 0.163206783570466 -0.422883765001728 -0.383527976713573 -0.489221907788158 15 0.151246203379718 2.09185108993614 2.03800472474384 -1.12474717143531 1.66557024390713 1.62535675109576 1.58751070483655 0.635852186043776 0.842577784605979 0.450761870778952 -1.39479033623028 -0.560984107567768 0.289349776549287 16 -0.464608740812712 0.36127694772303 1.2327673928287 -0.826033731086383 1.43475224709983 1.74451823818846 0.211096887484638 2.64816425140548 1.02511433146096 0.11975731603184 0.0596832073448267 -0.631231612661616 -0.207878671782927 17 -0.732977488012215 -0.526223425889779 0.61657871336593 -0.55447974332593 0.947484859025104 0.936833214138173 0.972516806335524 0.290251013865227 1.01285359725723 0.516207422283291 -0.0300689171988194 0.8787322524583 0.450254629309513 18 -0.167326459622119 0.175327165487237 0.287467725892572 -0.402652532084246 0.551181509418056 0.522204743290975 0.436837660094653 0.299564933845579 0.583109520896067 -0.704415820005353 -0.730810367994577 -1.95140580379896 -0.933504665700164 19 1.41159485787418 1.78722407901017 0.84397639585364 0.481278083772991 -0.0887673728508268 -0.49957757426858 0.304195897924847 -1.23884208383369 -0.153475724036624 -0.870486102788329 0.0955473331150403 -0.983708050882817 -0.3563445644514 20 -1.42997091652825 -0.490147045034213 0.272730237607695 -1.61029992954153 0.990787817197748 0.711687532608184 1.1885836012715 -0.371229188075638 1.24703459239952 -0.0389162332271516 0.883495749072872 2.58988026321017 3.33539552370368 21 -0.147247288176765 0.12328430415652 0.617549051912237 -0.18713077178262 0.256438107586694 0.17794983735083 0.412611806463263 -0.244809124559737 0.0947624806136492 0.723017223849532 -0.683948354633436 0.0873751276309269 -0.262209652750371 22 -0.187112676773894 -0.270777264595619 -1.01556818551606 0.0602850568600233 0.272419757757978 0.869133161879197 -0.657519461414234 2.32388522018189 -0.999936011525034 1.44671844178306 0.971157886040772 -0.358747904241515 -0.439657942096136 23 -1.82434047163768 -0.933480446068067 1.29474003766977 -1.94545221151036 0.33584651189654 0.359201654302844 0.513652924365886 -0.073197696696958 1.57139042812005 1.53329371326728 1.82076821859528 2.22740301867829 1.50063347195857 24 -2.29344084351335 -2.49161842344418 0.40383988742336 -2.36488074752948 1.4105254831956 1.42244117147792 1.17024166272172 0.84476650176855 1.79026875432495 0.648181858970515 -0.0857231057403538 -1.02789535292617 0.491288088952859 25 -0.434135932888305 0.740881989034652 0.699576357578518 -1.02405543187775 0.759529223983713 0.956656110895288 0.633299568656589 0.770733932268516 0.824988511714526 1.84287437634769 1.91045942063443 -0.502317207869366 0.132670133448219 26 -2.1920969546557 -2.49465664272271 0.354854763893431 -1.93155848635714 0.941979400289938 0.978917101414106 0.894860097289736 0.463239402831873 1.12537133317163 1.70528446191955 0.717792714479123 0.645888049108261 0.783968250169388 27 -1.46602269088422 -1.24921677101897 0.307977693653039 -1.55097364660989 0.618908494474798 0.662508171662042 0.475957173906078 0.484718674597707 0.401564892028249 0.55987973254026 -0.376938143754217 -0.933982629228218 0.390013151672955 28 -1.83317744236881 -1.53268787828701 2.55674262685865 -1.51827745783835 0.789409601746455 0.908747799728588 0.649971922941479 0.668373649931667 1.20058303519903 0.277963256075637 1.2504953198275 3.31370445071638 2.22035828885342 29 -0.784546628243178 0.276582579543931 3.01104958800057 -1.11978843206758 0.920823858422707 0.750217689886151 1.26153730009639 -0.403363882922417 0.400667296857811 -0.217597941303479 -0.724669537565068 -0.391945338467193 -0.650023936358253 30 0.464455916345135 1.3326356122229 -1.23059563374303 -0.357975958937414 1.18249746977104 1.54315938069757 -0.60339041154062 3.38308845958422 0.823740765148641 -0.129951318508883 -0.657979878422938 -0.499534924074273 -0.414476569095651  describe ( cg10k_trait [ : ,   3 : end ])   Trait1\nSummary Stats:\nMean:           0.002211\nMinimum:        -3.204128\n1st Quartile:   -0.645771\nMedian:         0.125010\n3rd Quartile:   0.723315\nMaximum:        3.479398\nLength:         6670\nType:           Float64\nNumber Missing: 0\n% Missing:      0.000000\n\nTrait2\nSummary Stats:\nMean:           0.001353\nMinimum:        -3.511659\n1st Quartile:   -0.642621\nMedian:         0.033517\n3rd Quartile:   0.657467\nMaximum:        4.913423\nLength:         6670\nType:           Float64\nNumber Missing: 0\n% Missing:      0.000000\n\nTrait3\nSummary Stats:\nMean:           -0.001296\nMinimum:        -3.938436\n1st Quartile:   -0.640907\nMedian:         -0.000782\n3rd Quartile:   0.637108\nMaximum:        7.916299\nLength:         6670\nType:           Float64\nNumber Missing: 0\n% Missing:      0.000000\n\nTrait4\nSummary Stats:\nMean:           0.002309\nMinimum:        -3.608403\n1st Quartile:   -0.546086\nMedian:         0.228165\n3rd Quartile:   0.715291\nMaximum:        3.127688\nLength:         6670\nType:           Float64\nNumber Missing: 0\n% Missing:      0.000000\n\nTrait5\nSummary Stats:\nMean:           -0.001790\nMinimum:        -4.148749\n1st Quartile:   -0.690765\nMedian:         0.031034\n3rd Quartile:   0.734916\nMaximum:        2.717184\nLength:         6670\nType:           Float64\nNumber Missing: 0\n% Missing:      0.000000\n\nTrait6\nSummary Stats:\nMean:           -0.001196\nMinimum:        -3.824792\n1st Quartile:   -0.662796\nMedian:         0.036242\n3rd Quartile:   0.741176\nMaximum:        2.589728\nLength:         6670\nType:           Float64\nNumber Missing: 0\n% Missing:      0.000000\n\nTrait7\nSummary Stats:\nMean:           -0.001989\nMinimum:        -4.272455\n1st Quartile:   -0.638923\nMedian:         0.069801\n3rd Quartile:   0.710423\nMaximum:        2.653779\nLength:         6670\nType:           Float64\nNumber Missing: 0\n% Missing:      0.000000\n\nTrait8\nSummary Stats:\nMean:           0.000614\nMinimum:        -5.625488\n1st Quartile:   -0.601575\nMedian:         -0.038630\n3rd Quartile:   0.527342\nMaximum:        5.805702\nLength:         6670\nType:           Float64\nNumber Missing: 0\n% Missing:      0.000000\n\nTrait9\nSummary Stats:\nMean:           -0.001810\nMinimum:        -5.381968\n1st Quartile:   -0.601429\nMedian:         0.106571\n3rd Quartile:   0.698567\nMaximum:        2.571936\nLength:         6670\nType:           Float64\nNumber Missing: 0\n% Missing:      0.000000\n\nTrait10\nSummary Stats:\nMean:           -0.000437\nMinimum:        -3.548506\n1st Quartile:   -0.633641\nMedian:         -0.096651\n3rd Quartile:   0.498610\nMaximum:        6.537820\nLength:         6670\nType:           Float64\nNumber Missing: 0\n% Missing:      0.000000\n\nTrait11\nSummary Stats:\nMean:           -0.000616\nMinimum:        -3.264910\n1st Quartile:   -0.673685\nMedian:         -0.068044\n3rd Quartile:   0.655486\nMaximum:        4.262410\nLength:         6670\nType:           Float64\nNumber Missing: 0\n% Missing:      0.000000\n\nTrait12\nSummary Stats:\nMean:           -0.000589\nMinimum:        -8.851909\n1st Quartile:   -0.539686\nMedian:         -0.141099\n3rd Quartile:   0.350779\nMaximum:        13.211402\nLength:         6670\nType:           Float64\nNumber Missing: 0\n% Missing:      0.000000\n\nTrait13\nSummary Stats:\nMean:           -0.000151\nMinimum:        -5.592104\n1st Quartile:   -0.492289\nMedian:         -0.141022\n3rd Quartile:   0.324804\nMaximum:        24.174436\nLength:         6670\nType:           Float64\nNumber Missing: 0\n% Missing:      0.000000  Y   =   convert ( Matrix { Float64 },   cg10k_trait [ : ,   3 : 15 ])  histogram ( Y ,   layout   =   13 )", 
            "title": "Phenotypes"
        }, 
        {
            "location": "/man/heritability/#pre-processing-data-for-heritability-analysis", 
            "text": "To prepare variance component model fitting, we form an instance of  VarianceComponentVariate . The two variance components are $(2\\Phi, I)$.  using   VarianceComponentModels  # form data as VarianceComponentVariate  cg10kdata   =   VarianceComponentVariate ( Y ,   ( 2 \u03a6grm ,   eye ( size ( Y ,   1 ))))  fieldnames ( cg10kdata )   3-element Array{Symbol,1}:\n :Y\n :X\n :V  cg10kdata   VarianceComponentModels.VarianceComponentVariate{Float64,2,Array{Float64,2},Array{Float64,2},Array{Float64,2}}([-1.81573 -0.94615 \u2026 -1.02853 -0.394049; -1.2444 0.10966 \u2026 1.09065 0.0256616; \u2026 ; 0.886626 0.487408 \u2026 -0.636874 -0.439825; -1.24394 0.213697 \u2026 0.299931 0.392809], Array{Float64}(6670,0), ([1.00605 0.00671009 \u2026 -0.000109037 -0.00556144; 0.00671009 0.997916 \u2026 0.00173694 0.00685701; \u2026 ; -0.000109037 0.00173694 \u2026 1.00166 0.000938955; -0.00556144 0.00685701 \u2026 0.000938955 1.00125], [1.0 0.0 \u2026 0.0 0.0; 0.0 1.0 \u2026 0.0 0.0; \u2026 ; 0.0 0.0 \u2026 1.0 0.0; 0.0 0.0 \u2026 0.0 1.0]))  Before fitting the variance component model, we pre-compute the eigen-decomposition of $2\\Phi_{\\text{GRM}}$, the rotated responses, and the constant part in log-likelihood, and store them as a  TwoVarCompVariateRotate  instance, which is re-used in various variane component estimation procedures.  # pre-compute eigen-decomposition (~50 secs on my laptop)  @time   cg10kdata_rotated   =   TwoVarCompVariateRotate ( cg10kdata )  fieldnames ( cg10kdata_rotated )    48.837361 seconds (39 allocations: 1021.427 MiB, 0.57% gc time)\n\n\n\n\n\n5-element Array{Symbol,1}:\n :Yrot    \n :Xrot    \n :eigval  \n :eigvec  \n :logdetV2", 
            "title": "Pre-processing data for heritability analysis"
        }, 
        {
            "location": "/man/heritability/#save-intermediate-results", 
            "text": "We don't want to re-compute SnpArray and empirical kinship matrices again and again for heritibility analysis.  # # Pkg.add( JLD )  # using JLD  # @save  cg10k.jld  # whos()   To load workspace  using   SnpArrays ,   JLD ,   DataFrames ,   VarianceComponentModels ,   Plots  pyplot ()  @load   cg10k.jld  whos ()                             Base               Module\n                       BinDeps  41348 KB     Module\n                         Blosc  41202 KB     Module\n                    ColorTypes  41457 KB     Module\n                        Colors  41480 KB     Module\n                        Compat  41196 KB     Module\n                         Conda  41205 KB     Module\n                          Core               Module\n                    DataArrays  41456 KB     Module\n                    DataFrames  41684 KB     Module\n                DataStructures  41356 KB     Module\n                        FileIO  41310 KB     Module\n             FixedPointNumbers  41695 KB     Module\n                          GZip  41181 KB     Module\n                          HDF5  41403 KB     Module\n                        IJulia 4185781 KB     Module\n                         Ipopt  41172 KB     Module\n                           JLD  41376 KB     Module\n                          JSON  41245 KB     Module\n                  LaTeXStrings   4058 bytes  Module\n                 LegacyStrings  41212 KB     Module\n                    LinearMaps     22 KB     Module\n                    MacroTools  41606 KB     Module\n                          Main               Module\n                  MathProgBase  41353 KB     Module\n                       MbedTLS  41269 KB     Module\n                      Measures  41175 KB     Module\n                       NaNMath  41200 KB     Module\n                    PlotThemes  41167 KB     Module\n                     PlotUtils  41332 KB     Module\n                         Plots  42960 KB     Module\n                        PyCall  41711 KB     Module\n                        PyPlot  41771 KB     Module\n                   RecipesBase  41283 KB     Module\n                      Reexport  41160 KB     Module\n                      Requires  41172 KB     Module\n                           SHA     62 KB     Module\n                       Showoff  41163 KB     Module\n                     SnpArrays  41218 KB     Module\n             SortingAlgorithms  41178 KB     Module\n              SpecialFunctions  41252 KB     Module\n                  StaticArrays  41744 KB     Module\n                     StatsBase  41810 KB     Module\n                     URIParser  41171 KB     Module\n       VarianceComponentModels  41278 KB     Module\n                             Y    677 KB     6670\u00d713 Array{Float64,2}\n                           ZMQ  41223 KB     Module\n                             _     77 KB     630860-element BitArray{1}\n                         cg10k 1027303 KB     6670\u00d7630860 SnpArrays.SnpArray{2}\n                   cg10k_trait    978 KB     6670\u00d715 DataFrames.DataFrame\n                     cg10kdata 695816 KB     VarianceComponentModels.VarianceCo\u2026\n             cg10kdata_rotated 348299 KB     VarianceComponentModels.TwoVarComp\u2026\n                             h     24 bytes  3-element Array{Float64,1}\n                           hST    104 bytes  13-element Array{Float64,1}\n                        hST_se    104 bytes  13-element Array{Float64,1}\n                           hse     24 bytes  3-element Array{Float64,1}\n                           maf   4928 KB     630860-element Array{Float64,1}\n               missings_by_snp   4928 KB     630860-element Array{Int64,1}\n                        people      8 bytes  Int64\n                          snps      8 bytes  Int64\n                  trait57_data 347778 KB     VarianceComponentModels.TwoVarComp\u2026\n                 trait57_model    232 bytes  VarianceComponentModels.VarianceCo\u2026\n                traitall_model   2792 bytes  VarianceComponentModels.VarianceCo\u2026\n                      traitidx     16 bytes  3-element UnitRange{Int64}\n                            \u03a3a   3848 bytes  13\u00d713 Array{Array{Float64,2},2}\n                          \u03a3cov   2592 bytes  18\u00d718 Array{Float64,2}\n                            \u03a3e   3848 bytes  13\u00d713 Array{Array{Float64,2},2}\n                          \u03a6grm 347569 KB     6670\u00d76670 Array{Float64,2}\n                           \u03c32a    104 bytes  13-element Array{Float64,1}\n                           \u03c32e    104 bytes  13-element Array{Float64,1}", 
            "title": "Save intermediate results"
        }, 
        {
            "location": "/man/heritability/#heritability-of-single-traits", 
            "text": "We use Fisher scoring algorithm to fit variance component model for each single trait.  # heritability from single trait analysis  hST   =   zeros ( 13 )  # standard errors of estimated heritability  hST_se   =   zeros ( 13 )  # additive genetic effects  \u03c32a   =   zeros ( 13 )  # enviromental effects  \u03c32e   =   zeros ( 13 )  tic ()  for   trait   in   1 : 13 \n     println ( names ( cg10k_trait )[ trait   +   2 ]) \n     # form data set for trait j \n     traitj_data   =   TwoVarCompVariateRotate ( cg10kdata_rotated . Yrot [ : ,   trait ],   cg10kdata_rotated . Xrot ,  \n         cg10kdata_rotated . eigval ,   cg10kdata_rotated . eigvec ,   cg10kdata_rotated . logdetV2 ) \n     # initialize model parameters \n     traitj_model   =   VarianceComponentModel ( traitj_data ) \n     # estimate variance components \n     _ ,   _ ,   _ ,   \u03a3cov ,   _ ,   _   =   mle_fs! ( traitj_model ,   traitj_data ;   solver =: Ipopt ,   verbose = false ) \n     \u03c32a [ trait ]   =   traitj_model . \u03a3 [ 1 ][ 1 ] \n     \u03c32e [ trait ]   =   traitj_model . \u03a3 [ 2 ][ 1 ] \n     @show   \u03c32a [ trait ],   \u03c32e [ trait ] \n     h ,   hse   =   heritability ( traitj_model . \u03a3 ,   \u03a3cov ) \n     hST [ trait ]   =   h [ 1 ] \n     hST_se [ trait ]   =   hse [ 1 ]  end  toc ()   Trait1\n(\u03c32a[trait], \u03c32e[trait]) = (0.25978160614793233, 0.7369535197912689)\nTrait2\n(\u03c32a[trait], \u03c32e[trait]) = (0.18647130348299173, 0.8129591079735827)\nTrait3\n(\u03c32a[trait], \u03c32e[trait]) = (0.3188368159422607, 0.6798809726936244)\nTrait4\n(\u03c32a[trait], \u03c32e[trait]) = (0.2651357653143703, 0.7308007669086968)\nTrait5\n(\u03c32a[trait], \u03c32e[trait]) = (0.28083388108246, 0.7172036435586534)\nTrait6\n(\u03c32a[trait], \u03c32e[trait]) = (0.2824159905728832, 0.7170988773569172)\nTrait7\n(\u03c32a[trait], \u03c32e[trait]) = (0.2155274336968625, 0.7815346282986375)\nTrait8\n(\u03c32a[trait], \u03c32e[trait]) = (0.194687807263945, 0.8049690651320599)\nTrait9\n(\u03c32a[trait], \u03c32e[trait]) = (0.24706855916591713, 0.7512942998567308)\nTrait10\n(\u03c32a[trait], \u03c32e[trait]) = (0.098712236297271, 0.9011756660217387)\nTrait11\n(\u03c32a[trait], \u03c32e[trait]) = (0.1664264642608195, 0.8322427413046204)\nTrait12\n(\u03c32a[trait], \u03c32e[trait]) = (0.0834296761650666, 0.9153609794266608)\nTrait13\n(\u03c32a[trait], \u03c32e[trait]) = (0.05893968504298988, 0.940270012443928)\nelapsed time: 0.160999612 seconds\n\n\n\n\n\n0.160999612  # heritability and standard errors  [ hST ;   hST_se ]   2\u00d713 Array{Float64,2}:\n 0.260633   0.186578   0.319246   \u2026  0.166648  0.0835307  0.0589863\n 0.0799732  0.0869002  0.0741007     0.08862   0.0944407  0.0953238", 
            "title": "Heritability of single traits"
        }, 
        {
            "location": "/man/heritability/#pairwise-traits", 
            "text": "Joint analysis of multiple traits is subject to intensive research recently. Following code snippet does joint analysis of all pairs of traits, a total of 78 bivariate variane component models.  # additive genetic effects (2x2 psd matrices) from bavariate trait analysis;  \u03a3a   =   Array { Matrix { Float64 }}( 13 ,   13 )  # environmental effects (2x2 psd matrices) from bavariate trait analysis;  \u03a3e   =   Array { Matrix { Float64 }}( 13 ,   13 )  tic ()  for   i   in   1 : 13 \n     for   j   in   ( i + 1 ) : 13 \n         println ( names ( cg10k_trait )[ i   +   2 ],   names ( cg10k_trait )[ j   +   2 ]) \n         # form data set for (trait1, trait2) \n         traitij_data   =   TwoVarCompVariateRotate ( cg10kdata_rotated . Yrot [ : ,   [ i ; j ]],   cg10kdata_rotated . Xrot ,  \n             cg10kdata_rotated . eigval ,   cg10kdata_rotated . eigvec ,   cg10kdata_rotated . logdetV2 ) \n         # initialize model parameters \n         traitij_model   =   VarianceComponentModel ( traitij_data ) \n         # estimate variance components \n         mle_fs! ( traitij_model ,   traitij_data ;   solver =: Ipopt ,   verbose = false ) \n         \u03a3a [ i ,   j ]   =   traitij_model . \u03a3 [ 1 ] \n         \u03a3e [ i ,   j ]   =   traitij_model . \u03a3 [ 2 ] \n         @show   \u03a3a [ i ,   j ],   \u03a3e [ i ,   j ] \n     end  end  toc ()   Trait1Trait2\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.258822 0.174358; 0.174358 0.185108], [0.737892 0.585751; 0.585751 0.814301])\nTrait1Trait3\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.260236 -0.0144726; -0.0144726 0.319245], [0.736512 -0.11979; -0.11979 0.679488])\nTrait1Trait4\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.259615 0.222203; 0.222203 0.265149], [0.737116 0.599854; 0.599854 0.730788])\nTrait1Trait5\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.259574 -0.146827; -0.146827 0.28153], [0.737153 -0.254777; -0.254777 0.71653])\nTrait1Trait6\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.259476 -0.129115; -0.129115 0.282688], [0.73725 -0.23161; -0.23161 0.716837])\nTrait1Trait7\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.259115 -0.140455; -0.140455 0.215297], [0.737606 -0.197616; -0.197616 0.781774])\nTrait1Trait8\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.259778 -0.0327756; -0.0327756 0.194698], [0.736957 -0.127026; -0.127026 0.804959])\nTrait1Trait9\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.261858 -0.204589; -0.204589 0.246027], [0.734961 -0.307734; -0.307734 0.75232])\nTrait1Trait10\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.259649 -0.0994858; -0.0994858 0.0956585], [0.737083 -0.303942; -0.303942 0.904218])\nTrait1Trait11\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.25947 -0.138603; -0.138603 0.164709], [0.737257 -0.359557; -0.359557 0.83395])\nTrait1Trait12\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.261779 -0.145414; -0.145414 0.0807748], [0.735076 -0.041823; -0.041823 0.9181])\nTrait1Trait13\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.261125 -0.108774; -0.108774 0.0538214], [0.735674 -0.114123; -0.114123 0.945416])\nTrait2Trait3\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.186541 0.144056; 0.144056 0.320627], [0.812888 0.0995944; 0.0995944 0.678167])\nTrait2Trait4\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.186131 0.0746032; 0.0746032 0.265122], [0.813293 0.221109; 0.221109 0.730814])\nTrait2Trait5\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.186442 -0.0118093; -0.0118093 0.280842], [0.812987 -0.0365191; -0.0365191 0.717195])\nTrait2Trait6\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.18649 -0.00366533; -0.00366533 0.282471], [0.812941 -0.0206271; -0.0206271 0.717046])\nTrait2Trait7\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.186104 -0.030665; -0.030665 0.215304], [0.81332 -0.000667009; -0.000667009 0.781755])\nTrait2Trait8\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.187023 0.0331783; 0.0331783 0.195259], [0.812421 -0.0326343; -0.0326343 0.804415])\nTrait2Trait9\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.185032 -0.085334; -0.085334 0.245909], [0.814386 -0.0809638; -0.0809638 0.752433])\nTrait2Trait10\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.186587 -0.123303; -0.123303 0.0987387], [0.812872 -0.273083; -0.273083 0.901229])\nTrait2Trait11\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.185484 -0.117256; -0.117256 0.167776], [0.81393 -0.296772; -0.296772 0.830934])\nTrait2Trait12\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.185907 -0.0909104; -0.0909104 0.0827171], [0.813555 0.0457924; 0.0457924 0.916135])\nTrait2Trait13\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.185979 -0.0720811; -0.0720811 0.0568238], [0.8135 0.0751703; 0.0751703 0.942424])\nTrait3Trait4\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.3188 -0.154562; -0.154562 0.264323], [0.679917 -0.303223; -0.303223 0.731591])\nTrait3Trait5\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.319216 0.183527; 0.183527 0.282063], [0.679514 0.33724; 0.33724 0.716008])\nTrait3Trait6\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.319776 0.165672; 0.165672 0.284448], [0.678972 0.298667; 0.298667 0.715124])\nTrait3Trait7\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.318838 0.166283; 0.166283 0.215261], [0.67988 0.347706; 0.347706 0.781796])\nTrait3Trait8\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.320718 0.0566397; 0.0566397 0.197764], [0.678063 0.0451569; 0.0451569 0.801956])\nTrait3Trait9\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.319001 0.137699; 0.137699 0.246142], [0.679722 0.266704; 0.266704 0.752197])\nTrait3Trait10\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.31908 -0.076513; -0.076513 0.0996001], [0.679646 -0.142905; -0.142905 0.900298])\nTrait3Trait11\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.318094 -0.0177494; -0.0177494 0.16629], [0.6806 -0.1144; -0.1144 0.832376])\nTrait3Trait12\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.321164 0.0843842; 0.0843842 0.0874609], [0.677639 0.0341558; 0.0341558 0.911368])\nTrait3Trait13\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.323273 0.109443; 0.109443 0.0634295], [0.675635 -0.0060525; -0.0060525 0.935819])\nTrait4Trait5\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.26525 -0.215125; -0.215125 0.282572], [0.73068 -0.377406; -0.377406 0.715518])\nTrait4Trait6\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.265715 -0.199714; -0.199714 0.283942], [0.730231 -0.347732; -0.347732 0.715619])\nTrait4Trait7\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.26407 -0.18238; -0.18238 0.214324], [0.731843 -0.32655; -0.32655 0.782733])\nTrait4Trait8\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.266229 -0.0965381; -0.0965381 0.196655], [0.729739 -0.151461; -0.151461 0.803044])\nTrait4Trait9\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.269627 -0.226931; -0.226931 0.247265], [0.726443 -0.416085; -0.416085 0.751086])\nTrait4Trait10\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.265098 -0.0352926; -0.0352926 0.0981462], [0.730847 -0.226248; -0.226248 0.901736])\nTrait4Trait11\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.265178 -0.0970634; -0.0970634 0.164885], [0.73076 -0.272291; -0.272291 0.833762])\nTrait4Trait12\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.267732 -0.140985; -0.140985 0.081029], [0.728323 -0.0834791; -0.0834791 0.917815])\nTrait4Trait13\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.265695 -0.0970238; -0.0970238 0.0564809], [0.730259 -0.226115; -0.226115 0.942736])\nTrait5Trait6\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.281198 0.280259; 0.280259 0.281764], [0.716855 0.661013; 0.661013 0.717735])\nTrait5Trait7\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.280442 0.231918; 0.231918 0.211837], [0.717597 0.674491; 0.674491 0.785172])\nTrait5Trait8\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.280958 0.163168; 0.163168 0.193315], [0.717089 0.221817; 0.221817 0.806314])\nTrait5Trait9\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.283544 0.243884; 0.243884 0.240564], [0.714585 0.509072; 0.509072 0.757631])\nTrait5Trait10\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.281378 -0.0454427; -0.0454427 0.100081], [0.716678 -0.0579778; -0.0579778 0.899822])\nTrait5Trait11\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.280066 0.0195669; 0.0195669 0.165607], [0.71795 -0.0345589; -0.0345589 0.833047])\nTrait5Trait12\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.28101 0.0592641; 0.0592641 0.0831831], [0.717036 0.0552788; 0.0552788 0.915608])\nTrait5Trait13\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.281854 0.0680641; 0.0680641 0.0591899], [0.716223 0.0551992; 0.0551992 0.940027])\nTrait6Trait7\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.282435 0.220236; 0.220236 0.213997], [0.71708 0.581507; 0.581507 0.783041])\nTrait6Trait8\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.282435 0.18375; 0.18375 0.192999], [0.717081 0.436932; 0.436932 0.80663])\nTrait6Trait9\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.284516 0.233768; 0.233768 0.242478], [0.715071 0.477502; 0.477502 0.755765])\nTrait6Trait10\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.283087 -0.0427658; -0.0427658 0.100634], [0.716449 -0.0599491; -0.0599491 0.899275])\nTrait6Trait11\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.281046 0.0272144; 0.0272144 0.165044], [0.71843 -0.0516242; -0.0516242 0.833601])\nTrait6Trait12\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.28256 0.0548537; 0.0548537 0.083133], [0.716961 0.0502064; 0.0502064 0.915658])\nTrait6Trait13\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.283231 0.0585667; 0.0585667 0.0592752], [0.716314 0.055827; 0.055827 0.939942])\nTrait7Trait8\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.213998 0.0875641; 0.0875641 0.192993], [0.78304 -0.055939; -0.055939 0.806635])\nTrait7Trait9\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.219039 0.216925; 0.216925 0.243338], [0.778156 0.463024; 0.463024 0.754935])\nTrait7Trait10\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.216296 -0.0412106; -0.0412106 0.100663], [0.780785 -0.0868086; -0.0868086 0.899246])\nTrait7Trait11\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.2142 0.0204227; 0.0204227 0.165077], [0.782833 -0.0478727; -0.0478727 0.833569])\nTrait7Trait12\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.215054 0.0738562; 0.0738562 0.0814228], [0.782012 0.0366272; 0.0366272 0.917365])\nTrait7Trait13\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.216093 0.0728515; 0.0728515 0.0570272], [0.781006 0.0409945; 0.0409945 0.942189])\nTrait8Trait9\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.195154 0.111756; 0.111756 0.246453], [0.804528 0.185842; 0.185842 0.751896])\nTrait8Trait10\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.195015 -0.015506; -0.015506 0.0990776], [0.804651 0.0118538; 0.0118538 0.900815])\nTrait8Trait11\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.194421 0.0215044; 0.0215044 0.166226], [0.805231 -0.026247; -0.026247 0.83244])\nTrait8Trait12\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.194491 -0.00425152; -0.00425152 0.0832711], [0.805162 0.0349872; 0.0349872 0.915518])\nTrait8Trait13\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.19448 0.00235501; 0.00235501 0.0589351], [0.805173 0.0396048; 0.0396048 0.940275])\nTrait9Trait10\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.246455 -0.00257997; -0.00257997 0.0984563], [0.751895 0.0743439; 0.0743439 0.901429])\nTrait9Trait11\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.247001 0.0303415; 0.0303415 0.166421], [0.75136 0.153765; 0.153765 0.832248])\nTrait9Trait12\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.249421 0.0829968; 0.0829968 0.0890874], [0.749007 0.109331; 0.109331 0.909778])\nTrait9Trait13\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.24861 0.0916799; 0.0916799 0.0602352], [0.749811 0.100027; 0.100027 0.939032])\nTrait10Trait11\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.0914658 0.100613; 0.100613 0.166501], [0.908376 0.473847; 0.473847 0.83217])\nTrait10Trait12\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.0951392 0.0588424; 0.0588424 0.0796744], [0.904735 0.0828862; 0.0828862 0.919115])\nTrait10Trait13\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.0995192 -0.0257171; -0.0257171 0.0598595], [0.900397 0.163778; 0.163778 0.939368])\nTrait11Trait12\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.165386 0.0579914; 0.0579914 0.0796005], [0.83327 0.144637; 0.144637 0.919166])\nTrait11Trait13\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.166417 -0.000985185; -0.000985185 0.0595681], [0.832265 0.200012; 0.200012 0.939646])\nTrait12Trait13\n(\u03a3a[i, j], \u03a3e[i, j]) = ([0.085082 0.0696185; 0.0696185 0.0569655], [0.913729 0.572041; 0.572041 0.942247])\nelapsed time: 3.587337102 seconds\n\n\n\n\n\n3.587337102", 
            "title": "Pairwise traits"
        }, 
        {
            "location": "/man/heritability/#3-trait-analysis", 
            "text": "Researchers want to jointly analyze traits 5-7. Our strategy is to try both Fisher scoring and MM algorithm with different starting point, and choose the best local optimum. We first form the data set and run Fisher scoring, which yields a final objective value -1.4700991+04.  traitidx   =   5 : 7  # form data set  trait57_data   =   TwoVarCompVariateRotate ( cg10kdata_rotated . Yrot [ : ,   traitidx ],   cg10kdata_rotated . Xrot ,  \n     cg10kdata_rotated . eigval ,   cg10kdata_rotated . eigvec ,   cg10kdata_rotated . logdetV2 )  # initialize model parameters  trait57_model   =   VarianceComponentModel ( trait57_data )  # estimate variance components  @time   mle_fs! ( trait57_model ,   trait57_data ;   solver =: Ipopt ,   verbose = true )  trait57_model   This is Ipopt version 3.12.4, running with linear solver mumps.\nNOTE: Other linear solvers might be more efficient (see Ipopt documentation).\n\nNumber of nonzeros in equality constraint Jacobian...:        0\nNumber of nonzeros in inequality constraint Jacobian.:        0\nNumber of nonzeros in Lagrangian Hessian.............:       78\n\nTotal number of variables............................:       12\n                     variables with only lower bounds:        0\n                variables with lower and upper bounds:        0\n                     variables with only upper bounds:        0\nTotal number of equality constraints.................:        0\nTotal number of inequality constraints...............:        0\n        inequality constraints with only lower bounds:        0\n   inequality constraints with lower and upper bounds:        0\n        inequality constraints with only upper bounds:        0\n\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n   0  3.0247565e+04 0.00e+00 1.00e+02   0.0 0.00e+00    -  0.00e+00 0.00e+00   0 \n   5  1.6835078e+04 0.00e+00 4.08e+02 -11.0 3.64e-01    -  1.00e+00 1.00e+00f  1 MaxS\n  10  1.4742941e+04 0.00e+00 1.10e+02 -11.0 2.35e-01    -  1.00e+00 1.00e+00f  1 MaxS\n  15  1.4701394e+04 0.00e+00 1.16e+01 -11.0 7.78e-02  -4.5 1.00e+00 1.00e+00f  1 MaxS\n  20  1.4701019e+04 0.00e+00 5.75e-01 -11.0 1.51e-04  -6.9 1.00e+00 1.00e+00f  1 MaxS\n  25  1.4701018e+04 0.00e+00 2.40e-02 -11.0 6.38e-06  -9.2 1.00e+00 1.00e+00f  1 MaxS\n  30  1.4701018e+04 0.00e+00 9.98e-04 -11.0 2.66e-07 -11.6 1.00e+00 1.00e+00f  1 MaxS\n  35  1.4701018e+04 0.00e+00 4.15e-05 -11.0 1.10e-08 -14.0 1.00e+00 1.00e+00h  1 MaxS\n  40  1.4701018e+04 0.00e+00 1.72e-06 -11.0 4.59e-10 -16.4 1.00e+00 1.00e+00f  1 MaxSA\n  45  1.4701018e+04 0.00e+00 7.17e-08 -11.0 1.91e-11 -18.8 1.00e+00 1.00e+00h  1 MaxSA\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n\nNumber of Iterations....: 49\n\n                                   (scaled)                 (unscaled)\nObjective...............:   4.4720359684330265e+02    1.4701017692082147e+04\nDual infeasibility......:   5.6081357364421780e-09    1.8435742302386474e-07\nConstraint violation....:   0.0000000000000000e+00    0.0000000000000000e+00\nComplementarity.........:   0.0000000000000000e+00    0.0000000000000000e+00\nOverall NLP error.......:   5.6081357364421780e-09    1.8435742302386474e-07\n\n\nNumber of objective function evaluations             = 50\nNumber of objective gradient evaluations             = 50\nNumber of equality constraint evaluations            = 0\nNumber of inequality constraint evaluations          = 0\nNumber of equality constraint Jacobian evaluations   = 0\nNumber of inequality constraint Jacobian evaluations = 0\nNumber of Lagrangian Hessian evaluations             = 49\nTotal CPU secs in IPOPT (w/o function evaluations)   =      0.014\nTotal CPU secs in NLP function evaluations           =      0.076\n\nEXIT: Optimal Solution Found.\n  0.097955 seconds (55.15 k allocations: 5.632 MiB)\n\n\n\n\n\nVarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}(Array{Float64}(0,3), ([0.280777 0.279441 0.232208; 0.279441 0.28422 0.219831; 0.232208 0.219831 0.212832], [0.717266 0.66183 0.674206; 0.66183 0.715287 0.581891; 0.674206 0.581891 0.784183]), Array{Float64}(0,0), Char[], Float64[], -Inf, Inf)  We then run the MM algorithm, starting from the Fisher scoring answer. MM finds an improved solution with objective value 8.955397e+03.  # trait59_model contains the fitted model by Fisher scoring now  @time   mle_mm! ( trait57_model ,   trait57_data ;   verbose = true )  trait57_model        MM Algorithm\n  Iter      Objective  \n--------  -------------\n       0  -1.470102e+04\n       1  -1.470102e+04\n\n  0.003006 seconds (21.01 k allocations: 1.551 MiB)\n\n\n\n\n\nVarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}(Array{Float64}(0,3), ([0.280777 0.279441 0.232208; 0.279441 0.28422 0.219831; 0.232208 0.219831 0.212832], [0.717266 0.66183 0.674206; 0.66183 0.715287 0.581891; 0.674206 0.581891 0.784183]), Array{Float64}(0,0), Char[], Float64[], -Inf, Inf)  Do another run of MM algorithm from default starting point. It leads to a slightly better local optimum -1.470104e+04, slighly worse than the Fisher scoring result. Follow up anlaysis should use the Fisher scoring result.  # default starting point  trait57_model   =   VarianceComponentModel ( trait57_data )  @time   _ ,   _ ,   _ ,   \u03a3cov ,   =   mle_mm! ( trait57_model ,   trait57_data ;   verbose = true )  trait57_model        MM Algorithm\n  Iter      Objective  \n--------  -------------\n       0  -3.024757e+04\n       1  -2.040300e+04\n       2  -1.656070e+04\n       3  -1.528529e+04\n       4  -1.490986e+04\n       5  -1.480638e+04\n       6  -1.477811e+04\n       7  -1.476968e+04\n       8  -1.476639e+04\n       9  -1.476444e+04\n      10  -1.476286e+04\n      20  -1.475000e+04\n      30  -1.474011e+04\n      40  -1.473248e+04\n      50  -1.472658e+04\n      60  -1.472200e+04\n      70  -1.471840e+04\n      80  -1.471555e+04\n      90  -1.471328e+04\n     100  -1.471145e+04\n     110  -1.470997e+04\n     120  -1.470875e+04\n     130  -1.470775e+04\n     140  -1.470691e+04\n     150  -1.470621e+04\n     160  -1.470562e+04\n     170  -1.470511e+04\n     180  -1.470469e+04\n     190  -1.470432e+04\n     200  -1.470400e+04\n     210  -1.470372e+04\n     220  -1.470348e+04\n     230  -1.470326e+04\n     240  -1.470308e+04\n     250  -1.470291e+04\n     260  -1.470276e+04\n     270  -1.470263e+04\n     280  -1.470251e+04\n     290  -1.470241e+04\n     300  -1.470231e+04\n     310  -1.470223e+04\n     320  -1.470215e+04\n     330  -1.470208e+04\n     340  -1.470201e+04\n     350  -1.470195e+04\n     360  -1.470190e+04\n     370  -1.470185e+04\n     380  -1.470180e+04\n     390  -1.470176e+04\n     400  -1.470172e+04\n     410  -1.470168e+04\n     420  -1.470165e+04\n     430  -1.470162e+04\n     440  -1.470159e+04\n     450  -1.470156e+04\n     460  -1.470153e+04\n     470  -1.470151e+04\n     480  -1.470149e+04\n     490  -1.470147e+04\n     500  -1.470145e+04\n     510  -1.470143e+04\n     520  -1.470141e+04\n     530  -1.470139e+04\n     540  -1.470138e+04\n     550  -1.470136e+04\n     560  -1.470135e+04\n     570  -1.470133e+04\n     580  -1.470132e+04\n     590  -1.470131e+04\n     600  -1.470130e+04\n     610  -1.470129e+04\n     620  -1.470128e+04\n     630  -1.470127e+04\n     640  -1.470126e+04\n     650  -1.470125e+04\n     660  -1.470124e+04\n     670  -1.470123e+04\n     680  -1.470122e+04\n     690  -1.470122e+04\n     700  -1.470121e+04\n     710  -1.470120e+04\n     720  -1.470120e+04\n     730  -1.470119e+04\n     740  -1.470118e+04\n     750  -1.470118e+04\n     760  -1.470117e+04\n     770  -1.470117e+04\n     780  -1.470116e+04\n     790  -1.470116e+04\n     800  -1.470115e+04\n     810  -1.470115e+04\n     820  -1.470114e+04\n     830  -1.470114e+04\n     840  -1.470114e+04\n     850  -1.470113e+04\n     860  -1.470113e+04\n     870  -1.470112e+04\n     880  -1.470112e+04\n     890  -1.470112e+04\n     900  -1.470111e+04\n     910  -1.470111e+04\n     920  -1.470111e+04\n     930  -1.470111e+04\n     940  -1.470110e+04\n     950  -1.470110e+04\n     960  -1.470110e+04\n     970  -1.470109e+04\n     980  -1.470109e+04\n     990  -1.470109e+04\n    1000  -1.470109e+04\n    1010  -1.470109e+04\n    1020  -1.470108e+04\n    1030  -1.470108e+04\n    1040  -1.470108e+04\n    1050  -1.470108e+04\n    1060  -1.470108e+04\n    1070  -1.470107e+04\n    1080  -1.470107e+04\n    1090  -1.470107e+04\n    1100  -1.470107e+04\n    1110  -1.470107e+04\n    1120  -1.470107e+04\n\n  0.794377 seconds (168.12 k allocations: 15.640 MiB, 0.80% gc time)\n\n\n\n\n\nVarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}(Array{Float64}(0,3), ([0.2808 0.279454 0.232256; 0.279454 0.284312 0.219977; 0.232256 0.219977 0.213052], [0.717243 0.661816 0.674158; 0.661816 0.715193 0.581746; 0.674158 0.581746 0.783965]), Array{Float64}(0,0), Char[], Float64[], -Inf, Inf)  Heritability from 3-variate estimate and their standard errors.  h ,   hse   =   heritability ( trait57_model . \u03a3 ,   \u03a3cov )  [ h ;   hse ]   2\u00d73 Array{Float64,2}:\n 0.281351   0.284453  0.213689\n 0.0778252  0.077378  0.084084", 
            "title": "3-trait analysis"
        }, 
        {
            "location": "/man/heritability/#13-trait-joint-analysis", 
            "text": "In some situations, such as studying the genetic covariance, we need to jointly analyze 13 traits. We first try the  Fisher scoring algorithm .  # initialize model parameters  traitall_model   =   VarianceComponentModel ( cg10kdata_rotated )  # estimate variance components using Fisher scoring algorithm  @time   mle_fs! ( traitall_model ,   cg10kdata_rotated ;   solver =: Ipopt ,   verbose = true )   This is Ipopt version 3.12.4, running with linear solver mumps.\nNOTE: Other linear solvers might be more efficient (see Ipopt documentation).\n\nNumber of nonzeros in equality constraint Jacobian...:        0\nNumber of nonzeros in inequality constraint Jacobian.:        0\nNumber of nonzeros in Lagrangian Hessian.............:    16653\n\nTotal number of variables............................:      182\n                     variables with only lower bounds:        0\n                variables with lower and upper bounds:        0\n                     variables with only upper bounds:        0\nTotal number of equality constraints.................:        0\nTotal number of inequality constraints...............:        0\n        inequality constraints with only lower bounds:        0\n   inequality constraints with lower and upper bounds:        0\n        inequality constraints with only upper bounds:        0\n\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n   0  1.3113371e+05 0.00e+00 1.00e+02   0.0 0.00e+00    -  0.00e+00 0.00e+00   0 \n   5  8.2233766e+04 0.00e+00 6.03e+02 -11.0 2.32e+00    -  1.00e+00 1.00e+00f  1 MaxS\n  10  1.1960260e+05 0.00e+00 8.76e+02 -11.0 6.20e+01  -5.4 1.00e+00 1.00e+00h  1 MaxS\n  15  2.4416551e+05 0.00e+00 2.50e+02 -11.0 8.69e+02  -7.8 1.00e+00 1.00e+00f  1 MaxS\n\n\n\nDomainError:\nlog will only return a complex result if called with a complex argument. Try log(complex(x)).\n\n\n\nStacktrace:\n\n [1] nan_dom_err at ./math.jl:300 [inlined]\n\n [2] log at ./math.jl:419 [inlined]\n\n [3] logdet(::Array{Float64,2}) at ./linalg/generic.jl:1244\n\n [4] VarianceComponentModels.TwoVarCompModelRotate(::VarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}) at /Users/huazhou/.julia/v0.6/VarianceComponentModels/src/VarianceComponentModels.jl:127\n\n [5] eval_f(::VarianceComponentModels.TwoVarCompOptProb{VarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}},VarianceComponentModels.TwoVarCompVariateRotate{Float64,Array{Float64,2},Array{Float64,2}},Array{Float64,2},Array{Float64,1},VarianceComponentModels.VarianceComponentAuxData{Array{Float64,2},Array{Float64,1}}}, ::Array{Float64,1}) at /Users/huazhou/.julia/v0.6/VarianceComponentModels/src/two_variance_component.jl:683\n\n [6] (::Ipopt.#eval_f_cb#4{VarianceComponentModels.TwoVarCompOptProb{VarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}},VarianceComponentModels.TwoVarCompVariateRotate{Float64,Array{Float64,2},Array{Float64,2}},Array{Float64,2},Array{Float64,1},VarianceComponentModels.VarianceComponentAuxData{Array{Float64,2},Array{Float64,1}}}})(::Array{Float64,1}) at /Users/huazhou/.julia/v0.6/Ipopt/src/IpoptSolverInterface.jl:53\n\n [7] eval_f_wrapper(::Int32, ::Ptr{Float64}, ::Int32, ::Ptr{Float64}, ::Ptr{Void}) at /Users/huazhou/.julia/v0.6/Ipopt/src/Ipopt.jl:89\n\n [8] solveProblem(::Ipopt.IpoptProblem) at /Users/huazhou/.julia/v0.6/Ipopt/src/Ipopt.jl:304\n\n [9] optimize!(::Ipopt.IpoptMathProgModel) at /Users/huazhou/.julia/v0.6/Ipopt/src/IpoptSolverInterface.jl:120\n\n [10] #mle_fs!#29(::Int64, ::Symbol, ::Symbol, ::Bool, ::Function, ::VarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}, ::VarianceComponentModels.TwoVarCompVariateRotate{Float64,Array{Float64,2},Array{Float64,2}}) at /Users/huazhou/.julia/v0.6/VarianceComponentModels/src/two_variance_component.jl:893\n\n [11] (::VarianceComponentModels.#kw##mle_fs!)(::Array{Any,1}, ::VarianceComponentModels.#mle_fs!, ::VarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}, ::VarianceComponentModels.TwoVarCompVariateRotate{Float64,Array{Float64,2},Array{Float64,2}}) at ./ missing :0\n\n [12] include_string(::String, ::String) at ./loading.jl:515  From the output we can see the Fisher scoring algorithm ran into some numerical issues. Let's try the  MM algorithm .  # reset model parameters  traitall_model   =   VarianceComponentModel ( cg10kdata_rotated )  # estimate variance components using Fisher scoring algorithm  @time   mle_mm! ( traitall_model ,   cg10kdata_rotated ;   verbose = true )        MM Algorithm\n  Iter      Objective  \n--------  -------------\n       0  -1.311337e+05\n       1  -8.002108e+04\n       2  -5.806935e+04\n       3  -4.926111e+04\n       4  -4.611059e+04\n       5  -4.511606e+04\n       6  -4.482679e+04\n       7  -4.474294e+04\n       8  -4.471496e+04\n       9  -4.470174e+04\n      10  -4.469246e+04\n      20  -4.462243e+04\n      30  -4.456888e+04\n      40  -4.452774e+04\n      50  -4.449601e+04\n      60  -4.447134e+04\n      70  -4.445199e+04\n      80  -4.443665e+04\n      90  -4.442436e+04\n     100  -4.441442e+04\n     110  -4.440630e+04\n     120  -4.439961e+04\n     130  -4.439405e+04\n     140  -4.438938e+04\n     150  -4.438544e+04\n     160  -4.438210e+04\n     170  -4.437923e+04\n     180  -4.437676e+04\n     190  -4.437463e+04\n     200  -4.437277e+04\n     210  -4.437115e+04\n     220  -4.436972e+04\n     230  -4.436846e+04\n     240  -4.436735e+04\n     250  -4.436636e+04\n     260  -4.436548e+04\n     270  -4.436469e+04\n     280  -4.436399e+04\n     290  -4.436335e+04\n     300  -4.436278e+04\n     310  -4.436226e+04\n     320  -4.436179e+04\n     330  -4.436137e+04\n     340  -4.436098e+04\n     350  -4.436063e+04\n     360  -4.436030e+04\n     370  -4.436001e+04\n     380  -4.435974e+04\n     390  -4.435949e+04\n     400  -4.435926e+04\n     410  -4.435905e+04\n     420  -4.435886e+04\n     430  -4.435868e+04\n     440  -4.435851e+04\n     450  -4.435836e+04\n     460  -4.435822e+04\n     470  -4.435809e+04\n     480  -4.435797e+04\n     490  -4.435785e+04\n     500  -4.435775e+04\n     510  -4.435765e+04\n     520  -4.435756e+04\n     530  -4.435747e+04\n     540  -4.435739e+04\n     550  -4.435732e+04\n     560  -4.435725e+04\n     570  -4.435718e+04\n     580  -4.435712e+04\n     590  -4.435706e+04\n     600  -4.435701e+04\n     610  -4.435696e+04\n     620  -4.435691e+04\n     630  -4.435687e+04\n     640  -4.435683e+04\n     650  -4.435679e+04\n     660  -4.435675e+04\n     670  -4.435671e+04\n     680  -4.435668e+04\n     690  -4.435665e+04\n     700  -4.435662e+04\n     710  -4.435659e+04\n     720  -4.435657e+04\n     730  -4.435654e+04\n     740  -4.435652e+04\n     750  -4.435649e+04\n     760  -4.435647e+04\n     770  -4.435645e+04\n     780  -4.435643e+04\n     790  -4.435642e+04\n     800  -4.435640e+04\n     810  -4.435638e+04\n     820  -4.435637e+04\n     830  -4.435635e+04\n     840  -4.435634e+04\n     850  -4.435633e+04\n     860  -4.435631e+04\n     870  -4.435630e+04\n     880  -4.435629e+04\n     890  -4.435628e+04\n     900  -4.435627e+04\n     910  -4.435626e+04\n     920  -4.435625e+04\n     930  -4.435624e+04\n     940  -4.435623e+04\n     950  -4.435622e+04\n     960  -4.435621e+04\n     970  -4.435621e+04\n     980  -4.435620e+04\n     990  -4.435619e+04\n    1000  -4.435619e+04\n    1010  -4.435618e+04\n    1020  -4.435617e+04\n    1030  -4.435617e+04\n    1040  -4.435616e+04\n    1050  -4.435616e+04\n    1060  -4.435615e+04\n    1070  -4.435615e+04\n    1080  -4.435614e+04\n    1090  -4.435614e+04\n\n  3.551301 seconds (178.42 k allocations: 70.115 MiB, 0.42% gc time)\n\n\n\n\n\n(-44356.138529861186, VarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}(Array{Float64}(0,13), ([0.272384 0.190358 \u2026 -0.128222 -0.0980655; 0.190358 0.21692 \u2026 -0.0689912 -0.0444349; \u2026 ; -0.128222 -0.0689912 \u2026 0.118227 0.0909188; -0.0980655 -0.0444349 \u2026 0.0909188 0.107456], [0.724562 0.56992 \u2026 -0.0590518 -0.124939; 0.56992 0.782639 \u2026 0.0238629 0.0475408; \u2026 ; -0.0590518 0.0238629 \u2026 0.880671 0.550889; -0.124939 0.0475408 \u2026 0.550889 0.891929]), Array{Float64}(0,0), Char[], Float64[], -Inf, Inf), ([0.0111619 0.0131088 \u2026 0.0128956 0.0127641; 0.0131091 0.0151759 \u2026 0.017162 0.0171466; \u2026 ; 0.0128956 0.017162 \u2026 0.0173994 0.0182002; 0.0127643 0.0171461 \u2026 0.0182003 0.0187848], [0.0112235 0.0133094 \u2026 0.0130111 0.0127861; 0.01331 0.0158262 \u2026 0.017867 0.017798; \u2026 ; 0.013011 0.0178666 \u2026 0.0179487 0.0187579; 0.012786 0.0177975 \u2026 0.0187578 0.0193328]), [0.000124587 7.24074e-5 \u2026 -3.35716e-7 -1.40982e-5; 7.24411e-5 0.000171849 \u2026 -2.05381e-5 -3.17975e-6; \u2026 ; -3.60221e-7 -2.05683e-5 \u2026 0.000351859 -1.5168e-5; -1.40799e-5 -3.16738e-6 \u2026 -1.51641e-5 0.000373756], Array{Float64}(0,13), Array{Float64}(0,0))  It converges after ~1000 iterations.", 
            "title": "13-trait joint analysis"
        }, 
        {
            "location": "/man/heritability/#save-analysis-results", 
            "text": "#using JLD  #@save  copd.jld  #whos()", 
            "title": "Save analysis results"
        }, 
        {
            "location": "/man/api/", 
            "text": "API\n\n\nDocumentation for \nVarianceComponentModels.jl\n's types and methods.\n\n\n\n\nIndex\n\n\n\n\nVarianceComponentModels.TwoVarCompModelRotate\n\n\nVarianceComponentModels.TwoVarCompVariateRotate\n\n\nVarianceComponentModels.VarianceComponentModel\n\n\nVarianceComponentModels.VarianceComponentVariate\n\n\nVarianceComponentModels.fit_mle!\n\n\nVarianceComponentModels.fit_reml!\n\n\nVarianceComponentModels.mle_fs!\n\n\nVarianceComponentModels.mle_mm!\n\n\n\n\n\n\nTypes\n\n\n#\n\n\nVarianceComponentModels.VarianceComponentModel\n \n \nType\n.\n\n\nVarianceComponentModel\n stores the model parameters of a variance component model.\n\n\nFields\n\n\n\n\nB\n: \np x d\n mean parameters\n\n\n\u03a3\n: tuple of \nd x d\n variance component parameters\n\n\nA\n: constraint matrix for \nvec(B)\n\n\nsense\n: vector of characters \n'='\n, \n'\n'\n or \n'\n'\n\n\nb\n: constraint vector for \nvec(B)\n\n\nlb\n: lower bounds for \nvec(B)\n\n\nub\n: upper bounds for \nvec(B)\n\n\n\n\nsource\n\n\n#\n\n\nVarianceComponentModels.VarianceComponentVariate\n \n \nType\n.\n\n\nVarianceComponentVariate\n stores the data of a variance component model.\n\n\nFeilds\n\n\n\n\nY\n: \nn x d\n responses\n\n\nX\n: \nn x p\n predictors\n\n\nV\n: tuple of \nn x n\n covariance matrices\n\n\n\n\nsource\n\n\n#\n\n\nVarianceComponentModels.TwoVarCompModelRotate\n \n \nType\n.\n\n\nTwoVarCompModelRotate\n stores the rotated two variance component model.\n\n\nFields\n\n\n\n\nBrot\n: rotated mean parameters \nB * eigvec\n\n\neigval\n: eigenvalues of \neig(\u03a3[1], \u03a3[2])\n\n\neigvec\n: eigenvectors of \neig(\u03a3[1], \u03a3[2])\n\n\nlogdet\u03a32\n: log-determinant of \n\u03a3[2]\n\n\n\n\nsource\n\n\n#\n\n\nVarianceComponentModels.TwoVarCompVariateRotate\n \n \nType\n.\n\n\nTwoVarCompVariateRotate\n stores the rotated two variance component data.\n\n\nFields\n\n\n\n\nYrot\n: rotated responses \neigvec * Y\n\n\nXrot\n: rotated covariates \neigvec * X\n\n\neigval\n: eigenvalues of \neig(V[1], V[2])\n\n\neigvec\n: eigenvectors of \neig(V[1], V[2])\n\n\nlogdetV2\n: log-determinant of \nV[2]\n\n\n\n\nsource\n\n\n\n\nFunctions\n\n\n#\n\n\nVarianceComponentModels.mle_fs!\n \n \nFunction\n.\n\n\nmle_fs!(vcmodel, vcdatarot; maxiter, solver, qpsolver, verbose)\n\n\n\n\n\nFind MLE by Fisher scoring algorithm.\n\n\nInput\n\n\n\n\nvcmodel\n: two variane component model \nVarianceComponentModel\n, with\n\n\n\n\nvcmodel.B\n and \nvcmodel.\u03a3\n used as starting point\n\n\n\n\nvcdatarot\n: rotated two varianec component data \nTwoVarCompVariateRotate\n\n\n\n\nKeyword\n\n\n\n\nmaxiter::Int\n: maximum number of iterations, default is 1000\n\n\nsolver::Symbol\n: backend nonlinear programming solver, \n:Ipopt\n (default) or \n:Knitro\n\n\nqpsolver::Symbol\n: backend quadratic programming solver, \n:Ipopt\n (default) or \n:Gurobi\n or \nMosek\n\n\nverbose::Bool\n: display information\n\n\n\n\nOutput\n\n\n\n\nmaxlogl\n: log-likelihood at solution\n\n\nvcmodel\n: \nVarianceComponentModel\n with updated model parameters\n\n\n\u03a3se=(\u03a3se[1],\u03a3se[2])\n: standard errors of estimate \n\u03a3=(\u03a3[1],\u03a3[2])\n\n\n\u03a3cov\n: covariance matrix of estimate \n\u03a3=(\u03a3[1],\u03a3[2])\n\n\nBse\n: standard errors of estimate \nB\n\n\nBcov\n: covariance of estimate \nB\n\n\n\n\nsource\n\n\n#\n\n\nVarianceComponentModels.mle_mm!\n \n \nFunction\n.\n\n\nmle_mm!(vcmodel, vcdatarot; maxiter, qpsolver, verbose)\n\n\n\n\n\nFind MLE by minorization-maximization (MM) algorithm.\n\n\nInput\n\n\n\n\nvcmodel\n: two variane component model \nVarianceComponentModel\n, with\n\n\n\n\nvcmodel.B\n and \nvcmodel.\u03a3\n used as starting point\n\n\n\n\nvcdatarot\n: rotated two varianec component data \nTwoVarCompVariateRotate\n\n\n\n\nKeyword\n\n\n\n\nmaxiter::Int\n: maximum number of iterations, default is 1000\n\n\nqpsolver::Symbol\n: backend quadratic programming solver, \n:Ipopt\n (default) or \n:Gurobi\n or \nMosek\n\n\nverbose::Bool\n: display information\n\n\n\n\nOutput\n\n\n\n\nmaxlogl\n: log-likelihood at solution\n\n\nvcmodel\n: \nVarianceComponentModel\n with updated model parameters\n\n\n\u03a3se=(\u03a3se[1],\u03a3se[2])\n: standard errors of estimate \n\u03a3=(\u03a3[1],\u03a3[2])\n\n\n\u03a3cov\n: covariance matrix of estimate \n\u03a3=(\u03a3[1],\u03a3[2])\n\n\nBse\n: standard errors of estimate \nB\n\n\nBcov\n: covariance of estimate \nB\n\n\n\n\nReference\n\n\n\n\nH. Zhou, L. Hu, J. Zhou, and K. Lange (2015) MM algorithms for variance components models. \nhttp://arxiv.org/abs/1509.07426\n\n\n\n\nsource\n\n\n#\n\n\nVarianceComponentModels.fit_mle!\n \n \nFunction\n.\n\n\nfit_mle!(vcmodel, vcdata; algo)\n\n\n\n\n\nFind MLE of variane component model.\n\n\nInput\n\n\n\n\nvcmodel\n: two variane component model \nVarianceComponentModel\n, with\n\n\n\n\nvcmodel.B\n and \nvcmodel.\u03a3\n used as starting point\n\n\n\n\nvcdata\n: two varianec component data \nVarianceComponentVariate\n\n\n\n\nKeyword\n\n\n\n\nalgo::Symbol\n: algorithm, \n:FS\n (Fisher scoring) for \n:MM\n\n\n\n\n(minorization-maximization algorithm)\n\n\nOutput\n\n\n\n\nmaxlogl\n: log-likelihood at solution\n\n\nvcmodel\n: \nVarianceComponentModel\n with updated model parameters\n\n\n\u03a3se=(\u03a3se[1],\u03a3se[2])\n: standard errors of estimate \n\u03a3=(\u03a3[1],\u03a3[2])\n\n\n\u03a3cov\n: covariance matrix of estimate \n\u03a3=(\u03a3[1],\u03a3[2])\n\n\nBse\n: standard errors of estimate \nB\n\n\nBcov\n: covariance of estimate \nB\n\n\n\n\nsource\n\n\n#\n\n\nVarianceComponentModels.fit_reml!\n \n \nFunction\n.\n\n\nfit_reml!(vcmodel, vcdata; algo)\n\n\n\n\n\nFind restricted MLE (REML) of variane component model.\n\n\nInput\n\n\n\n\nvcmodel\n: two variane component model \nVarianceComponentModel\n, with\n\n\n\n\nvcmodel.B\n and \nvcmodel.\u03a3\n used as starting point\n\n\n\n\nvcdata\n: two varianec component data \nVarianceComponentVariate\n\n\n\n\nKeyword\n\n\n\n\nalgo::Symbol\n: algorithm, \n:FS\n (Fisher scoring) for \n:MM\n\n\n\n\n(minorization-maximization algorithm)\n\n\nOutput\n\n\n\n\nmaxlogl\n: log-likelihood at solution\n\n\nvcmodel\n: \nVarianceComponentModel\n with updated model parameters\n\n\n\u03a3se=(\u03a3se[1],\u03a3se[2])\n: standard errors of estimate \n\u03a3=(\u03a3[1],\u03a3[2])\n\n\n\u03a3cov\n: covariance matrix of estimate \n\u03a3=(\u03a3[1],\u03a3[2])\n\n\nBse\n: standard errors of estimate \nB\n\n\nBcov\n: covariance of estimate \nB\n\n\n\n\nsource", 
            "title": "API"
        }, 
        {
            "location": "/man/api/#api", 
            "text": "Documentation for  VarianceComponentModels.jl 's types and methods.", 
            "title": "API"
        }, 
        {
            "location": "/man/api/#index", 
            "text": "VarianceComponentModels.TwoVarCompModelRotate  VarianceComponentModels.TwoVarCompVariateRotate  VarianceComponentModels.VarianceComponentModel  VarianceComponentModels.VarianceComponentVariate  VarianceComponentModels.fit_mle!  VarianceComponentModels.fit_reml!  VarianceComponentModels.mle_fs!  VarianceComponentModels.mle_mm!", 
            "title": "Index"
        }, 
        {
            "location": "/man/api/#types", 
            "text": "#  VarianceComponentModels.VarianceComponentModel     Type .  VarianceComponentModel  stores the model parameters of a variance component model.  Fields   B :  p x d  mean parameters  \u03a3 : tuple of  d x d  variance component parameters  A : constraint matrix for  vec(B)  sense : vector of characters  '=' ,  ' '  or  ' '  b : constraint vector for  vec(B)  lb : lower bounds for  vec(B)  ub : upper bounds for  vec(B)   source  #  VarianceComponentModels.VarianceComponentVariate     Type .  VarianceComponentVariate  stores the data of a variance component model.  Feilds   Y :  n x d  responses  X :  n x p  predictors  V : tuple of  n x n  covariance matrices   source  #  VarianceComponentModels.TwoVarCompModelRotate     Type .  TwoVarCompModelRotate  stores the rotated two variance component model.  Fields   Brot : rotated mean parameters  B * eigvec  eigval : eigenvalues of  eig(\u03a3[1], \u03a3[2])  eigvec : eigenvectors of  eig(\u03a3[1], \u03a3[2])  logdet\u03a32 : log-determinant of  \u03a3[2]   source  #  VarianceComponentModels.TwoVarCompVariateRotate     Type .  TwoVarCompVariateRotate  stores the rotated two variance component data.  Fields   Yrot : rotated responses  eigvec * Y  Xrot : rotated covariates  eigvec * X  eigval : eigenvalues of  eig(V[1], V[2])  eigvec : eigenvectors of  eig(V[1], V[2])  logdetV2 : log-determinant of  V[2]   source", 
            "title": "Types"
        }, 
        {
            "location": "/man/api/#functions", 
            "text": "#  VarianceComponentModels.mle_fs!     Function .  mle_fs!(vcmodel, vcdatarot; maxiter, solver, qpsolver, verbose)  Find MLE by Fisher scoring algorithm.  Input   vcmodel : two variane component model  VarianceComponentModel , with   vcmodel.B  and  vcmodel.\u03a3  used as starting point   vcdatarot : rotated two varianec component data  TwoVarCompVariateRotate   Keyword   maxiter::Int : maximum number of iterations, default is 1000  solver::Symbol : backend nonlinear programming solver,  :Ipopt  (default) or  :Knitro  qpsolver::Symbol : backend quadratic programming solver,  :Ipopt  (default) or  :Gurobi  or  Mosek  verbose::Bool : display information   Output   maxlogl : log-likelihood at solution  vcmodel :  VarianceComponentModel  with updated model parameters  \u03a3se=(\u03a3se[1],\u03a3se[2]) : standard errors of estimate  \u03a3=(\u03a3[1],\u03a3[2])  \u03a3cov : covariance matrix of estimate  \u03a3=(\u03a3[1],\u03a3[2])  Bse : standard errors of estimate  B  Bcov : covariance of estimate  B   source  #  VarianceComponentModels.mle_mm!     Function .  mle_mm!(vcmodel, vcdatarot; maxiter, qpsolver, verbose)  Find MLE by minorization-maximization (MM) algorithm.  Input   vcmodel : two variane component model  VarianceComponentModel , with   vcmodel.B  and  vcmodel.\u03a3  used as starting point   vcdatarot : rotated two varianec component data  TwoVarCompVariateRotate   Keyword   maxiter::Int : maximum number of iterations, default is 1000  qpsolver::Symbol : backend quadratic programming solver,  :Ipopt  (default) or  :Gurobi  or  Mosek  verbose::Bool : display information   Output   maxlogl : log-likelihood at solution  vcmodel :  VarianceComponentModel  with updated model parameters  \u03a3se=(\u03a3se[1],\u03a3se[2]) : standard errors of estimate  \u03a3=(\u03a3[1],\u03a3[2])  \u03a3cov : covariance matrix of estimate  \u03a3=(\u03a3[1],\u03a3[2])  Bse : standard errors of estimate  B  Bcov : covariance of estimate  B   Reference   H. Zhou, L. Hu, J. Zhou, and K. Lange (2015) MM algorithms for variance components models.  http://arxiv.org/abs/1509.07426   source  #  VarianceComponentModels.fit_mle!     Function .  fit_mle!(vcmodel, vcdata; algo)  Find MLE of variane component model.  Input   vcmodel : two variane component model  VarianceComponentModel , with   vcmodel.B  and  vcmodel.\u03a3  used as starting point   vcdata : two varianec component data  VarianceComponentVariate   Keyword   algo::Symbol : algorithm,  :FS  (Fisher scoring) for  :MM   (minorization-maximization algorithm)  Output   maxlogl : log-likelihood at solution  vcmodel :  VarianceComponentModel  with updated model parameters  \u03a3se=(\u03a3se[1],\u03a3se[2]) : standard errors of estimate  \u03a3=(\u03a3[1],\u03a3[2])  \u03a3cov : covariance matrix of estimate  \u03a3=(\u03a3[1],\u03a3[2])  Bse : standard errors of estimate  B  Bcov : covariance of estimate  B   source  #  VarianceComponentModels.fit_reml!     Function .  fit_reml!(vcmodel, vcdata; algo)  Find restricted MLE (REML) of variane component model.  Input   vcmodel : two variane component model  VarianceComponentModel , with   vcmodel.B  and  vcmodel.\u03a3  used as starting point   vcdata : two varianec component data  VarianceComponentVariate   Keyword   algo::Symbol : algorithm,  :FS  (Fisher scoring) for  :MM   (minorization-maximization algorithm)  Output   maxlogl : log-likelihood at solution  vcmodel :  VarianceComponentModel  with updated model parameters  \u03a3se=(\u03a3se[1],\u03a3se[2]) : standard errors of estimate  \u03a3=(\u03a3[1],\u03a3[2])  \u03a3cov : covariance matrix of estimate  \u03a3=(\u03a3[1],\u03a3[2])  Bse : standard errors of estimate  B  Bcov : covariance of estimate  B   source", 
            "title": "Functions"
        }
    ]
}