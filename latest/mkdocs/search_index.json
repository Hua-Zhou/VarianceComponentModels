{
    "docs": [
        {
            "location": "/", 
            "text": "VarianceComponentModels.jl\n\n\nUtilities for fitting and testing variance component models\n\n\nVarianceComponentModels.jl implements computation routines for fitting and testing variance component model of form\n\n\n\n\n\n\\text{vec}(Y) \\sim \\text{Nomral}(X B, \\Sigma_1 \\otimes V_1 + \\cdots + \\Sigma_m \\otimes V_m),\n\n\n\n\n\nwhere $\\otimes$ is the \nKronecker product\n.\n\n\nIn this model, \ndata\n is represented by  \n\n\n\n\nY\n: \nn x d\n response matrix\n\n\nX\n: \nn x p\n covariate matrix\n\n\nV=(V1,...,Vm)\n: a tuple \nm\n \nn x n\n covariance matrices\n\n\n\n\nand \nparameters\n are  \n\n\n\n\nB\n: \np x d\n mean parameter matrix\n\n\n\u03a3=(\u03a31,...,\u03a3m)\n: a tuple of \nm\n \nd x d\n variance components\n\n\n\n\n\n\nPackage Features\n\n\n\n\nMaximum likelihood estimation (MLE) and restricted maximum likelihood estimation (REML) of mean parameters \nB\n and variance component parameters \n\u03a3\n\n\nAllow constrains in the mean parameters \nB\n\n\nChoice of optimization algorithms: \nFisher scoring\n and \nminorization-maximization algorithm\n\n\nHeritability Analysis\n in genetics\n\n\n\n\n\n\nInstallation\n\n\nUse the Julia package manager to install VarianceComponentModels.jl.\n\n\nPkg\n.\nclone\n(\nhttps://github.com/OpenMendel/VarianceComponentModels.jl.git\n)\n\n\n\n\n\n\nThis package supports Julia \n0.4\n and \n0.5\n.\n\n\n\n\nManual Outline\n\n\n\n\nMLE and REML\n\n\nDemo data\n\n\nMaximum likelihood estimation (MLE)\n\n\nRestricted maximum likelihood estimation (REML)\n\n\nOptimization algorithms\n\n\nStarting point\n\n\nConstrained estimation of \nB\n\n\n\n\n\n\nHeritability Analysis\n\n\nRead in binary SNP data\n\n\nSummary statistics of SNP data\n\n\nEmpirical kinship matrix\n\n\nPhenotypes\n\n\nPre-processing data for heritability analysis\n\n\nSave intermediate results\n\n\nHeritability of single traits\n\n\nPairwise traits\n\n\n3-trait analysis\n\n\n13-trait joint analysis\n\n\nSave analysis results", 
            "title": "Home"
        }, 
        {
            "location": "/#variancecomponentmodelsjl", 
            "text": "Utilities for fitting and testing variance component models  VarianceComponentModels.jl implements computation routines for fitting and testing variance component model of form   \n\\text{vec}(Y) \\sim \\text{Nomral}(X B, \\Sigma_1 \\otimes V_1 + \\cdots + \\Sigma_m \\otimes V_m),   where $\\otimes$ is the  Kronecker product .  In this model,  data  is represented by     Y :  n x d  response matrix  X :  n x p  covariate matrix  V=(V1,...,Vm) : a tuple  m   n x n  covariance matrices   and  parameters  are     B :  p x d  mean parameter matrix  \u03a3=(\u03a31,...,\u03a3m) : a tuple of  m   d x d  variance components", 
            "title": "VarianceComponentModels.jl"
        }, 
        {
            "location": "/#package-features", 
            "text": "Maximum likelihood estimation (MLE) and restricted maximum likelihood estimation (REML) of mean parameters  B  and variance component parameters  \u03a3  Allow constrains in the mean parameters  B  Choice of optimization algorithms:  Fisher scoring  and  minorization-maximization algorithm  Heritability Analysis  in genetics", 
            "title": "Package Features"
        }, 
        {
            "location": "/#installation", 
            "text": "Use the Julia package manager to install VarianceComponentModels.jl.  Pkg . clone ( https://github.com/OpenMendel/VarianceComponentModels.jl.git )   This package supports Julia  0.4  and  0.5 .", 
            "title": "Installation"
        }, 
        {
            "location": "/#manual-outline", 
            "text": "MLE and REML  Demo data  Maximum likelihood estimation (MLE)  Restricted maximum likelihood estimation (REML)  Optimization algorithms  Starting point  Constrained estimation of  B    Heritability Analysis  Read in binary SNP data  Summary statistics of SNP data  Empirical kinship matrix  Phenotypes  Pre-processing data for heritability analysis  Save intermediate results  Heritability of single traits  Pairwise traits  3-trait analysis  13-trait joint analysis  Save analysis results", 
            "title": "Manual Outline"
        }, 
        {
            "location": "/man/mle_reml/", 
            "text": "MLE and REML\n\n\n\n\nDemo data\n\n\nFor demonstration, we generate a random data set.\n\n\n# generate data from a d-variate response variane component model\n\n\nsrand\n(\n123\n)\n\n\nn\n \n=\n \n1000\n   \n# no. observations\n\n\nd\n \n=\n \n2\n      \n# dimension of responses\n\n\nm\n \n=\n \n2\n      \n# no. variance components\n\n\np\n \n=\n \n2\n      \n# no. covariates\n\n\n# n-by-p design matrix\n\n\nX\n \n=\n \nrandn\n(\nn\n,\n \np\n)\n\n\n# p-by-d mean component regression coefficient\n\n\nB\n \n=\n \nones\n(\np\n,\n \nd\n)\n  \n\n# a tuple of m covariance matrices\n\n\nV\n \n=\n \nntuple\n(\nx\n \n-\n \nzeros\n(\nn\n,\n \nn\n),\n \nm\n)\n \n\nfor\n \ni\n \n=\n \n1\n:\nm\n-\n1\n\n  \nVi\n \n=\n \nrandn\n(\nn\n,\n \n50\n)\n\n  \ncopy!\n(\nV\n[\ni\n],\n \nVi\n \n*\n \nVi\n)\n\n\nend\n\n\ncopy!\n(\nV\n[\nm\n],\n \neye\n(\nn\n))\n \n# last covarianec matrix is idendity\n\n\n# a tuple of m d-by-d variance component parameters\n\n\n\u03a3\n \n=\n \nntuple\n(\nx\n \n-\n \nzeros\n(\nd\n,\n \nd\n),\n \nm\n)\n \n\nfor\n \ni\n \nin\n \n1\n:\nm\n\n  \n\u03a3i\n \n=\n \nrandn\n(\nd\n,\n \nd\n)\n\n  \ncopy!\n(\n\u03a3\n[\ni\n],\n \n\u03a3i\n \n*\n \n\u03a3i\n)\n\n\nend\n\n\n# form overall nd-by-nd covariance matrix \u03a9\n\n\n\u03a9\n \n=\n \nzeros\n(\nn\n \n*\n \nd\n,\n \nn\n \n*\n \nd\n)\n\n\nfor\n \ni\n \n=\n \n1\n:\nm\n\n  \n\u03a9\n \n+=\n \nkron\n(\n\u03a3\n[\ni\n],\n \nV\n[\ni\n])\n\n\nend\n\n\n\u03a9chol\n \n=\n \ncholfact\n(\n\u03a9\n)\n\n\n# n-by-d responses\n\n\nY\n \n=\n \nX\n \n*\n \nB\n \n+\n \nreshape\n(\n\u03a9chol\n[\n:\nL\n]\n \n*\n \nrandn\n(\nn\n*\nd\n),\n \nn\n,\n \nd\n);\n\n\n\n\n\n\n\n\nMaximum likelihood estimation (MLE)\n\n\nTo find the MLE of parameters $(B,\\Sigma_1,\\ldots,\\Sigma_m)$, we take 3 steps:  \n\n\nStep 1 (Construct data)\n. Construct an instance of \nVarianceComponentVariate\n, which consists fields  \n\n\n\n\nY\n: $n$-by-$d$ responses\n\n\nX\n: $n$-by-$p$ covariate matrix\n\n\nV=(V[1],...,V[m])\n: a tuple of $n$-by-$n$ covariance matrices. The last covariance matrix must be positive definite and usually is the identity matrix.\n\n\n\n\nusing\n \nVarianceComponentModels\n\n\nvcdata\n \n=\n \nVarianceComponentVariate\n(\nY\n,\n \nX\n,\n \nV\n)\n\n\nfieldnames\n(\nvcdata\n)\n\n\n\n\n\n\n3-element Array{Symbol,1}:\n :Y\n :X\n :V\n\n\n\n\n\nIn the absence of covariates $X$, we can simply initialize by \nvcdata = VarianceComponentVariate(Y, V)\n.\n\n\nStep 2 (Construct a model)\n. Construct an instance of \nVarianceComponentModel\n, which consists of fields  \n\n\n\n\nB\n: $n$-by-$p$ mean regression coefficients\n\n\n\u03a3=(\u03a3[1],...,\u03a3[m])\n: variane component parameters respectively.\n\n\n\n\nWhen constructed from a \nVarianceComponentVariate\n instance, the mean parameters $B$ are initialized to be zero and the tuple of variance component parameters $\\Sigma$ to be \n(eye(d),...,eye(d))\n.\n\n\nvcmodel\n \n=\n \nVarianceComponentModel\n(\nvcdata\n)\n\n\nfieldnames\n(\nvcmodel\n)\n\n\n\n\n\n\n7-element Array{Symbol,1}:\n :B    \n :\u03a3    \n :A    \n :sense\n :b    \n :lb   \n :ub\n\n\n\n\n\nvcmodel\n\n\n\n\n\n\nVarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}([0.0 0.0; 0.0 0.0],(\n[1.0 0.0; 0.0 1.0],\n\n[1.0 0.0; 0.0 1.0]),,Char[],Float64[],-Inf,Inf)\n\n\n\n\n\nThe remaining fields \nA\n, \nsense\n, \nb\n, \nlb\n, \nub\n specify (optional) constraints on the mean parameters \nB\n:\n\n\n\n\n\nA * \\text{vec}(B) \\,\\, =(\\text{or } \\ge \\text{or } \\le) \\,\\, b\n\n\n\n\n\n\n\n\nlb \\le \\text{vec}(B) \\le ub\n\n\n\n\n\nA\n is an constraint matrix with $pd$ columns, \nsense\n is a vector of charaters taking values \n'\n'\n, \n'='\n or \n'\n'\n, and \nlb\n and \nub\n are the lower and upper bounds for \nvec(B)\n. By default, \nA\n, \nsense\n, \nb\n are empty, \nlb\n is \n-Inf\n, and \nub\n is \nInf\n. If any constraits are non-trivial, final estimates of \nB\n are enforced to satisfy them.\n\n\nWhen a better initial guess is available, we can initialize by calling \nvcmodel=VarianceComponentModel(B0, \u03a30)\n directly.\n\n\nStep 3 (Fit model)\n. Call optmization routine \nfit_mle!\n. The keywork \nalgo\n dictates the optimization algorithm: \n:MM\n (minorization-maximization algorithm) or \n:FS\n (Fisher scoring algorithm).\n\n\nvcmodel_mle\n \n=\n \ndeepcopy\n(\nvcmodel\n)\n\n\n@time\n \nlogl\n,\n \nvcmodel_mle\n,\n \n\u03a3se\n,\n \n\u03a3cov\n,\n \nBse\n,\n \nBcov\n \n=\n \nfit_mle!\n(\nvcmodel_mle\n,\n \nvcdata\n;\n \nalgo\n \n=\n \n:\nMM\n);\n\n\n\n\n\n\n     MM Algorithm\n  Iter      Objective  \n--------  -------------\n       0  -6.253551e+03\n       1  -3.881454e+03\n       2  -3.853179e+03\n       3  -3.846525e+03\n       4  -3.844906e+03\n       5  -3.844506e+03\n       6  -3.844406e+03\n       7  -3.844381e+03\n       8  -3.844375e+03\n       9  -3.844374e+03\n      10  -3.844373e+03\n\n  0.234076 seconds (9.71 k allocations: 23.934 MB, 1.82% gc time)\n\n\n\n\n\nThe output of \nfit_mle!\n contains  \n\n\n\n\nfinal log-likelihood\n\n\n\n\nlogl\n\n\n\n\n\n\n-3844.3731814180805\n\n\n\n\n\n\n\nfitted model\n\n\n\n\nfieldnames\n(\nvcmodel_mle\n)\n\n\n\n\n\n\n7-element Array{Symbol,1}:\n :B    \n :\u03a3    \n :A    \n :sense\n :b    \n :lb   \n :ub\n\n\n\n\n\nvcmodel_mle\n\n\n\n\n\n\nVarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}([1.092 1.04727; 0.955346 1.01632],(\n[0.380637 -0.305465; -0.305465 4.51938],\n\n[1.84009 0.265569; 0.265569 2.17275]),,Char[],Float64[],-Inf,Inf)\n\n\n\n\n\n\n\nstandard errors of the estimated varianec component parameters\n\n\n\n\n\u03a3se\n\n\n\n\n\n\n(\n[0.0765136 0.263047; 0.263047 0.904332],\n\n[0.0844292 0.0917441; 0.0917441 0.0996927])\n\n\n\n\n\n\n\ncovariance matrix of the variance component parameters estimates\n\n\n\n\n\u03a3cov\n\n\n\n\n\n\n8\u00d78 Array{Float64,2}:\n  0.00585433  -0.00467019  -0.00467019  \u2026  -1.07903e-6   -1.557e-7   \n -0.00467019   0.0691937    0.00372555     -1.557e-7     -1.27444e-6 \n -0.00467019   0.00372555   0.0691937      -8.83212e-6   -1.27444e-6 \n  0.00372555  -0.055198    -0.055198       -1.27444e-6   -1.04316e-5 \n -7.4779e-6   -1.07903e-6  -1.07903e-6      0.00102878    0.000148477\n -1.07903e-6  -8.83212e-6  -1.557e-7    \u2026   0.000148477   0.00121477 \n -1.07903e-6  -1.557e-7    -8.83212e-6      0.00841698    0.00121477 \n -1.557e-7    -1.27444e-6  -1.27444e-6      0.00121477    0.00993864\n\n\n\n\n\n\n\nstandard errors of the estimated mean parameters\n\n\n\n\nBse\n\n\n\n\n\n\n2\u00d72 Array{Float64,2}:\n 0.042559   0.0487086\n 0.0430588  0.049178\n\n\n\n\n\n\n\ncovariance matrix of the mean parameter estimates\n\n\n\n\nBcov\n\n\n\n\n\n\n4\u00d74 Array{Float64,2}:\n  0.00181127   -1.98035e-5    0.000240705  -2.59506e-6 \n -1.98035e-5    0.00185406   -2.59506e-6    0.000247285\n  0.000240705  -2.59506e-6    0.00237252   -2.63542e-5 \n -2.59506e-6    0.000247285  -2.63542e-5    0.00241848\n\n\n\n\n\n\n\nRestricted maximum likelihood estimation (REML)\n\n\nREML (restricted maximum likelihood estimation)\n is a popular alternative to the MLE. To find the REML of a variane component model, we replace the above step 3 by  \n\n\nStep 3\n. Call optmization routine \nfit_reml!\n.\n\n\nvcmodel_reml\n \n=\n \ndeepcopy\n(\nvcmodel\n)\n\n\n@time\n \nlogl\n,\n \nvcmodel_reml\n,\n \n\u03a3se\n,\n \n\u03a3cov\n,\n \nBse\n,\n \nBcov\n \n=\n \nfit_reml!\n(\nvcmodel_reml\n,\n \nvcdata\n;\n \nalgo\n \n=\n \n:\nMM\n);\n\n\n\n\n\n\n     MM Algorithm\n  Iter      Objective  \n--------  -------------\n       0  -4.215053e+03\n       1  -3.925799e+03\n       2  -3.865114e+03\n       3  -3.851105e+03\n       4  -3.847732e+03\n       5  -3.846903e+03\n       6  -3.846698e+03\n       7  -3.846647e+03\n       8  -3.846634e+03\n       9  -3.846631e+03\n      10  -3.846630e+03\n\n  0.448162 seconds (10.73 k allocations: 62.676 MB, 2.12% gc time)\n\n\n\n\n\nThe output of \nfit_reml!\n contains\n\n\n\n\nthe final log-likelihood at REML estimate\n\n\n\n\nlogl\n\n\n\n\n\n\n-3844.3777179025096\n\n\n\n\n\n\n\nREML estimates\n\n\n\n\nfieldnames\n(\nvcmodel_reml\n)\n\n\n\n\n\n\n7-element Array{Symbol,1}:\n :B    \n :\u03a3    \n :A    \n :sense\n :b    \n :lb   \n :ub\n\n\n\n\n\nvcmodel_reml\n\n\n\n\n\n\nVarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}([1.092 1.04727; 0.955345 1.01632],(\n[0.380594 -0.305485; -0.305485 4.51994],\n\n[1.84285 0.261963; 0.261963 2.17842]),,Char[],Float64[],-Inf,Inf)\n\n\n\n\n\n\n\nstandard errors of the estimated varianec component parameters\n\n\n\n\n\u03a3se\n\n\n\n\n\n\n(\n[0.0765055 0.26305; 0.26305 0.904446],\n\n[0.0845559 0.0919325; 0.0919325 0.0999526])\n\n\n\n\n\n\n\ncovariance matrix of the variance component parameters estimates\n\n\n\n\n\u03a3cov\n\n\n\n\n\n\n8\u00d78 Array{Float64,2}:\n  0.0058531   -0.00467005  -0.00467005  \u2026  -1.06597e-6   -1.51499e-7 \n -0.00467005   0.0691951    0.00372613     -1.51499e-7   -1.26041e-6 \n -0.00467005   0.00372613   0.0691951      -8.86843e-6   -1.26041e-6 \n  0.00372613  -0.0552092   -0.0552092      -1.26041e-6   -1.0486e-5  \n -7.50035e-6  -1.06597e-6  -1.06597e-6      0.00101633    0.000144472\n -1.06597e-6  -8.86843e-6  -1.51499e-7  \u2026   0.000144472   0.0012014  \n -1.06597e-6  -1.51499e-7  -8.86843e-6      0.00845158    0.0012014  \n -1.51499e-7  -1.26041e-6  -1.26041e-6      0.0012014     0.00999052\n\n\n\n\n\n\n\nstandard errors of the estimated mean parameters\n\n\n\n\nBse\n\n\n\n\n\n\n2\u00d72 Array{Float64,2}:\n 0.0425909  0.0487744\n 0.043091   0.0492444\n\n\n\n\n\n\n\ncovariance matrix of the mean parameter estimates\n\n\n\n\nBcov\n\n\n\n\n\n\n4\u00d74 Array{Float64,2}:\n  0.00181398   -1.98331e-5    0.000237127  -2.55589e-6 \n -1.98331e-5    0.00185683   -2.55589e-6    0.000243624\n  0.000237127  -2.55589e-6    0.00237894   -2.6426e-5  \n -2.55589e-6    0.000243624  -2.6426e-5     0.00242501\n\n\n\n\n\n\n\nOptimization algorithms\n\n\nFinding the MLE or REML of variance component models is a non-trivial nonlinear optimization problem. The main complications are the non-convexity of objective function and the positive semi-definiteness constraint of variane component parameters $\\Sigma_1,\\ldots,\\Sigma_m$. In specific applications, users should try different algorithms with different starting points in order to find a better solution. Here are some tips for efficient computation. \n\n\nIn general the optimization algorithm needs to invert the $nd$ by $nd$ overall covariance matrix $\\Omega = \\Sigma_1 \\otimes V_1 + \\cdots + \\Sigma_m \\otimes V_m$ in each iteration. Inverting a matrix is an expensive operation with $O(n^3 d^3)$ floating operations. When there are only \ntwo\n varianec components ($m=2$), this tedious task can be avoided by taking one (generalized) eigendecomposion of $(V_1, V_2)$ and rotating data $(Y, X)$ by the eigen-vectors. \n\n\nvcdatarot\n \n=\n \nTwoVarCompVariateRotate\n(\nvcdata\n)\n\n\nfieldnames\n(\nvcdatarot\n)\n\n\n\n\n\n\n5-element Array{Symbol,1}:\n :Yrot    \n :Xrot    \n :eigval  \n :eigvec  \n :logdetV2\n\n\n\n\n\nTwo optimization algorithms are implemented: \nFisher scoring\n (\nmle_fs!\n) and the \nminorization-maximization (MM) algorithm\n (\nmle_mm!\n). Both take the rotated data as input. These two functions give finer control of the optimization algorithms. Generally speaking, MM algorithm is more stable while Fisher scoring (if it converges) yields more accurate answer.\n\n\nvcmodel_mm\n \n=\n \ndeepcopy\n(\nvcmodel\n)\n\n\n@time\n \nmle_mm!\n(\nvcmodel_mm\n,\n \nvcdatarot\n;\n \nmaxiter\n=\n10000\n,\n \nfuntol\n=\n1e-8\n,\n \nverbose\n \n=\n \ntrue\n);\n\n\n\n\n\n\n     MM Algorithm\n  Iter      Objective  \n--------  -------------\n       0  -6.253551e+03\n       1  -3.881454e+03\n       2  -3.853179e+03\n       3  -3.846525e+03\n       4  -3.844906e+03\n       5  -3.844506e+03\n       6  -3.844406e+03\n       7  -3.844381e+03\n       8  -3.844375e+03\n       9  -3.844374e+03\n      10  -3.844373e+03\n\n  0.022691 seconds (9.67 k allocations: 655.344 KB)\n\n\n\n\n\n# MM estimates\n\n\nvcmodel_mm\n.\nB\n\n\n\n\n\n\n2\u00d72 Array{Float64,2}:\n 1.092     1.04727\n 0.955346  1.01632\n\n\n\n\n\n# MM estimates\n\n\nvcmodel_mm\n.\n\u03a3\n\n\n\n\n\n\n(\n[0.380637 -0.305465; -0.305465 4.51938],\n\n[1.84009 0.265569; 0.265569 2.17275])\n\n\n\n\n\nFisher scoring (\nmle_fs!\n) uses either \nIpopt.jl\n (keyword \nsolver=:Ipopt\n) or \nKNITRO.jl\n (keyword \nsolver=:Knitro\n) as the backend solver. Ipopt is open source and installation of \nIpopt.jl\n package alone is sufficient.\n\n\n# Fisher scoring using Ipopt\n\n\nvcmodel_ipopt\n \n=\n \ndeepcopy\n(\nvcmodel\n)\n\n\n@time\n \nmle_fs!\n(\nvcmodel_ipopt\n,\n \nvcdatarot\n;\n \nsolver\n=:\nIpopt\n,\n \nmaxiter\n=\n1000\n,\n \nverbose\n=\ntrue\n);\n\n\n\n\n\n\nThis is Ipopt version 3.12.4, running with linear solver mumps.\nNOTE: Other linear solvers might be more efficient (see Ipopt documentation).\n\nNumber of nonzeros in equality constraint Jacobian...:        0\nNumber of nonzeros in inequality constraint Jacobian.:        0\nNumber of nonzeros in Lagrangian Hessian.............:       21\n\nTotal number of variables............................:        6\n                     variables with only lower bounds:        0\n                variables with lower and upper bounds:        0\n                     variables with only upper bounds:        0\nTotal number of equality constraints.................:        0\nTotal number of inequality constraints...............:        0\n        inequality constraints with only lower bounds:        0\n   inequality constraints with lower and upper bounds:        0\n        inequality constraints with only upper bounds:        0\n\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n   0  4.2109423e+03 0.00e+00 1.00e+02   0.0 0.00e+00    -  0.00e+00 0.00e+00   0 \n   5  3.8445586e+03 0.00e+00 7.87e-01 -11.0 4.94e-02    -  1.00e+00 1.00e+00f  1 MaxS\n  10  3.8443870e+03 0.00e+00 2.25e-01 -11.0 1.38e-02    -  1.00e+00 1.00e+00f  1 MaxS\n  15  3.8443742e+03 0.00e+00 6.23e-02 -11.0 3.78e-03    -  1.00e+00 1.00e+00f  1 MaxS\n  20  3.8443733e+03 0.00e+00 1.70e-02 -11.0 1.03e-03    -  1.00e+00 1.00e+00f  1 MaxS\n  25  3.8443732e+03 0.00e+00 4.61e-03 -11.0 2.79e-04    -  1.00e+00 1.00e+00f  1 MaxS\n  30  3.8443732e+03 0.00e+00 1.25e-03 -11.0 7.56e-05    -  1.00e+00 1.00e+00f  1 MaxS\n  35  3.8443732e+03 0.00e+00 3.39e-04 -11.0 2.05e-05    -  1.00e+00 1.00e+00f  1 MaxS\n  40  3.8443732e+03 0.00e+00 9.19e-05 -11.0 5.55e-06    -  1.00e+00 1.00e+00f  1 MaxS\n  45  3.8443732e+03 0.00e+00 2.49e-05 -11.0 1.51e-06    -  1.00e+00 1.00e+00f  1 MaxS\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n  50  3.8443732e+03 0.00e+00 6.76e-06 -11.0 4.08e-07    -  1.00e+00 1.00e+00f  1 MaxSA\n  55  3.8443732e+03 0.00e+00 1.83e-06 -11.0 1.11e-07    -  1.00e+00 1.00e+00f  1 MaxSA\n  60  3.8443732e+03 0.00e+00 4.97e-07 -11.0 3.00e-08    -  1.00e+00 1.00e+00f  1 MaxSA\n\nNumber of Iterations....: 63\n\n                                   (scaled)                 (unscaled)\nObjective...............:   3.4496886481728075e+02    3.8443731733053696e+03\nDual infeasibility......:   2.2693631692678575e-07    2.5290047242499938e-06\nConstraint violation....:   0.0000000000000000e+00    0.0000000000000000e+00\nComplementarity.........:   0.0000000000000000e+00    0.0000000000000000e+00\nOverall NLP error.......:   2.2693631692678575e-07    2.5290047242499938e-06\n\n\nNumber of objective function evaluations             = 64\nNumber of objective gradient evaluations             = 64\nNumber of equality constraint evaluations            = 0\nNumber of inequality constraint evaluations          = 0\nNumber of equality constraint Jacobian evaluations   = 0\nNumber of inequality constraint Jacobian evaluations = 0\nNumber of Lagrangian Hessian evaluations             = 63\nTotal CPU secs in IPOPT (w/o function evaluations)   =      0.018\nTotal CPU secs in NLP function evaluations           =      0.244\n\nEXIT: Solved To Acceptable Level.\n  0.285427 seconds (104.32 k allocations: 7.336 MB)\n\n\n\n\n\n# Ipopt estimates\n\n\nvcmodel_ipopt\n.\nB\n\n\n\n\n\n\n2\u00d72 Array{Float64,2}:\n 1.092     1.04727\n 0.955346  1.01632\n\n\n\n\n\n# Ipopt estimates\n\n\nvcmodel_ipopt\n.\n\u03a3\n\n\n\n\n\n\n(\n[0.380552 -0.305594; -0.305594 4.52106],\n\n[1.84008 0.265385; 0.265385 2.17287])\n\n\n\n\n\nKnitro is a commercial software and users need to follow instructions at \nKNITRO.jl\n for proper functioning. Following code invoke Knitro as the backend optimization solver.\n\n\nusing\n \nKNITRO\n\n\n\n# Fisher scoring using Knitro\n\n\nvcmodel_knitro\n \n=\n \ndeepcopy\n(\nvcmodel\n)\n\n\n@time\n \nmle_fs!\n(\nvcmodel_knitro\n,\n \nvcdatarot\n;\n \nsolver\n=:\nKnitro\n,\n \nmaxiter\n=\n1000\n,\n \nverbose\n=\ntrue\n);\n\n\n\n# Knitro estimates\n\n\nvcmodel_knitro\n.\nB\n\n\n\n# Knitro estimates\n\n\nvcmodel_knitro\n.\n\u03a3\n\n\n\n\n\n\n\n\nStarting point\n\n\nHere are a few strategies for successful optimization. \n\n\n\n\nFor $d\n1$ (multivariate response), initialize $B, \\Sigma$ from univariate estimates.\n\n\nUse REML estimate as starting point for MLE.\n\n\nWhen there are only $m=2$ variance components, pre-compute \nTwoVarCompVariateRotate\n and use it for optimization.\n\n\n\n\n\n\nConstrained estimation of \nB\n\n\nMany applications invoke constraints on the mean parameters \nB\n. For demonstration, we enforce \nB[1,1]=B[1,2]\n and all entries of \nB\n are within [0, 2].\n\n\n# set up constraints on B\n\n\nvcmodel_constr\n \n=\n \ndeepcopy\n(\nvcmodel\n)\n\n\nvcmodel_constr\n.\nA\n \n=\n \n[\n1.0\n \n0.0\n \n-\n1.0\n \n0.0\n]\n\n\nvcmodel_constr\n.\nsense\n \n=\n \n=\n\n\nvcmodel_constr\n.\nb\n \n=\n \n0.0\n\n\nvcmodel_constr\n.\nlb\n \n=\n \n0.0\n\n\nvcmodel_constr\n.\nub\n \n=\n \n2.0\n\n\nvcmodel_constr\n\n\n\n\n\n\nVarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}([0.0 0.0; 0.0 0.0],(\n[1.0 0.0; 0.0 1.0],\n\n[1.0 0.0; 0.0 1.0]),[1.0 0.0 -1.0 0.0],\n=\n,0.0,0.0,2.0)\n\n\n\n\n\nWe first try the MM algorithm.\n\n\n# MM algorithm for constrained estimation of B\n\n\n@time\n \nmle_mm!\n(\nvcmodel_constr\n,\n \nvcdatarot\n;\n \nmaxiter\n=\n10000\n,\n \nfuntol\n=\n1e-8\n,\n \nverbose\n \n=\n \ntrue\n);\n\n\n\n\n\n\n     MM Algorithm\n  Iter      Objective  \n--------  -------------\n       0  -6.253551e+03\n       1  -3.881820e+03\n       2  -3.853477e+03\n       3  -3.846807e+03\n       4  -3.845184e+03\n       5  -3.844783e+03\n       6  -3.844683e+03\n       7  -3.844658e+03\n       8  -3.844652e+03\n       9  -3.844650e+03\n      10  -3.844650e+03\n\n  0.045048 seconds (11.02 k allocations: 743.094 KB)\n\n\n\n\n\nfieldnames\n(\nvcmodel_constr\n)\n\n\n\n\n\n\n7-element Array{Symbol,1}:\n :B    \n :\u03a3    \n :A    \n :sense\n :b    \n :lb   \n :ub\n\n\n\n\n\nvcmodel_constr\n.\nB\n\n\n\n\n\n\n2\u00d72 Array{Float64,2}:\n 1.07177   1.07177\n 0.955683  1.01591\n\n\n\n\n\nvcmodel_constr\n.\n\u03a3\n\n\n\n\n\n\n(\n[0.380624 -0.305498; -0.305498 4.51948],\n\n[1.84051 0.265065; 0.265065 2.17336])\n\n\n\n\n\nNow let's try Fisher scoring.\n\n\n# Fisher scoring using Ipopt for constrained estimation of B\n\n\nvcmodel_constr\n \n=\n \ndeepcopy\n(\nvcmodel\n)\n\n\nvcmodel_constr\n.\nA\n \n=\n \n[\n1.0\n \n0.0\n \n-\n1.0\n \n0.0\n]\n\n\nvcmodel_constr\n.\nsense\n \n=\n \n=\n\n\nvcmodel_constr\n.\nb\n \n=\n \n0.0\n\n\nvcmodel_constr\n.\nlb\n \n=\n \n0.0\n\n\nvcmodel_constr\n.\nub\n \n=\n \n2.0\n\n\nvcmodel_constr\n\n\n@time\n \nmle_fs!\n(\nvcmodel_constr\n,\n \nvcdatarot\n;\n \nsolver\n=:\nIpopt\n,\n \nmaxiter\n=\n1000\n,\n \nverbose\n=\ntrue\n);\n\n\n\n\n\n\nThis is Ipopt version 3.12.4, running with linear solver mumps.\nNOTE: Other linear solvers might be more efficient (see Ipopt documentation).\n\nNumber of nonzeros in equality constraint Jacobian...:        0\nNumber of nonzeros in inequality constraint Jacobian.:        0\nNumber of nonzeros in Lagrangian Hessian.............:       21\n\nTotal number of variables............................:        6\n                     variables with only lower bounds:        0\n                variables with lower and upper bounds:        0\n                     variables with only upper bounds:        0\nTotal number of equality constraints.................:        0\nTotal number of inequality constraints...............:        0\n        inequality constraints with only lower bounds:        0\n   inequality constraints with lower and upper bounds:        0\n        inequality constraints with only upper bounds:        0\n\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n   0  4.2114270e+03 0.00e+00 1.00e+02   0.0 0.00e+00    -  0.00e+00 0.00e+00   0 \n   5  3.8448353e+03 0.00e+00 7.87e-01 -11.0 4.94e-02    -  1.00e+00 1.00e+00f  1 MaxS\n  10  3.8446636e+03 0.00e+00 2.25e-01 -11.0 1.38e-02    -  1.00e+00 1.00e+00f  1 MaxS\n  15  3.8446509e+03 0.00e+00 6.23e-02 -11.0 3.78e-03    -  1.00e+00 1.00e+00f  1 MaxS\n  20  3.8446499e+03 0.00e+00 1.70e-02 -11.0 1.03e-03    -  1.00e+00 1.00e+00f  1 MaxS\n  25  3.8446498e+03 0.00e+00 4.61e-03 -11.0 2.79e-04    -  1.00e+00 1.00e+00f  1 MaxS\n  30  3.8446498e+03 0.00e+00 1.25e-03 -11.0 7.56e-05    -  1.00e+00 1.00e+00f  1 MaxS\n  35  3.8446498e+03 0.00e+00 3.39e-04 -11.0 2.05e-05    -  1.00e+00 1.00e+00f  1 MaxS\n  40  3.8446498e+03 0.00e+00 9.19e-05 -11.0 5.56e-06    -  1.00e+00 1.00e+00f  1 MaxS\n  45  3.8446498e+03 0.00e+00 2.49e-05 -11.0 1.51e-06    -  1.00e+00 1.00e+00f  1 MaxS\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n  50  3.8446498e+03 0.00e+00 6.76e-06 -11.0 4.08e-07    -  1.00e+00 1.00e+00f  1 MaxSA\n  55  3.8446498e+03 0.00e+00 1.83e-06 -11.0 1.11e-07    -  1.00e+00 1.00e+00f  1 MaxSA\n  60  3.8446498e+03 0.00e+00 4.97e-07 -11.0 3.00e-08    -  1.00e+00 1.00e+00f  1 MaxSA\n\nNumber of Iterations....: 63\n\n                                   (scaled)                 (unscaled)\nObjective...............:   3.4484507551949008e+02    3.8446498170293403e+03\nDual infeasibility......:   2.2694405349430929e-07    2.5301808715939735e-06\nConstraint violation....:   0.0000000000000000e+00    0.0000000000000000e+00\nComplementarity.........:   0.0000000000000000e+00    0.0000000000000000e+00\nOverall NLP error.......:   2.2694405349430929e-07    2.5301808715939735e-06\n\n\nNumber of objective function evaluations             = 64\nNumber of objective gradient evaluations             = 64\nNumber of equality constraint evaluations            = 0\nNumber of inequality constraint evaluations          = 0\nNumber of equality constraint Jacobian evaluations   = 0\nNumber of inequality constraint Jacobian evaluations = 0\nNumber of Lagrangian Hessian evaluations             = 63\nTotal CPU secs in IPOPT (w/o function evaluations)   =      0.019\nTotal CPU secs in NLP function evaluations           =      0.481\n\nEXIT: Solved To Acceptable Level.\n  0.557942 seconds (126.13 k allocations: 8.715 MB)\n\n\n\n\n\nvcmodel_constr\n.\nB\n\n\n\n\n\n\n2\u00d72 Array{Float64,2}:\n 1.07177   1.07177\n 0.955683  1.01591\n\n\n\n\n\nvcmodel_constr\n.\n\u03a3\n\n\n\n\n\n\n(\n[0.380539 -0.305626; -0.305626 4.52116],\n\n[1.8405 0.264881; 0.264881 2.17348])", 
            "title": "MLE/REML"
        }, 
        {
            "location": "/man/mle_reml/#mle-and-reml", 
            "text": "", 
            "title": "MLE and REML"
        }, 
        {
            "location": "/man/mle_reml/#demo-data", 
            "text": "For demonstration, we generate a random data set.  # generate data from a d-variate response variane component model  srand ( 123 )  n   =   1000     # no. observations  d   =   2        # dimension of responses  m   =   2        # no. variance components  p   =   2        # no. covariates  # n-by-p design matrix  X   =   randn ( n ,   p )  # p-by-d mean component regression coefficient  B   =   ones ( p ,   d )    # a tuple of m covariance matrices  V   =   ntuple ( x   -   zeros ( n ,   n ),   m )   for   i   =   1 : m - 1 \n   Vi   =   randn ( n ,   50 ) \n   copy! ( V [ i ],   Vi   *   Vi )  end  copy! ( V [ m ],   eye ( n ))   # last covarianec matrix is idendity  # a tuple of m d-by-d variance component parameters  \u03a3   =   ntuple ( x   -   zeros ( d ,   d ),   m )   for   i   in   1 : m \n   \u03a3i   =   randn ( d ,   d ) \n   copy! ( \u03a3 [ i ],   \u03a3i   *   \u03a3i )  end  # form overall nd-by-nd covariance matrix \u03a9  \u03a9   =   zeros ( n   *   d ,   n   *   d )  for   i   =   1 : m \n   \u03a9   +=   kron ( \u03a3 [ i ],   V [ i ])  end  \u03a9chol   =   cholfact ( \u03a9 )  # n-by-d responses  Y   =   X   *   B   +   reshape ( \u03a9chol [ : L ]   *   randn ( n * d ),   n ,   d );", 
            "title": "Demo data"
        }, 
        {
            "location": "/man/mle_reml/#maximum-likelihood-estimation-mle", 
            "text": "To find the MLE of parameters $(B,\\Sigma_1,\\ldots,\\Sigma_m)$, we take 3 steps:    Step 1 (Construct data) . Construct an instance of  VarianceComponentVariate , which consists fields     Y : $n$-by-$d$ responses  X : $n$-by-$p$ covariate matrix  V=(V[1],...,V[m]) : a tuple of $n$-by-$n$ covariance matrices. The last covariance matrix must be positive definite and usually is the identity matrix.   using   VarianceComponentModels  vcdata   =   VarianceComponentVariate ( Y ,   X ,   V )  fieldnames ( vcdata )   3-element Array{Symbol,1}:\n :Y\n :X\n :V  In the absence of covariates $X$, we can simply initialize by  vcdata = VarianceComponentVariate(Y, V) .  Step 2 (Construct a model) . Construct an instance of  VarianceComponentModel , which consists of fields     B : $n$-by-$p$ mean regression coefficients  \u03a3=(\u03a3[1],...,\u03a3[m]) : variane component parameters respectively.   When constructed from a  VarianceComponentVariate  instance, the mean parameters $B$ are initialized to be zero and the tuple of variance component parameters $\\Sigma$ to be  (eye(d),...,eye(d)) .  vcmodel   =   VarianceComponentModel ( vcdata )  fieldnames ( vcmodel )   7-element Array{Symbol,1}:\n :B    \n :\u03a3    \n :A    \n :sense\n :b    \n :lb   \n :ub  vcmodel   VarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}([0.0 0.0; 0.0 0.0],(\n[1.0 0.0; 0.0 1.0],\n\n[1.0 0.0; 0.0 1.0]),,Char[],Float64[],-Inf,Inf)  The remaining fields  A ,  sense ,  b ,  lb ,  ub  specify (optional) constraints on the mean parameters  B :   \nA * \\text{vec}(B) \\,\\, =(\\text{or } \\ge \\text{or } \\le) \\,\\, b    \nlb \\le \\text{vec}(B) \\le ub   A  is an constraint matrix with $pd$ columns,  sense  is a vector of charaters taking values  ' ' ,  '='  or  ' ' , and  lb  and  ub  are the lower and upper bounds for  vec(B) . By default,  A ,  sense ,  b  are empty,  lb  is  -Inf , and  ub  is  Inf . If any constraits are non-trivial, final estimates of  B  are enforced to satisfy them.  When a better initial guess is available, we can initialize by calling  vcmodel=VarianceComponentModel(B0, \u03a30)  directly.  Step 3 (Fit model) . Call optmization routine  fit_mle! . The keywork  algo  dictates the optimization algorithm:  :MM  (minorization-maximization algorithm) or  :FS  (Fisher scoring algorithm).  vcmodel_mle   =   deepcopy ( vcmodel )  @time   logl ,   vcmodel_mle ,   \u03a3se ,   \u03a3cov ,   Bse ,   Bcov   =   fit_mle! ( vcmodel_mle ,   vcdata ;   algo   =   : MM );        MM Algorithm\n  Iter      Objective  \n--------  -------------\n       0  -6.253551e+03\n       1  -3.881454e+03\n       2  -3.853179e+03\n       3  -3.846525e+03\n       4  -3.844906e+03\n       5  -3.844506e+03\n       6  -3.844406e+03\n       7  -3.844381e+03\n       8  -3.844375e+03\n       9  -3.844374e+03\n      10  -3.844373e+03\n\n  0.234076 seconds (9.71 k allocations: 23.934 MB, 1.82% gc time)  The output of  fit_mle!  contains     final log-likelihood   logl   -3844.3731814180805   fitted model   fieldnames ( vcmodel_mle )   7-element Array{Symbol,1}:\n :B    \n :\u03a3    \n :A    \n :sense\n :b    \n :lb   \n :ub  vcmodel_mle   VarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}([1.092 1.04727; 0.955346 1.01632],(\n[0.380637 -0.305465; -0.305465 4.51938],\n\n[1.84009 0.265569; 0.265569 2.17275]),,Char[],Float64[],-Inf,Inf)   standard errors of the estimated varianec component parameters   \u03a3se   (\n[0.0765136 0.263047; 0.263047 0.904332],\n\n[0.0844292 0.0917441; 0.0917441 0.0996927])   covariance matrix of the variance component parameters estimates   \u03a3cov   8\u00d78 Array{Float64,2}:\n  0.00585433  -0.00467019  -0.00467019  \u2026  -1.07903e-6   -1.557e-7   \n -0.00467019   0.0691937    0.00372555     -1.557e-7     -1.27444e-6 \n -0.00467019   0.00372555   0.0691937      -8.83212e-6   -1.27444e-6 \n  0.00372555  -0.055198    -0.055198       -1.27444e-6   -1.04316e-5 \n -7.4779e-6   -1.07903e-6  -1.07903e-6      0.00102878    0.000148477\n -1.07903e-6  -8.83212e-6  -1.557e-7    \u2026   0.000148477   0.00121477 \n -1.07903e-6  -1.557e-7    -8.83212e-6      0.00841698    0.00121477 \n -1.557e-7    -1.27444e-6  -1.27444e-6      0.00121477    0.00993864   standard errors of the estimated mean parameters   Bse   2\u00d72 Array{Float64,2}:\n 0.042559   0.0487086\n 0.0430588  0.049178   covariance matrix of the mean parameter estimates   Bcov   4\u00d74 Array{Float64,2}:\n  0.00181127   -1.98035e-5    0.000240705  -2.59506e-6 \n -1.98035e-5    0.00185406   -2.59506e-6    0.000247285\n  0.000240705  -2.59506e-6    0.00237252   -2.63542e-5 \n -2.59506e-6    0.000247285  -2.63542e-5    0.00241848", 
            "title": "Maximum likelihood estimation (MLE)"
        }, 
        {
            "location": "/man/mle_reml/#restricted-maximum-likelihood-estimation-reml", 
            "text": "REML (restricted maximum likelihood estimation)  is a popular alternative to the MLE. To find the REML of a variane component model, we replace the above step 3 by    Step 3 . Call optmization routine  fit_reml! .  vcmodel_reml   =   deepcopy ( vcmodel )  @time   logl ,   vcmodel_reml ,   \u03a3se ,   \u03a3cov ,   Bse ,   Bcov   =   fit_reml! ( vcmodel_reml ,   vcdata ;   algo   =   : MM );        MM Algorithm\n  Iter      Objective  \n--------  -------------\n       0  -4.215053e+03\n       1  -3.925799e+03\n       2  -3.865114e+03\n       3  -3.851105e+03\n       4  -3.847732e+03\n       5  -3.846903e+03\n       6  -3.846698e+03\n       7  -3.846647e+03\n       8  -3.846634e+03\n       9  -3.846631e+03\n      10  -3.846630e+03\n\n  0.448162 seconds (10.73 k allocations: 62.676 MB, 2.12% gc time)  The output of  fit_reml!  contains   the final log-likelihood at REML estimate   logl   -3844.3777179025096   REML estimates   fieldnames ( vcmodel_reml )   7-element Array{Symbol,1}:\n :B    \n :\u03a3    \n :A    \n :sense\n :b    \n :lb   \n :ub  vcmodel_reml   VarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}([1.092 1.04727; 0.955345 1.01632],(\n[0.380594 -0.305485; -0.305485 4.51994],\n\n[1.84285 0.261963; 0.261963 2.17842]),,Char[],Float64[],-Inf,Inf)   standard errors of the estimated varianec component parameters   \u03a3se   (\n[0.0765055 0.26305; 0.26305 0.904446],\n\n[0.0845559 0.0919325; 0.0919325 0.0999526])   covariance matrix of the variance component parameters estimates   \u03a3cov   8\u00d78 Array{Float64,2}:\n  0.0058531   -0.00467005  -0.00467005  \u2026  -1.06597e-6   -1.51499e-7 \n -0.00467005   0.0691951    0.00372613     -1.51499e-7   -1.26041e-6 \n -0.00467005   0.00372613   0.0691951      -8.86843e-6   -1.26041e-6 \n  0.00372613  -0.0552092   -0.0552092      -1.26041e-6   -1.0486e-5  \n -7.50035e-6  -1.06597e-6  -1.06597e-6      0.00101633    0.000144472\n -1.06597e-6  -8.86843e-6  -1.51499e-7  \u2026   0.000144472   0.0012014  \n -1.06597e-6  -1.51499e-7  -8.86843e-6      0.00845158    0.0012014  \n -1.51499e-7  -1.26041e-6  -1.26041e-6      0.0012014     0.00999052   standard errors of the estimated mean parameters   Bse   2\u00d72 Array{Float64,2}:\n 0.0425909  0.0487744\n 0.043091   0.0492444   covariance matrix of the mean parameter estimates   Bcov   4\u00d74 Array{Float64,2}:\n  0.00181398   -1.98331e-5    0.000237127  -2.55589e-6 \n -1.98331e-5    0.00185683   -2.55589e-6    0.000243624\n  0.000237127  -2.55589e-6    0.00237894   -2.6426e-5  \n -2.55589e-6    0.000243624  -2.6426e-5     0.00242501", 
            "title": "Restricted maximum likelihood estimation (REML)"
        }, 
        {
            "location": "/man/mle_reml/#optimization-algorithms", 
            "text": "Finding the MLE or REML of variance component models is a non-trivial nonlinear optimization problem. The main complications are the non-convexity of objective function and the positive semi-definiteness constraint of variane component parameters $\\Sigma_1,\\ldots,\\Sigma_m$. In specific applications, users should try different algorithms with different starting points in order to find a better solution. Here are some tips for efficient computation.   In general the optimization algorithm needs to invert the $nd$ by $nd$ overall covariance matrix $\\Omega = \\Sigma_1 \\otimes V_1 + \\cdots + \\Sigma_m \\otimes V_m$ in each iteration. Inverting a matrix is an expensive operation with $O(n^3 d^3)$ floating operations. When there are only  two  varianec components ($m=2$), this tedious task can be avoided by taking one (generalized) eigendecomposion of $(V_1, V_2)$ and rotating data $(Y, X)$ by the eigen-vectors.   vcdatarot   =   TwoVarCompVariateRotate ( vcdata )  fieldnames ( vcdatarot )   5-element Array{Symbol,1}:\n :Yrot    \n :Xrot    \n :eigval  \n :eigvec  \n :logdetV2  Two optimization algorithms are implemented:  Fisher scoring  ( mle_fs! ) and the  minorization-maximization (MM) algorithm  ( mle_mm! ). Both take the rotated data as input. These two functions give finer control of the optimization algorithms. Generally speaking, MM algorithm is more stable while Fisher scoring (if it converges) yields more accurate answer.  vcmodel_mm   =   deepcopy ( vcmodel )  @time   mle_mm! ( vcmodel_mm ,   vcdatarot ;   maxiter = 10000 ,   funtol = 1e-8 ,   verbose   =   true );        MM Algorithm\n  Iter      Objective  \n--------  -------------\n       0  -6.253551e+03\n       1  -3.881454e+03\n       2  -3.853179e+03\n       3  -3.846525e+03\n       4  -3.844906e+03\n       5  -3.844506e+03\n       6  -3.844406e+03\n       7  -3.844381e+03\n       8  -3.844375e+03\n       9  -3.844374e+03\n      10  -3.844373e+03\n\n  0.022691 seconds (9.67 k allocations: 655.344 KB)  # MM estimates  vcmodel_mm . B   2\u00d72 Array{Float64,2}:\n 1.092     1.04727\n 0.955346  1.01632  # MM estimates  vcmodel_mm . \u03a3   (\n[0.380637 -0.305465; -0.305465 4.51938],\n\n[1.84009 0.265569; 0.265569 2.17275])  Fisher scoring ( mle_fs! ) uses either  Ipopt.jl  (keyword  solver=:Ipopt ) or  KNITRO.jl  (keyword  solver=:Knitro ) as the backend solver. Ipopt is open source and installation of  Ipopt.jl  package alone is sufficient.  # Fisher scoring using Ipopt  vcmodel_ipopt   =   deepcopy ( vcmodel )  @time   mle_fs! ( vcmodel_ipopt ,   vcdatarot ;   solver =: Ipopt ,   maxiter = 1000 ,   verbose = true );   This is Ipopt version 3.12.4, running with linear solver mumps.\nNOTE: Other linear solvers might be more efficient (see Ipopt documentation).\n\nNumber of nonzeros in equality constraint Jacobian...:        0\nNumber of nonzeros in inequality constraint Jacobian.:        0\nNumber of nonzeros in Lagrangian Hessian.............:       21\n\nTotal number of variables............................:        6\n                     variables with only lower bounds:        0\n                variables with lower and upper bounds:        0\n                     variables with only upper bounds:        0\nTotal number of equality constraints.................:        0\nTotal number of inequality constraints...............:        0\n        inequality constraints with only lower bounds:        0\n   inequality constraints with lower and upper bounds:        0\n        inequality constraints with only upper bounds:        0\n\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n   0  4.2109423e+03 0.00e+00 1.00e+02   0.0 0.00e+00    -  0.00e+00 0.00e+00   0 \n   5  3.8445586e+03 0.00e+00 7.87e-01 -11.0 4.94e-02    -  1.00e+00 1.00e+00f  1 MaxS\n  10  3.8443870e+03 0.00e+00 2.25e-01 -11.0 1.38e-02    -  1.00e+00 1.00e+00f  1 MaxS\n  15  3.8443742e+03 0.00e+00 6.23e-02 -11.0 3.78e-03    -  1.00e+00 1.00e+00f  1 MaxS\n  20  3.8443733e+03 0.00e+00 1.70e-02 -11.0 1.03e-03    -  1.00e+00 1.00e+00f  1 MaxS\n  25  3.8443732e+03 0.00e+00 4.61e-03 -11.0 2.79e-04    -  1.00e+00 1.00e+00f  1 MaxS\n  30  3.8443732e+03 0.00e+00 1.25e-03 -11.0 7.56e-05    -  1.00e+00 1.00e+00f  1 MaxS\n  35  3.8443732e+03 0.00e+00 3.39e-04 -11.0 2.05e-05    -  1.00e+00 1.00e+00f  1 MaxS\n  40  3.8443732e+03 0.00e+00 9.19e-05 -11.0 5.55e-06    -  1.00e+00 1.00e+00f  1 MaxS\n  45  3.8443732e+03 0.00e+00 2.49e-05 -11.0 1.51e-06    -  1.00e+00 1.00e+00f  1 MaxS\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n  50  3.8443732e+03 0.00e+00 6.76e-06 -11.0 4.08e-07    -  1.00e+00 1.00e+00f  1 MaxSA\n  55  3.8443732e+03 0.00e+00 1.83e-06 -11.0 1.11e-07    -  1.00e+00 1.00e+00f  1 MaxSA\n  60  3.8443732e+03 0.00e+00 4.97e-07 -11.0 3.00e-08    -  1.00e+00 1.00e+00f  1 MaxSA\n\nNumber of Iterations....: 63\n\n                                   (scaled)                 (unscaled)\nObjective...............:   3.4496886481728075e+02    3.8443731733053696e+03\nDual infeasibility......:   2.2693631692678575e-07    2.5290047242499938e-06\nConstraint violation....:   0.0000000000000000e+00    0.0000000000000000e+00\nComplementarity.........:   0.0000000000000000e+00    0.0000000000000000e+00\nOverall NLP error.......:   2.2693631692678575e-07    2.5290047242499938e-06\n\n\nNumber of objective function evaluations             = 64\nNumber of objective gradient evaluations             = 64\nNumber of equality constraint evaluations            = 0\nNumber of inequality constraint evaluations          = 0\nNumber of equality constraint Jacobian evaluations   = 0\nNumber of inequality constraint Jacobian evaluations = 0\nNumber of Lagrangian Hessian evaluations             = 63\nTotal CPU secs in IPOPT (w/o function evaluations)   =      0.018\nTotal CPU secs in NLP function evaluations           =      0.244\n\nEXIT: Solved To Acceptable Level.\n  0.285427 seconds (104.32 k allocations: 7.336 MB)  # Ipopt estimates  vcmodel_ipopt . B   2\u00d72 Array{Float64,2}:\n 1.092     1.04727\n 0.955346  1.01632  # Ipopt estimates  vcmodel_ipopt . \u03a3   (\n[0.380552 -0.305594; -0.305594 4.52106],\n\n[1.84008 0.265385; 0.265385 2.17287])  Knitro is a commercial software and users need to follow instructions at  KNITRO.jl  for proper functioning. Following code invoke Knitro as the backend optimization solver.  using   KNITRO  # Fisher scoring using Knitro  vcmodel_knitro   =   deepcopy ( vcmodel )  @time   mle_fs! ( vcmodel_knitro ,   vcdatarot ;   solver =: Knitro ,   maxiter = 1000 ,   verbose = true );  # Knitro estimates  vcmodel_knitro . B  # Knitro estimates  vcmodel_knitro . \u03a3", 
            "title": "Optimization algorithms"
        }, 
        {
            "location": "/man/mle_reml/#starting-point", 
            "text": "Here are a few strategies for successful optimization.    For $d 1$ (multivariate response), initialize $B, \\Sigma$ from univariate estimates.  Use REML estimate as starting point for MLE.  When there are only $m=2$ variance components, pre-compute  TwoVarCompVariateRotate  and use it for optimization.", 
            "title": "Starting point"
        }, 
        {
            "location": "/man/mle_reml/#constrained-estimation-of-b", 
            "text": "Many applications invoke constraints on the mean parameters  B . For demonstration, we enforce  B[1,1]=B[1,2]  and all entries of  B  are within [0, 2].  # set up constraints on B  vcmodel_constr   =   deepcopy ( vcmodel )  vcmodel_constr . A   =   [ 1.0   0.0   - 1.0   0.0 ]  vcmodel_constr . sense   =   =  vcmodel_constr . b   =   0.0  vcmodel_constr . lb   =   0.0  vcmodel_constr . ub   =   2.0  vcmodel_constr   VarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}([0.0 0.0; 0.0 0.0],(\n[1.0 0.0; 0.0 1.0],\n\n[1.0 0.0; 0.0 1.0]),[1.0 0.0 -1.0 0.0], = ,0.0,0.0,2.0)  We first try the MM algorithm.  # MM algorithm for constrained estimation of B  @time   mle_mm! ( vcmodel_constr ,   vcdatarot ;   maxiter = 10000 ,   funtol = 1e-8 ,   verbose   =   true );        MM Algorithm\n  Iter      Objective  \n--------  -------------\n       0  -6.253551e+03\n       1  -3.881820e+03\n       2  -3.853477e+03\n       3  -3.846807e+03\n       4  -3.845184e+03\n       5  -3.844783e+03\n       6  -3.844683e+03\n       7  -3.844658e+03\n       8  -3.844652e+03\n       9  -3.844650e+03\n      10  -3.844650e+03\n\n  0.045048 seconds (11.02 k allocations: 743.094 KB)  fieldnames ( vcmodel_constr )   7-element Array{Symbol,1}:\n :B    \n :\u03a3    \n :A    \n :sense\n :b    \n :lb   \n :ub  vcmodel_constr . B   2\u00d72 Array{Float64,2}:\n 1.07177   1.07177\n 0.955683  1.01591  vcmodel_constr . \u03a3   (\n[0.380624 -0.305498; -0.305498 4.51948],\n\n[1.84051 0.265065; 0.265065 2.17336])  Now let's try Fisher scoring.  # Fisher scoring using Ipopt for constrained estimation of B  vcmodel_constr   =   deepcopy ( vcmodel )  vcmodel_constr . A   =   [ 1.0   0.0   - 1.0   0.0 ]  vcmodel_constr . sense   =   =  vcmodel_constr . b   =   0.0  vcmodel_constr . lb   =   0.0  vcmodel_constr . ub   =   2.0  vcmodel_constr  @time   mle_fs! ( vcmodel_constr ,   vcdatarot ;   solver =: Ipopt ,   maxiter = 1000 ,   verbose = true );   This is Ipopt version 3.12.4, running with linear solver mumps.\nNOTE: Other linear solvers might be more efficient (see Ipopt documentation).\n\nNumber of nonzeros in equality constraint Jacobian...:        0\nNumber of nonzeros in inequality constraint Jacobian.:        0\nNumber of nonzeros in Lagrangian Hessian.............:       21\n\nTotal number of variables............................:        6\n                     variables with only lower bounds:        0\n                variables with lower and upper bounds:        0\n                     variables with only upper bounds:        0\nTotal number of equality constraints.................:        0\nTotal number of inequality constraints...............:        0\n        inequality constraints with only lower bounds:        0\n   inequality constraints with lower and upper bounds:        0\n        inequality constraints with only upper bounds:        0\n\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n   0  4.2114270e+03 0.00e+00 1.00e+02   0.0 0.00e+00    -  0.00e+00 0.00e+00   0 \n   5  3.8448353e+03 0.00e+00 7.87e-01 -11.0 4.94e-02    -  1.00e+00 1.00e+00f  1 MaxS\n  10  3.8446636e+03 0.00e+00 2.25e-01 -11.0 1.38e-02    -  1.00e+00 1.00e+00f  1 MaxS\n  15  3.8446509e+03 0.00e+00 6.23e-02 -11.0 3.78e-03    -  1.00e+00 1.00e+00f  1 MaxS\n  20  3.8446499e+03 0.00e+00 1.70e-02 -11.0 1.03e-03    -  1.00e+00 1.00e+00f  1 MaxS\n  25  3.8446498e+03 0.00e+00 4.61e-03 -11.0 2.79e-04    -  1.00e+00 1.00e+00f  1 MaxS\n  30  3.8446498e+03 0.00e+00 1.25e-03 -11.0 7.56e-05    -  1.00e+00 1.00e+00f  1 MaxS\n  35  3.8446498e+03 0.00e+00 3.39e-04 -11.0 2.05e-05    -  1.00e+00 1.00e+00f  1 MaxS\n  40  3.8446498e+03 0.00e+00 9.19e-05 -11.0 5.56e-06    -  1.00e+00 1.00e+00f  1 MaxS\n  45  3.8446498e+03 0.00e+00 2.49e-05 -11.0 1.51e-06    -  1.00e+00 1.00e+00f  1 MaxS\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n  50  3.8446498e+03 0.00e+00 6.76e-06 -11.0 4.08e-07    -  1.00e+00 1.00e+00f  1 MaxSA\n  55  3.8446498e+03 0.00e+00 1.83e-06 -11.0 1.11e-07    -  1.00e+00 1.00e+00f  1 MaxSA\n  60  3.8446498e+03 0.00e+00 4.97e-07 -11.0 3.00e-08    -  1.00e+00 1.00e+00f  1 MaxSA\n\nNumber of Iterations....: 63\n\n                                   (scaled)                 (unscaled)\nObjective...............:   3.4484507551949008e+02    3.8446498170293403e+03\nDual infeasibility......:   2.2694405349430929e-07    2.5301808715939735e-06\nConstraint violation....:   0.0000000000000000e+00    0.0000000000000000e+00\nComplementarity.........:   0.0000000000000000e+00    0.0000000000000000e+00\nOverall NLP error.......:   2.2694405349430929e-07    2.5301808715939735e-06\n\n\nNumber of objective function evaluations             = 64\nNumber of objective gradient evaluations             = 64\nNumber of equality constraint evaluations            = 0\nNumber of inequality constraint evaluations          = 0\nNumber of equality constraint Jacobian evaluations   = 0\nNumber of inequality constraint Jacobian evaluations = 0\nNumber of Lagrangian Hessian evaluations             = 63\nTotal CPU secs in IPOPT (w/o function evaluations)   =      0.019\nTotal CPU secs in NLP function evaluations           =      0.481\n\nEXIT: Solved To Acceptable Level.\n  0.557942 seconds (126.13 k allocations: 8.715 MB)  vcmodel_constr . B   2\u00d72 Array{Float64,2}:\n 1.07177   1.07177\n 0.955683  1.01591  vcmodel_constr . \u03a3   (\n[0.380539 -0.305626; -0.305626 4.52116],\n\n[1.8405 0.264881; 0.264881 2.17348])", 
            "title": "Constrained estimation of B"
        }, 
        {
            "location": "/man/heritability/", 
            "text": "Heritability Analysis\n\n\nAs an application of the variance component model, this note demonstrates the workflow for heritability analysis in genetics, using a sample data set \ncg10k\n with \n6,670\n individuals and \n630,860\n SNPs. Person IDs and phenotype names are masked for privacy. \ncg10k.bed\n, \ncg10k.bim\n, and \ncg10k.fam\n is a set of Plink files in binary format. \ncg10k_traits.txt\n contains 13 phenotypes of the 6,670 individuals.\n\n\n;\nls\n \ncg10k\n*.*\n\n\n\n\n\n\ncg10k.bed\ncg10k.bim\ncg10k.fam\ncg10k_traits.txt\n\n\n\n\n\nMachine information:\n\n\nversioninfo\n()\n\n\n\n\n\n\nJulia Version 0.5.1\nCommit 6445c82 (2017-03-05 13:25 UTC)\nPlatform Info:\n  OS: macOS (x86_64-apple-darwin13.4.0)\n  CPU: Intel(R) Core(TM) i7-4790K CPU @ 4.00GHz\n  WORD_SIZE: 64\n  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Haswell)\n  LAPACK: libopenblas64_\n  LIBM: libopenlibm\n  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)\n\n\n\n\n\n\n\nRead in binary SNP data\n\n\nWe will use the \nSnpArrays.jl\n package to read in binary SNP data and compute the empirical kinship matrix. Issue \n\n\nPkg\n.\nclone\n(\nhttps://github.com/OpenMendel/SnpArrays.jl.git\n)\n\n\n\n\n\n\nwithin \nJulia\n to install the \nSnpArrays\n package.\n\n\nusing\n \nSnpArrays\n\n\n\n\n\n\n# read in genotype data from Plink binary file (~50 secs on my laptop)\n\n\n@time\n \ncg10k\n \n=\n \nSnpArray\n(\ncg10k\n)\n\n\n\n\n\n\n 33.883711 seconds (282.07 k allocations: 1015.002 MB, 0.23% gc time)\n\n\n\n\n\n6670\u00d7630860 SnpArrays.SnpArray{2}:\n (false,true)   (false,true)   (true,true)   \u2026  (true,true)    (true,true) \n (true,true)    (true,true)    (false,true)     (false,true)   (true,false)\n (true,true)    (true,true)    (false,true)     (true,true)    (true,true) \n (true,true)    (true,true)    (true,true)      (false,true)   (true,true) \n (true,true)    (true,true)    (true,true)      (true,true)    (false,true)\n (false,true)   (false,true)   (true,true)   \u2026  (true,true)    (true,true) \n (false,false)  (false,false)  (true,true)      (true,true)    (true,true) \n (true,true)    (true,true)    (true,true)      (true,true)    (false,true)\n (true,true)    (true,true)    (false,true)     (true,true)    (true,true) \n (true,true)    (true,true)    (true,true)      (false,true)   (true,true) \n (true,true)    (true,true)    (false,true)  \u2026  (true,true)    (true,true) \n (false,true)   (false,true)   (true,true)      (true,true)    (false,true)\n (true,true)    (true,true)    (true,true)      (true,true)    (false,true)\n \u22ee                                           \u22f1                             \n (false,true)   (false,true)   (true,true)      (false,true)   (false,true)\n (false,true)   (false,true)   (false,true)     (false,true)   (true,true) \n (true,true)    (true,true)    (false,true)  \u2026  (false,true)   (true,true) \n (false,true)   (false,true)   (true,true)      (true,true)    (false,true)\n (true,true)    (true,true)    (true,true)      (false,true)   (true,true) \n (true,true)    (true,true)    (true,false)     (false,false)  (false,true)\n (true,true)    (true,true)    (true,true)      (true,true)    (false,true)\n (true,true)    (true,true)    (false,true)  \u2026  (true,true)    (true,true) \n (true,true)    (true,true)    (true,true)      (false,true)   (true,true) \n (true,true)    (true,true)    (true,true)      (true,true)    (false,true)\n (false,true)   (false,true)   (true,true)      (true,true)    (true,true) \n (true,true)    (true,true)    (true,true)      (true,true)    (true,true)\n\n\n\n\n\n\n\nSummary statistics of SNP data\n\n\npeople\n,\n \nsnps\n \n=\n \nsize\n(\ncg10k\n)\n\n\n\n\n\n\n(6670,630860)\n\n\n\n\n\n# summary statistics (~50 secs on my laptop)\n\n\n@time\n \nmaf\n,\n \n_\n,\n \nmissings_by_snp\n,\n \n=\n \nsummarize\n(\ncg10k\n);\n\n\n\n\n\n\n 26.436140 seconds (33.44 k allocations: 10.920 MB)\n\n\n\n\n\n# 5 number summary and average MAF (minor allele frequencies)\n\n\nquantile\n(\nmaf\n,\n \n[\n0.0\n \n.\n25\n \n.\n5\n \n.\n75\n \n1.0\n]),\n \nmean\n(\nmaf\n)\n\n\n\n\n\n\n(\n[0.00841726 0.124063 \u2026 0.364253 0.5],\n\n0.24536516625042462)\n\n\n\n\n\n# Pkg.add(\nPlots\n)\n\n\n# Pkg.add(\nPyPlot\n)\n\n\nusing\n \nPlots\n\n\npyplot\n()\n\n\n\nhistogram\n(\nmaf\n,\n \nxlab\n \n=\n \nMinor Allele Frequency (MAF)\n,\n \nlabel\n \n=\n \nMAF\n)\n\n\n\n\n\n\n\n\n# proportion of missing genotypes\n\n\nsum\n(\nmissings_by_snp\n)\n \n/\n \nlength\n(\ncg10k\n)\n\n\n\n\n\n\n0.0013128198764010824\n\n\n\n\n\n# proportion of rare SNPs with maf \n 0.05\n\n\ncountnz\n(\nmaf\n \n.\n \n0.05\n)\n \n/\n \nlength\n(\nmaf\n)\n\n\n\n\n\n\n0.07228069619249913\n\n\n\n\n\n\n\nEmpirical kinship matrix\n\n\nWe estimate empirical kinship based on all SNPs by the genetic relation matrix (GRM). Missing genotypes are imputed on the fly by drawing according to the minor allele frequencies.\n\n\n# GRM using all SNPs (~10 mins on my laptop)\n\n\nsrand\n(\n123\n)\n\n\n@time\n \n\u03a6grm\n \n=\n \ngrm\n(\ncg10k\n;\n \nmethod\n \n=\n \n:\nGRM\n)\n\n\n\n\n\n\n2099.911248 seconds (29.50 G allocations: 441.429 GB, 1.50% gc time)\n\n\n\n\n\n6670\u00d76670 Array{Float64,2}:\n  0.502916      0.00329978   -0.000116213  \u2026  -6.46286e-5   -0.00281229 \n  0.00329978    0.49892      -0.00201992       0.000909871   0.00345573 \n -0.000116213  -0.00201992    0.493632         0.000294565  -0.000349854\n  0.000933977  -0.00320391   -0.0018611       -0.00241682   -0.00127078 \n -7.75429e-5   -0.0036075     0.00181442       0.00213976   -0.00158382 \n  0.00200371    0.000577386   0.0025455    \u2026   0.000943753  -1.82994e-6 \n  0.000558503   0.00241421   -0.0018782        0.001217     -0.00123924 \n -0.000659495   0.00319987   -0.00101496       0.00353646   -0.00024093 \n -0.00102619   -0.00120448   -0.00055462       0.00175586    0.00181899 \n -0.00136838    0.00211996    0.000119128     -0.00147305   -0.00105239 \n -0.00206144    0.000148818  -0.000475177  \u2026  -0.000265522  -0.00106123 \n  0.000951016   0.00167042    0.00183545      -0.000703658  -0.00313334 \n  0.000330442  -0.000904147   0.00301478       0.000754772  -0.00127413 \n  \u22ee                                        \u22f1                            \n  0.00301137    0.00116042    0.00100426       6.67254e-6    0.00307069 \n -0.00214008    0.00270925   -0.00185054      -0.00109935    0.00366816 \n  0.000546739  -0.00242646   -0.00305264   \u2026  -0.000629014   0.00210779 \n -0.00422553   -0.0020713    -0.00109052      -0.000705804  -0.000508055\n -0.00318405   -0.00075385    0.00312377       0.00052883   -3.60969e-5 \n  0.000430196  -0.00197163    0.00268545      -0.00633175   -0.00520337 \n  0.00221429    0.000849792  -0.00101111      -0.000943129  -0.000624419\n -0.00229025   -0.000130598   0.000101853  \u2026   0.000840136  -0.00230224 \n -0.00202917    0.00233007   -0.00131006       0.00197798   -0.000513771\n -0.000964907  -0.000872326  -7.06722e-5       0.00124702   -0.00295844 \n -6.46286e-5    0.000909871   0.000294565      0.500983      0.000525615\n -0.00281229    0.00345573   -0.000349854      0.000525615   0.500792\n\n\n\n\n\n\n\nPhenotypes\n\n\nRead in the phenotype data and compute descriptive statistics.\n\n\n# Pkg.add(\nDataFrames\n)\n\n\nusing\n \nDataFrames\n\n\n\ncg10k_trait\n \n=\n \nreadtable\n(\n\n    \ncg10k_traits.txt\n;\n \n    \nseparator\n \n=\n \n \n,\n\n    \nnames\n \n=\n \n[\n:\nFID\n;\n \n:\nIID\n;\n \n:\nTrait1\n;\n \n:\nTrait2\n;\n \n:\nTrait3\n;\n \n:\nTrait4\n;\n \n:\nTrait5\n;\n \n:\nTrait6\n;\n \n             \n:\nTrait7\n;\n \n:\nTrait8\n;\n \n:\nTrait9\n;\n \n:\nTrait10\n;\n \n:\nTrait11\n;\n \n:\nTrait12\n;\n \n:\nTrait13\n],\n  \n    \neltypes\n \n=\n \n[\nString\n;\n \nString\n;\n \nFloat64\n;\n \nFloat64\n;\n \nFloat64\n;\n \nFloat64\n;\n \nFloat64\n;\n \n               \nFloat64\n;\n \nFloat64\n;\n \nFloat64\n;\n \nFloat64\n;\n \nFloat64\n;\n \nFloat64\n;\n \nFloat64\n;\n \nFloat64\n]\n\n    \n)\n\n\n# do not display FID and IID for privacy\n\n\ncg10k_trait\n[\n:\n,\n \n3\n:\nend\n]\n\n\n\n\n\n\nTrait1\nTrait2\nTrait3\nTrait4\nTrait5\nTrait6\nTrait7\nTrait8\nTrait9\nTrait10\nTrait11\nTrait12\nTrait13\n1\n-1.81573145026234\n-0.94615046147283\n1.11363077580442\n-2.09867121119159\n0.744416614111748\n0.00139171884080131\n0.934732480409667\n-1.22677315418103\n1.1160784277875\n-0.4436280335029\n0.824465656443384\n-1.02852542216546\n-0.394049201727681\n2\n-1.24440094378729\n0.109659992547179\n0.467119394241789\n-1.62131304097589\n1.0566758355683\n0.978946979419181\n1.00014633946047\n0.32487427140228\n1.16232175219696\n2.6922706948705\n3.08263672461047\n1.09064954786013\n0.0256616415357438\n3\n1.45566914502305\n1.53866932923243\n1.09402959376555\n0.586655272226893\n-0.32796454430367\n-0.30337709778827\n-0.0334354881314741\n-0.464463064285437\n-0.3319396273436\n-0.486839089635991\n-1.10648681564373\n-1.42015780427231\n-0.687463456644413\n4\n-0.768809276698548\n0.513490885514249\n0.244263028382142\n-1.31740254475691\n1.19393774326845\n1.17344127734288\n1.08737426675232\n0.536022583732261\n0.802759240762068\n0.234159411749815\n0.394174866891074\n-0.767365892476029\n0.0635385761884935\n5\n-0.264415132547719\n-0.348240421825694\n-0.0239065083413606\n0.00473915802244948\n1.25619191712193\n1.2038883667631\n1.29800739042627\n0.310113660247311\n0.626159861059352\n0.899289129831224\n0.54996783350812\n0.540687809542048\n0.179675416046033\n6\n-1.37617270917293\n-1.47191967744564\n0.291179894254146\n-0.803110740704731\n-0.264239977442213\n-0.260573027836772\n-0.165372266287781\n-0.219257294118362\n1.04702422290318\n-0.0985815534616482\n0.947393438068448\n0.594014812031438\n0.245407436348479\n7\n0.1009416296374\n-0.191615722103455\n-0.567421321596677\n0.378571487240382\n-0.246656179817904\n-0.608810750053858\n0.189081058215596\n-1.27077787326519\n-0.452476199143965\n0.702562877297724\n0.332636218957179\n0.0026916503626181\n0.317117176705358\n8\n-0.319818276367464\n1.35774480657283\n0.818689545938528\n-1.15565531644352\n0.63448368102259\n0.291461908634679\n0.933323714954726\n-0.741083289682492\n0.647477683507572\n-0.970877627077966\n0.220861165411304\n0.852512250237764\n-0.225904624283945\n9\n-0.288334173342032\n0.566082538090752\n0.254958336116175\n-0.652578302869714\n0.668921559277347\n0.978309199170558\n0.122862966041938\n1.4790926378214\n0.0672132424173449\n0.0795903917527827\n0.167532455243232\n0.246915579442139\n0.539932616458363\n10\n-1.15759732583991\n-0.781198583545165\n-0.595807759833517\n-1.00554980260402\n0.789828885933321\n0.571058413379044\n0.951304176233755\n-0.295962982984816\n0.99042002479707\n0.561309366988983\n0.733100030623233\n-1.73467772245684\n-1.35278484330654\n11\n0.740569150459031\n1.40873846755415\n0.734689999440088\n0.0208322841295094\n-0.337440968561619\n-0.458304040611395\n-0.142582512772326\n-0.580392297464107\n-0.684684998101516\n-0.00785381461893456\n-0.712244337518008\n-0.313345561230878\n-0.345419463162219\n12\n-0.675892486454995\n0.279892613829682\n0.267915996308248\n-1.04103665392985\n0.910741715645888\n0.866027618513171\n1.07414431702005\n0.0381751003538302\n0.766355377018601\n-0.340118016143495\n-0.809013958505059\n0.548521663785885\n-0.0201828675962336\n13\n-0.795410435603455\n-0.699989939762738\n0.3991295030063\n-0.510476261900736\n1.51552245416844\n1.28743032939467\n1.53772393250903\n0.133989160117702\n1.02025736886037\n0.499018733899186\n-0.36948273277931\n-1.10153460436318\n-0.598132438886619\n14\n-0.193483122930324\n-0.286021160323518\n-0.691494225262995\n0.0131581678700699\n1.52337470686782\n1.4010638072262\n1.53114620451896\n0.333066483478075\n1.04372480381099\n0.163206783570466\n-0.422883765001728\n-0.383527976713573\n-0.489221907788158\n15\n0.151246203379718\n2.09185108993614\n2.03800472474384\n-1.12474717143531\n1.66557024390713\n1.62535675109576\n1.58751070483655\n0.635852186043776\n0.842577784605979\n0.450761870778952\n-1.39479033623028\n-0.560984107567768\n0.289349776549287\n16\n-0.464608740812712\n0.36127694772303\n1.2327673928287\n-0.826033731086383\n1.43475224709983\n1.74451823818846\n0.211096887484638\n2.64816425140548\n1.02511433146096\n0.11975731603184\n0.0596832073448267\n-0.631231612661616\n-0.207878671782927\n17\n-0.732977488012215\n-0.526223425889779\n0.61657871336593\n-0.55447974332593\n0.947484859025104\n0.936833214138173\n0.972516806335524\n0.290251013865227\n1.01285359725723\n0.516207422283291\n-0.0300689171988194\n0.8787322524583\n0.450254629309513\n18\n-0.167326459622119\n0.175327165487237\n0.287467725892572\n-0.402652532084246\n0.551181509418056\n0.522204743290975\n0.436837660094653\n0.299564933845579\n0.583109520896067\n-0.704415820005353\n-0.730810367994577\n-1.95140580379896\n-0.933504665700164\n19\n1.41159485787418\n1.78722407901017\n0.84397639585364\n0.481278083772991\n-0.0887673728508268\n-0.49957757426858\n0.304195897924847\n-1.23884208383369\n-0.153475724036624\n-0.870486102788329\n0.0955473331150403\n-0.983708050882817\n-0.3563445644514\n20\n-1.42997091652825\n-0.490147045034213\n0.272730237607695\n-1.61029992954153\n0.990787817197748\n0.711687532608184\n1.1885836012715\n-0.371229188075638\n1.24703459239952\n-0.0389162332271516\n0.883495749072872\n2.58988026321017\n3.33539552370368\n21\n-0.147247288176765\n0.12328430415652\n0.617549051912237\n-0.18713077178262\n0.256438107586694\n0.17794983735083\n0.412611806463263\n-0.244809124559737\n0.0947624806136492\n0.723017223849532\n-0.683948354633436\n0.0873751276309269\n-0.262209652750371\n22\n-0.187112676773894\n-0.270777264595619\n-1.01556818551606\n0.0602850568600233\n0.272419757757978\n0.869133161879197\n-0.657519461414234\n2.32388522018189\n-0.999936011525034\n1.44671844178306\n0.971157886040772\n-0.358747904241515\n-0.439657942096136\n23\n-1.82434047163768\n-0.933480446068067\n1.29474003766977\n-1.94545221151036\n0.33584651189654\n0.359201654302844\n0.513652924365886\n-0.073197696696958\n1.57139042812005\n1.53329371326728\n1.82076821859528\n2.22740301867829\n1.50063347195857\n24\n-2.29344084351335\n-2.49161842344418\n0.40383988742336\n-2.36488074752948\n1.4105254831956\n1.42244117147792\n1.17024166272172\n0.84476650176855\n1.79026875432495\n0.648181858970515\n-0.0857231057403538\n-1.02789535292617\n0.491288088952859\n25\n-0.434135932888305\n0.740881989034652\n0.699576357578518\n-1.02405543187775\n0.759529223983713\n0.956656110895288\n0.633299568656589\n0.770733932268516\n0.824988511714526\n1.84287437634769\n1.91045942063443\n-0.502317207869366\n0.132670133448219\n26\n-2.1920969546557\n-2.49465664272271\n0.354854763893431\n-1.93155848635714\n0.941979400289938\n0.978917101414106\n0.894860097289736\n0.463239402831873\n1.12537133317163\n1.70528446191955\n0.717792714479123\n0.645888049108261\n0.783968250169388\n27\n-1.46602269088422\n-1.24921677101897\n0.307977693653039\n-1.55097364660989\n0.618908494474798\n0.662508171662042\n0.475957173906078\n0.484718674597707\n0.401564892028249\n0.55987973254026\n-0.376938143754217\n-0.933982629228218\n0.390013151672955\n28\n-1.83317744236881\n-1.53268787828701\n2.55674262685865\n-1.51827745783835\n0.789409601746455\n0.908747799728588\n0.649971922941479\n0.668373649931667\n1.20058303519903\n0.277963256075637\n1.2504953198275\n3.31370445071638\n2.22035828885342\n29\n-0.784546628243178\n0.276582579543931\n3.01104958800057\n-1.11978843206758\n0.920823858422707\n0.750217689886151\n1.26153730009639\n-0.403363882922417\n0.400667296857811\n-0.217597941303479\n-0.724669537565068\n-0.391945338467193\n-0.650023936358253\n30\n0.464455916345135\n1.3326356122229\n-1.23059563374303\n-0.357975958937414\n1.18249746977104\n1.54315938069757\n-0.60339041154062\n3.38308845958422\n0.823740765148641\n-0.129951318508883\n-0.657979878422938\n-0.499534924074273\n-0.414476569095651\n\n\n\ndescribe\n(\ncg10k_trait\n[\n:\n,\n \n3\n:\nend\n])\n\n\n\n\n\n\nTrait1\nMin      -3.2041280147118\n1st Qu.  -0.645770976594801\nMedian   0.12500996951180798\nMean     0.0022113846331389903\n3rd Qu.  0.7233154897636109\nMax      3.47939787136478\nNAs      0\nNA%      0.0%\n\nTrait2\nMin      -3.51165862877157\n1st Qu.  -0.6426205239938769\nMedian   0.0335172506981786\nMean     0.0013525291443179934\n3rd Qu.  0.6574666174104795\nMax      4.91342267449592\nNAs      0\nNA%      0.0%\n\nTrait3\nMin      -3.93843646263987\n1st Qu.  -0.6409067201835312\nMedian   -0.000782161570259152\nMean     -0.0012959062525954158\n3rd Qu.  0.6371084235689337\nMax      7.91629946619107\nNAs      0\nNA%      0.0%\n\nTrait4\nMin      -3.60840330795393\n1st Qu.  -0.5460856267376792\nMedian   0.228165419346029\nMean     0.0023089259432067487\n3rd Qu.  0.7152907338009037\nMax      3.12768818152017\nNAs      0\nNA%      0.0%\n\nTrait5\nMin      -4.14874907974159\n1st Qu.  -0.6907651815712424\nMedian   0.03103429560265845\nMean     -0.0017903947913742396\n3rd Qu.  0.7349158784775832\nMax      2.71718436484651\nNAs      0\nNA%      0.0%\n\nTrait6\nMin      -3.82479174931095\n1st Qu.  -0.66279630694076\nMedian   0.03624198629403995\nMean     -0.001195980597331062\n3rd Qu.  0.7411755243742835\nMax      2.58972802240228\nNAs      0\nNA%      0.0%\n\nTrait7\nMin      -4.27245540828955\n1st Qu.  -0.6389232588654877\nMedian   0.0698010018021233\nMean     -0.0019890555724116853\n3rd Qu.  0.7104228734967848\nMax      2.65377857124275\nNAs      0\nNA%      0.0%\n\nTrait8\nMin      -5.62548796912517\n1st Qu.  -0.6015747053036895\nMedian   -0.0386301401797661\nMean     0.0006140754882985941\n3rd Qu.  0.5273417705306229\nMax      5.8057022359485\nNAs      0\nNA%      0.0%\n\nTrait9\nMin      -5.38196778211456\n1st Qu.  -0.6014287731518225\nMedian   0.10657100636146799\nMean     -0.0018096522573535152\n3rd Qu.  0.6985667613073132\nMax      2.57193558386964\nNAs      0\nNA%      0.0%\n\nTrait10\nMin      -3.54850550601412\n1st Qu.  -0.6336406665339003\nMedian   -0.0966507436079331\nMean     -0.0004370294352533275\n3rd Qu.  0.498610364573902\nMax      6.53782005410551\nNAs      0\nNA%      0.0%\n\nTrait11\nMin      -3.26491021902041\n1st Qu.  -0.6736846396608624\nMedian   -0.0680437088585371\nMean     -0.0006159181111862523\n3rd Qu.  0.6554864114250585\nMax      4.26240968462615\nNAs      0\nNA%      0.0%\n\nTrait12\nMin      -8.85190902714652\n1st Qu.  -0.5396855871831098\nMedian   -0.1410985990171995\nMean     -0.0005887830910961934\n3rd Qu.  0.35077884186653374\nMax      13.2114017261714\nNAs      0\nNA%      0.0%\n\nTrait13\nMin      -5.59210353493304\n1st Qu.  -0.49228904714392474\nMedian   -0.14102175804213551\nMean     -0.0001512383146454028\n3rd Qu.  0.32480412746681697\nMax      24.174436145414\nNAs      0\nNA%      0.0%\n\n\n\n\n\nY\n \n=\n \nconvert\n(\nMatrix\n{\nFloat64\n},\n \ncg10k_trait\n[\n:\n,\n \n3\n:\n15\n])\n\n\nhistogram\n(\nY\n,\n \nlayout\n \n=\n \n13\n)\n\n\n\n\n\n\n\n\n\n\nPre-processing data for heritability analysis\n\n\nTo prepare variance component model fitting, we form an instance of \nVarianceComponentVariate\n. The two variance components are $(2\\Phi, I)$.\n\n\nusing\n \nVarianceComponentModels\n\n\n\n# form data as VarianceComponentVariate\n\n\ncg10kdata\n \n=\n \nVarianceComponentVariate\n(\nY\n,\n \n(\n2\n\u03a6grm\n,\n \neye\n(\nsize\n(\nY\n,\n \n1\n))))\n\n\nfieldnames\n(\ncg10kdata\n)\n\n\n\n\n\n\n3-element Array{Symbol,1}:\n :Y\n :X\n :V\n\n\n\n\n\ncg10kdata\n\n\n\n\n\n\nVarianceComponentModels.VarianceComponentVariate{Float64,2,Array{Float64,2},Array{Float64,2},Array{Float64,2}}([-1.81573 -0.94615 \u2026 -1.02853 -0.394049; -1.2444 0.10966 \u2026 1.09065 0.0256616; \u2026 ; 0.886626 0.487408 \u2026 -0.636874 -0.439825; -1.24394 0.213697 \u2026 0.299931 0.392809],,(\n[1.00583 0.00659955 \u2026 -0.000129257 -0.00562459; 0.00659955 0.99784 \u2026 0.00181974 0.00691145; \u2026 ; -0.000129257 0.00181974 \u2026 1.00197 0.00105123; -0.00562459 0.00691145 \u2026 0.00105123 1.00158],\n\n[1.0 0.0 \u2026 0.0 0.0; 0.0 1.0 \u2026 0.0 0.0; \u2026 ; 0.0 0.0 \u2026 1.0 0.0; 0.0 0.0 \u2026 0.0 1.0]))\n\n\n\n\n\nBefore fitting the variance component model, we pre-compute the eigen-decomposition of $2\\Phi_{\\text{GRM}}$, the rotated responses, and the constant part in log-likelihood, and store them as a \nTwoVarCompVariateRotate\n instance, which is re-used in various variane component estimation procedures.\n\n\n# pre-compute eigen-decomposition (~50 secs on my laptop)\n\n\n@time\n \ncg10kdata_rotated\n \n=\n \nTwoVarCompVariateRotate\n(\ncg10kdata\n)\n\n\nfieldnames\n(\ncg10kdata_rotated\n)\n\n\n\n\n\n\n 46.051433 seconds (726.58 k allocations: 1.024 GB, 0.45% gc time)\n\n\n\n\n\n5-element Array{Symbol,1}:\n :Yrot    \n :Xrot    \n :eigval  \n :eigvec  \n :logdetV2\n\n\n\n\n\n\n\nSave intermediate results\n\n\nWe don't want to re-compute SnpArray and empirical kinship matrices again and again for heritibility analysis.\n\n\n# Pkg.add(\nJLD\n)\n\n\n#using JLD\n\n\n#@save \ncg10k.jld\n\n\n#whos()\n\n\n\n\n\n\n                          Base  47039 KB     Module\n                       BinDeps    218 KB     Module\n                         Blosc     59 KB     Module\n                    ColorTypes   8492 KB     Module\n                        Colors   8414 KB     Module\n                        Compat   8100 KB     Module\n                         Conda   8491 KB     Module\n                          Core  19714 KB     Module\n                    DataArrays   8444 KB     Module\n                    DataFrames   8940 KB     Module\n                DataStructures    234 KB     Module\n                        FileIO   9044 KB     Module\n             FixedPointNumbers   8573 KB     Module\n               FixedSizeArrays   8222 KB     Module\n                          GZip   8003 KB     Module\n                          HDF5   8809 KB     Module\n                        IJulia 2082578 KB     Module\n                         Ipopt     32 KB     Module\n              IterativeSolvers    333 KB     Module\n                     Iterators     44 KB     Module\n                           JLD   9092 KB     Module\n                          JSON   8124 KB     Module\n                        KNITRO    218 KB     Module\n                  LaTeXStrings   4622 bytes  Module\n                 LegacyStrings     67 KB     Module\n                    MacroTools   8190 KB     Module\n                          Main 2498588 KB     Module\n                  MathProgBase    272 KB     Module\n                      Measures     24 KB     Module\n                        Nettle   8048 KB     Module\n                    PlotThemes   7999 KB     Module\n                     PlotUtils   8264 KB     Module\n                         Plots  12383 KB     Module\n                        PyCall  10435 KB     Module\n                        PyPlot   9938 KB     Module\n                   RecipesBase   8158 KB     Module\n                      Reexport   6178 bytes  Module\n                           SHA     71 KB     Module\n                       Showoff   8024 KB     Module\n                     SnpArrays   8191 KB     Module\n             SortingAlgorithms     28 KB     Module\n              SpecialFunctions   8257 KB     Module\n                     StatsBase   8662 KB     Module\n                     URIParser   8064 KB     Module\n       VarianceComponentModels    189 KB     Module\n                             Y    677 KB     6670\u00d713 Array{Float64,2}\n                           ZMQ   8058 KB     Module\n                             _     77 KB     630860-element BitArray{1}\n                         cg10k 1027303 KB     6670\u00d7630860 SnpArrays.SnpArray{2}\n                   cg10k_trait    978 KB     6670\u00d715 DataFrames.DataFrame\n                     cg10kdata 695816 KB     VarianceComponentModels.VarianceCo\u2026\n             cg10kdata_rotated 348299 KB     VarianceComponentModels.TwoVarComp\u2026\n                           maf   4928 KB     630860-element Array{Float64,1}\n               missings_by_snp   4928 KB     630860-element Array{Int64,1}\n                        people      8 bytes  Int64\n                          snps      8 bytes  Int64\n                          \u03a6grm 347569 KB     6670\u00d76670 Array{Float64,2}\n\n\n\n\n\nTo load workspace\n\n\nusing\n \nSnpArrays\n,\n \nJLD\n,\n \nDataFrames\n,\n \nVarianceComponentModels\n,\n \nPlots\n\n\npyplot\n()\n\n\n@load\n \ncg10k.jld\n\n\nwhos\n()\n\n\n\n\n\n\n                          Base  39406 KB     Module\n                       BinDeps    218 KB     Module\n                         Blosc     59 KB     Module\n                    ColorTypes   6371 KB     Module\n                        Colors   6378 KB     Module\n                        Compat   6099 KB     Module\n                         Conda   6460 KB     Module\n                          Core  15532 KB     Module\n                    DataArrays   6388 KB     Module\n                    DataFrames    649 KB     Module\n                DataStructures    234 KB     Module\n                        FileIO   6870 KB     Module\n             FixedPointNumbers   6579 KB     Module\n               FixedSizeArrays   6226 KB     Module\n                          GZip   6004 KB     Module\n                          HDF5   6657 KB     Module\n                        IJulia   7125 KB     Module\n                         Ipopt     32 KB     Module\n              IterativeSolvers    333 KB     Module\n                     Iterators     44 KB     Module\n                           JLD   6839 KB     Module\n                          JSON   6132 KB     Module\n                        KNITRO    218 KB     Module\n                  LaTeXStrings   4622 bytes  Module\n                 LegacyStrings     68 KB     Module\n                    MacroTools   6196 KB     Module\n                          Main 2489170 KB     Module\n                  MathProgBase    272 KB     Module\n                      Measures     21 KB     Module\n                        Nettle   6068 KB     Module\n                    PlotThemes   6016 KB     Module\n                     PlotUtils   6174 KB     Module\n                         Plots   9209 KB     Module\n                        PyCall   7377 KB     Module\n                        PyPlot   7686 KB     Module\n                   RecipesBase    179 KB     Module\n                      Reexport   6178 bytes  Module\n                           SHA     71 KB     Module\n                       Showoff     27 KB     Module\n                     SnpArrays     98 KB     Module\n             SortingAlgorithms     28 KB     Module\n              SpecialFunctions   6277 KB     Module\n                     StatsBase    573 KB     Module\n                     URIParser   6085 KB     Module\n       VarianceComponentModels    189 KB     Module\n                             Y    677 KB     6670\u00d713 Array{Float64,2}\n                           ZMQ   6076 KB     Module\n                         cg10k 1027303 KB     6670\u00d7630860 SnpArrays.SnpArray{2}\n                   cg10k_trait    978 KB     6670\u00d715 DataFrames.DataFrame\n                     cg10kdata 695816 KB     VarianceComponentModels.VarianceCo\u2026\n             cg10kdata_rotated 348299 KB     VarianceComponentModels.TwoVarComp\u2026\n                           maf   4928 KB     630860-element Array{Float64,1}\n               missings_by_snp   4928 KB     630860-element Array{Int64,1}\n                        people      8 bytes  Int64\n                          snps      8 bytes  Int64\n                          \u03a6grm 347569 KB     6670\u00d76670 Array{Float64,2}\n\n\n\n\n\n\n\nHeritability of single traits\n\n\nWe use Fisher scoring algorithm to fit variance component model for each single trait.\n\n\n# heritability from single trait analysis\n\n\nhST\n \n=\n \nzeros\n(\n13\n)\n\n\n# standard errors of estimated heritability\n\n\nhST_se\n \n=\n \nzeros\n(\n13\n)\n\n\n# additive genetic effects\n\n\n\u03c32a\n \n=\n \nzeros\n(\n13\n)\n\n\n# enviromental effects\n\n\n\u03c32e\n \n=\n \nzeros\n(\n13\n)\n\n\n\ntic\n()\n\n\nfor\n \ntrait\n \nin\n \n1\n:\n13\n\n    \nprintln\n(\nnames\n(\ncg10k_trait\n)[\ntrait\n \n+\n \n2\n])\n\n    \n# form data set for trait j\n\n    \ntraitj_data\n \n=\n \nTwoVarCompVariateRotate\n(\ncg10kdata_rotated\n.\nYrot\n[\n:\n,\n \ntrait\n],\n \ncg10kdata_rotated\n.\nXrot\n,\n \n        \ncg10kdata_rotated\n.\neigval\n,\n \ncg10kdata_rotated\n.\neigvec\n,\n \ncg10kdata_rotated\n.\nlogdetV2\n)\n\n    \n# initialize model parameters\n\n    \ntraitj_model\n \n=\n \nVarianceComponentModel\n(\ntraitj_data\n)\n\n    \n# estimate variance components\n\n    \n_\n,\n \n_\n,\n \n_\n,\n \n\u03a3cov\n,\n \n_\n,\n \n_\n \n=\n \nmle_fs!\n(\ntraitj_model\n,\n \ntraitj_data\n;\n \nsolver\n=:\nIpopt\n,\n \nverbose\n=\nfalse\n)\n\n    \n\u03c32a\n[\ntrait\n]\n \n=\n \ntraitj_model\n.\n\u03a3\n[\n1\n][\n1\n]\n\n    \n\u03c32e\n[\ntrait\n]\n \n=\n \ntraitj_model\n.\n\u03a3\n[\n2\n][\n1\n]\n\n    \n@show\n \n\u03c32a\n[\ntrait\n],\n \n\u03c32e\n[\ntrait\n]\n\n    \nh\n,\n \nhse\n \n=\n \nheritability\n(\ntraitj_model\n.\n\u03a3\n,\n \n\u03a3cov\n)\n\n    \nhST\n[\ntrait\n]\n \n=\n \nh\n[\n1\n]\n\n    \nhST_se\n[\ntrait\n]\n \n=\n \nhse\n[\n1\n]\n\n\nend\n\n\ntoc\n()\n\n\n\n\n\n\nTrait1\n(\u03c32a[trait],\u03c32e[trait]) = (0.26104123217397623,0.7356884432614108)\nTrait2\n(\u03c32a[trait],\u03c32e[trait]) = (0.18874147380283665,0.8106899991616688)\nTrait3\n(\u03c32a[trait],\u03c32e[trait]) = (0.3185719276547346,0.6801458862875847)\nTrait4\n(\u03c32a[trait],\u03c32e[trait]) = (0.26556901333953487,0.7303588364945325)\nTrait5\n(\u03c32a[trait],\u03c32e[trait]) = (0.28123321193922013,0.7167989047155017)\nTrait6\n(\u03c32a[trait],\u03c32e[trait]) = (0.2829461149704479,0.7165629534396428)\nTrait7\n(\u03c32a[trait],\u03c32e[trait]) = (0.21543856403949083,0.7816211121585646)\nTrait8\n(\u03c32a[trait],\u03c32e[trait]) = (0.19412648732666096,0.8055277649986139)\nTrait9\n(\u03c32a[trait],\u03c32e[trait]) = (0.24789561127296741,0.7504615853619878)\nTrait10\n(\u03c32a[trait],\u03c32e[trait]) = (0.10007455815561886,0.899815277360586)\nTrait11\n(\u03c32a[trait],\u03c32e[trait]) = (0.16486778169300415,0.8338002257315682)\nTrait12\n(\u03c32a[trait],\u03c32e[trait]) = (0.08298660416198149,0.9158035668415443)\nTrait13\n(\u03c32a[trait],\u03c32e[trait]) = (0.05684248094794614,0.9423653381325947)\nelapsed time: 0.19565668 seconds\n\n\n\n\n\n0.19565668\n\n\n\n\n\n# heritability and standard errors\n\n\n[\nhST\n;\n \nhST_se\n]\n\n\n\n\n\n\n2\u00d713 Array{Float64,2}:\n 0.261898  0.188849   0.318981   \u2026  0.165088   0.0830871  0.0568875\n 0.079869  0.0867203  0.0741462     0.0887725  0.0944835  0.0953863\n\n\n\n\n\n\n\nPairwise traits\n\n\nJoint analysis of multiple traits is subject to intensive research recently. Following code snippet does joint analysis of all pairs of traits, a total of 78 bivariate variane component models.\n\n\n# additive genetic effects (2x2 psd matrices) from bavariate trait analysis;\n\n\n\u03a3a\n \n=\n \nArray\n{\nMatrix\n{\nFloat64\n}}(\n13\n,\n \n13\n)\n\n\n# environmental effects (2x2 psd matrices) from bavariate trait analysis;\n\n\n\u03a3e\n \n=\n \nArray\n{\nMatrix\n{\nFloat64\n}}(\n13\n,\n \n13\n)\n\n\n\ntic\n()\n\n\nfor\n \ni\n \nin\n \n1\n:\n13\n\n    \nfor\n \nj\n \nin\n \n(\ni\n+\n1\n)\n:\n13\n\n        \nprintln\n(\nnames\n(\ncg10k_trait\n)[\ni\n \n+\n \n2\n],\n \nnames\n(\ncg10k_trait\n)[\nj\n \n+\n \n2\n])\n\n        \n# form data set for (trait1, trait2)\n\n        \ntraitij_data\n \n=\n \nTwoVarCompVariateRotate\n(\ncg10kdata_rotated\n.\nYrot\n[\n:\n,\n \n[\ni\n;\nj\n]],\n \ncg10kdata_rotated\n.\nXrot\n,\n \n            \ncg10kdata_rotated\n.\neigval\n,\n \ncg10kdata_rotated\n.\neigvec\n,\n \ncg10kdata_rotated\n.\nlogdetV2\n)\n\n        \n# initialize model parameters\n\n        \ntraitij_model\n \n=\n \nVarianceComponentModel\n(\ntraitij_data\n)\n\n        \n# estimate variance components\n\n        \nmle_fs!\n(\ntraitij_model\n,\n \ntraitij_data\n;\n \nsolver\n=:\nIpopt\n,\n \nverbose\n=\nfalse\n)\n\n        \n\u03a3a\n[\ni\n,\n \nj\n]\n \n=\n \ntraitij_model\n.\n\u03a3\n[\n1\n]\n\n        \n\u03a3e\n[\ni\n,\n \nj\n]\n \n=\n \ntraitij_model\n.\n\u03a3\n[\n2\n]\n\n        \n@show\n \n\u03a3a\n[\ni\n,\n \nj\n],\n \n\u03a3e\n[\ni\n,\n \nj\n]\n\n    \nend\n\n\nend\n\n\ntoc\n()\n\n\n\n\n\n\nTrait1Trait2\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.260119 0.176216; 0.176216 0.187376],\n\n[0.736589 0.583892; 0.583892 0.812033])\nTrait1Trait3\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.261564 -0.0131268; -0.0131268 0.319057],\n\n[0.73518 -0.121127; -0.121127 0.679679])\nTrait1Trait4\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.26088 0.222614; 0.222614 0.265581],\n\n[0.735846 0.599435; 0.599435 0.730347])\nTrait1Trait5\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.260783 -0.147012; -0.147012 0.281877],\n\n[0.735937 -0.254584; -0.254584 0.716176])\nTrait1Trait6\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.260707 -0.129356; -0.129356 0.283188],\n\n[0.736013 -0.231361; -0.231361 0.716329])\nTrait1Trait7\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.260308 -0.140258; -0.140258 0.215081],\n\n[0.736406 -0.197805; -0.197805 0.781985])\nTrait1Trait8\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.261035 -0.0335296; -0.0335296 0.194143],\n\n[0.735695 -0.126272; -0.126272 0.805512])\nTrait1Trait9\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.263016 -0.204865; -0.204865 0.246796],\n\n[0.733794 -0.30745; -0.30745 0.751544])\nTrait1Trait10\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.260898 -0.0998176; -0.0998176 0.0970233],\n\n[0.735828 -0.303609; -0.303609 0.902853])\nTrait1Trait11\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.26074 -0.138983; -0.138983 0.163063],\n\n[0.735982 -0.359175; -0.359175 0.835595])\nTrait1Trait12\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.263069 -0.145536; -0.145536 0.0805136],\n\n[0.733781 -0.0416975; -0.0416975 0.918359])\nTrait1Trait13\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.262344 -0.108896; -0.108896 0.051294],\n\n[0.73445 -0.113996; -0.113996 0.947942])\nTrait2Trait3\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.189015 0.146157; 0.146157 0.320529],\n\n[0.810418 0.0974992; 0.0974992 0.678271])\nTrait2Trait4\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.188395 0.0752146; 0.0752146 0.265558],\n\n[0.81103 0.220495; 0.220495 0.730369])\nTrait2Trait5\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.188716 -0.011314; -0.011314 0.281247],\n\n[0.810715 -0.0370105; -0.0370105 0.716786])\nTrait2Trait6\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.188774 -0.0031066; -0.0031066 0.283013],\n\n[0.810658 -0.0211827; -0.0211827 0.716499])\nTrait2Trait7\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.188352 -0.0299579; -0.0299579 0.215189],\n\n[0.811072 -0.00136939; -0.00136939 0.781868])\nTrait2Trait8\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.189262 0.0331423; 0.0331423 0.194666],\n\n[0.810182 -0.0326003; -0.0326003 0.805005])\nTrait2Trait9\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.187285 -0.0854146; -0.0854146 0.246719],\n\n[0.812133 -0.0808791; -0.0808791 0.751617])\nTrait2Trait10\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.188965 -0.125319; -0.125319 0.100121],\n\n[0.810498 -0.271071; -0.271071 0.899849])\nTrait2Trait11\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.187762 -0.118479; -0.118479 0.166273],\n\n[0.811653 -0.295549; -0.295549 0.832437])\nTrait2Trait12\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.188191 -0.0905383; -0.0905383 0.0822634],\n\n[0.811272 0.045422; 0.045422 0.916586])\nTrait2Trait13\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.18826 -0.0707041; -0.0707041 0.0547239],\n\n[0.811217 0.0737977; 0.0737977 0.944521])\nTrait3Trait4\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.31852 -0.154339; -0.154339 0.264754],\n\n[0.680196 -0.30344; -0.30344 0.731152])\nTrait3Trait5\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.31897 0.184354; 0.184354 0.2825],\n\n[0.67976 0.336411; 0.336411 0.715567])\nTrait3Trait6\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.319566 0.16664; 0.16664 0.285031],\n\n[0.679183 0.297698; 0.297698 0.714536])\nTrait3Trait7\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.318576 0.166852; 0.166852 0.215232],\n\n[0.680142 0.347139; 0.347139 0.781823])\nTrait3Trait8\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.320499 0.0575319; 0.0575319 0.197245],\n\n[0.678283 0.0442597; 0.0442597 0.802474])\nTrait3Trait9\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.318719 0.137292; 0.137292 0.246976],\n\n[0.680004 0.267105; 0.267105 0.751357])\nTrait3Trait10\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.318915 -0.0786338; -0.0786338 0.101103],\n\n[0.679815 -0.140789; -0.140789 0.898798])\nTrait3Trait11\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.317822 -0.017984; -0.017984 0.164743],\n\n[0.680871 -0.114166; -0.114166 0.833923])\nTrait3Trait12\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.320888 0.0845248; 0.0845248 0.0869868],\n\n[0.677914 0.0340133; 0.0340133 0.911841])\nTrait3Trait13\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.323009 0.110681; 0.110681 0.0611739],\n\n[0.675901 -0.00729662; -0.00729662 0.938072])\nTrait4Trait5\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.265667 -0.215848; -0.215848 0.282919],\n\n[0.730254 -0.376675; -0.376675 0.715164])\nTrait4Trait6\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.266143 -0.200634; -0.200634 0.284442],\n\n[0.729794 -0.346804; -0.346804 0.715112])\nTrait4Trait7\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.26449 -0.182752; -0.182752 0.214117],\n\n[0.731415 -0.326172; -0.326172 0.782935])\nTrait4Trait8\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.266694 -0.0976354; -0.0976354 0.196126],\n\n[0.729266 -0.15036; -0.15036 0.803571])\nTrait4Trait9\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.270037 -0.227407; -0.227407 0.248046],\n\n[0.726025 -0.415601; -0.415601 0.750298])\nTrait4Trait10\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.265543 -0.0338107; -0.0338107 0.0996098],\n\n[0.730395 -0.227725; -0.227725 0.900275])\nTrait4Trait11\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.265628 -0.09674; -0.09674 0.163274],\n\n[0.730302 -0.272611; -0.272611 0.835371])\nTrait4Trait12\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.268164 -0.141613; -0.141613 0.0803968],\n\n[0.727883 -0.0828465; -0.0828465 0.918446])\nTrait4Trait13\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.266171 -0.0980731; -0.0980731 0.0540102],\n\n[0.729775 -0.22506; -0.22506 0.945204])\nTrait5Trait6\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.281592 0.280898; 0.280898 0.282298],\n\n[0.716455 0.660368; 0.660368 0.717195])\nTrait5Trait7\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.280814 0.2321; 0.2321 0.211662],\n\n[0.717218 0.674304; 0.674304 0.785343])\nTrait5Trait8\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.28134 0.163948; 0.163948 0.192703],\n\n[0.716701 0.221032; 0.221032 0.806922])\nTrait5Trait9\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.283878 0.244532; 0.244532 0.241293],\n\n[0.714244 0.508417; 0.508417 0.756894])\nTrait5Trait10\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.281813 -0.0462114; -0.0462114 0.101481],\n\n[0.716238 -0.0572111; -0.0572111 0.898424])\nTrait5Trait11\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.280424 0.0202495; 0.0202495 0.164003],\n\n[0.717586 -0.0352436; -0.0352436 0.834649])\nTrait5Trait12\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.281427 0.0616131; 0.0616131 0.0827166],\n\n[0.716615 0.0529286; 0.0529286 0.916074])\nTrait5Trait13\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.282292 0.0704221; 0.0704221 0.0569463],\n\n[0.715782 0.0528374; 0.0528374 0.942268])\nTrait6Trait7\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.282961 0.220656; 0.220656 0.213856],\n\n[0.716549 0.581083; 0.581083 0.783179])\nTrait6Trait8\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.282961 0.18408; 0.18408 0.192379],\n\n[0.716549 0.436597; 0.436597 0.807246])\nTrait6Trait9\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.284978 0.234436; 0.234436 0.243207],\n\n[0.714601 0.476826; 0.476826 0.755028])\nTrait6Trait10\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.283655 -0.0435485; -0.0435485 0.102025],\n\n[0.715877 -0.0591681; -0.0591681 0.897886])\nTrait6Trait11\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.281522 0.0279992; 0.0279992 0.16343],\n\n[0.717946 -0.0524106; -0.0524106 0.835213])\nTrait6Trait12\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.283114 0.0571399; 0.0571399 0.0826789],\n\n[0.716403 0.0479199; 0.0479199 0.916112])\nTrait6Trait13\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.28382 0.0611212; 0.0611212 0.0570817],\n\n[0.715722 0.0532698; 0.0532698 0.942133])\nTrait7Trait8\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.213857 0.0884555; 0.0884555 0.192373],\n\n[0.783178 -0.0568331; -0.0568331 0.807252])\nTrait7Trait9\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.218756 0.217042; 0.217042 0.244144],\n\n[0.778433 0.462901; 0.462901 0.754123])\nTrait7Trait10\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.216273 -0.0421146; -0.0421146 0.10209],\n\n[0.780807 -0.0859075; -0.0859075 0.897822])\nTrait7Trait11\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.214069 0.0206967; 0.0206967 0.163474],\n\n[0.782961 -0.048148; -0.048148 0.83517])\nTrait7Trait12\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.214932 0.0757874; 0.0757874 0.0808731],\n\n[0.782132 0.0346956; 0.0346956 0.917915])\nTrait7Trait13\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.215959 0.0749373; 0.0749373 0.0545934],\n\n[0.781139 0.0389058; 0.0389058 0.944622])\nTrait8Trait9\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.194555 0.112816; 0.112816 0.247244],\n\n[0.805124 0.184778; 0.184778 0.751098])\nTrait8Trait10\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.194441 -0.0156342; -0.0156342 0.100426],\n\n[0.805222 0.0119826; 0.0119826 0.899468])\nTrait8Trait11\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.193853 0.0225325; 0.0225325 0.164669],\n\n[0.805796 -0.0272747; -0.0272747 0.833997])\nTrait8Trait12\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.193951 -0.00287601; -0.00287601 0.0828573],\n\n[0.8057 0.0336128; 0.0336128 0.915932])\nTrait8Trait13\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.19398 0.00407867; 0.00407867 0.0569071],\n\n[0.805672 0.0378817; 0.0378817 0.942301])\nTrait9Trait10\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.247294 -0.00230836; -0.00230836 0.0998264],\n\n[0.751051 0.0740729; 0.0740729 0.900061])\nTrait9Trait11\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.247823 0.0318235; 0.0318235 0.16489],\n\n[0.750532 0.152285; 0.152285 0.833778])\nTrait9Trait12\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.250335 0.0845714; 0.0845714 0.0887587],\n\n[0.748091 0.107756; 0.107756 0.910108])\nTrait9Trait13\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.249442 0.0934845; 0.0934845 0.057932],\n\n[0.748975 0.0982191; 0.0982191 0.941335])\nTrait10Trait11\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.0931397 0.100034; 0.100034 0.164955],\n\n[0.906703 0.474427; 0.474427 0.833715])\nTrait10Trait12\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.0967247 0.0564043; 0.0564043 0.0794528],\n\n[0.90315 0.0853232; 0.0853232 0.919334])\nTrait10Trait13\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.100985 -0.0279916; -0.0279916 0.0578369],\n\n[0.898937 0.166051; 0.166051 0.94139])\nTrait11Trait12\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.163841 0.0570318; 0.0570318 0.0792144],\n\n[0.834814 0.145597; 0.145597 0.919552])\nTrait11Trait13\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.164883 -0.00158414; -0.00158414 0.0574968],\n\n[0.833798 0.200612; 0.200612 0.941715])\nTrait12Trait13\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.0845946 0.0685052; 0.0685052 0.0554759],\n\n[0.914214 0.573152; 0.573152 0.943735])\nelapsed time: 7.752196517 seconds\n\n\n\n\n\n7.752196517\n\n\n\n\n\n\n\n3-trait analysis\n\n\nResearchers want to jointly analyze traits 5-7. Our strategy is to try both Fisher scoring and MM algorithm with different starting point, and choose the best local optimum. We first form the data set and run Fisher scoring, which yields a final objective value -1.4700991+04.\n\n\ntraitidx\n \n=\n \n5\n:\n7\n\n\n# form data set\n\n\ntrait57_data\n \n=\n \nTwoVarCompVariateRotate\n(\ncg10kdata_rotated\n.\nYrot\n[\n:\n,\n \ntraitidx\n],\n \ncg10kdata_rotated\n.\nXrot\n,\n \n    \ncg10kdata_rotated\n.\neigval\n,\n \ncg10kdata_rotated\n.\neigvec\n,\n \ncg10kdata_rotated\n.\nlogdetV2\n)\n\n\n# initialize model parameters\n\n\ntrait57_model\n \n=\n \nVarianceComponentModel\n(\ntrait57_data\n)\n\n\n# estimate variance components\n\n\n@time\n \nmle_fs!\n(\ntrait57_model\n,\n \ntrait57_data\n;\n \nsolver\n=:\nIpopt\n,\n \nverbose\n=\ntrue\n)\n\n\ntrait57_model\n\n\n\n\n\n\nThis is Ipopt version 3.12.4, running with linear solver mumps.\nNOTE: Other linear solvers might be more efficient (see Ipopt documentation).\n\nNumber of nonzeros in equality constraint Jacobian...:        0\nNumber of nonzeros in inequality constraint Jacobian.:        0\nNumber of nonzeros in Lagrangian Hessian.............:       78\n\nTotal number of variables............................:       12\n                     variables with only lower bounds:        0\n                variables with lower and upper bounds:        0\n                     variables with only upper bounds:        0\nTotal number of equality constraints.................:        0\nTotal number of inequality constraints...............:        0\n        inequality constraints with only lower bounds:        0\n   inequality constraints with lower and upper bounds:        0\n        inequality constraints with only upper bounds:        0\n\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n   0  3.0247512e+04 0.00e+00 1.00e+02   0.0 0.00e+00    -  0.00e+00 0.00e+00   0 \n   5  1.6834796e+04 0.00e+00 4.07e+02 -11.0 3.66e-01    -  1.00e+00 1.00e+00f  1 MaxS\n  10  1.4744497e+04 0.00e+00 1.12e+02 -11.0 2.45e-01    -  1.00e+00 1.00e+00f  1 MaxS\n  15  1.4701497e+04 0.00e+00 1.30e+01 -11.0 1.15e-01  -4.5 1.00e+00 1.00e+00f  1 MaxS\n  20  1.4700992e+04 0.00e+00 6.65e-01 -11.0 1.74e-04  -6.9 1.00e+00 1.00e+00f  1 MaxS\n  25  1.4700991e+04 0.00e+00 2.77e-02 -11.0 7.36e-06  -9.2 1.00e+00 1.00e+00f  1 MaxS\n  30  1.4700991e+04 0.00e+00 1.15e-03 -11.0 3.06e-07 -11.6 1.00e+00 1.00e+00f  1 MaxS\n  35  1.4700991e+04 0.00e+00 4.76e-05 -11.0 1.27e-08 -14.0 1.00e+00 1.00e+00h  1 MaxS\n  40  1.4700991e+04 0.00e+00 1.97e-06 -11.0 5.26e-10 -16.4 1.00e+00 1.00e+00f  1 MaxSA\n  45  1.4700991e+04 0.00e+00 8.17e-08 -11.0 2.18e-11 -18.8 1.00e+00 1.00e+00h  1 MaxSA\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n\nNumber of Iterations....: 49\n\n                                   (scaled)                 (unscaled)\nObjective...............:   4.4724330090668286e+02    1.4700991028593347e+04\nDual infeasibility......:   6.4345872286001950e-09    2.1150637455850896e-07\nConstraint violation....:   0.0000000000000000e+00    0.0000000000000000e+00\nComplementarity.........:   0.0000000000000000e+00    0.0000000000000000e+00\nOverall NLP error.......:   6.4345872286001950e-09    2.1150637455850896e-07\n\n\nNumber of objective function evaluations             = 50\nNumber of objective gradient evaluations             = 50\nNumber of equality constraint evaluations            = 0\nNumber of inequality constraint evaluations          = 0\nNumber of equality constraint Jacobian evaluations   = 0\nNumber of inequality constraint Jacobian evaluations = 0\nNumber of Lagrangian Hessian evaluations             = 49\nTotal CPU secs in IPOPT (w/o function evaluations)   =      0.013\nTotal CPU secs in NLP function evaluations           =      0.085\n\nEXIT: Optimal Solution Found.\n  0.118428 seconds (66.82 k allocations: 6.250 MB)\n\n\n\n\n\nVarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}(,(\n[0.281163 0.280014 0.232384; 0.280014 0.284899 0.220285; 0.232384 0.220285 0.212687],\n\n[0.716875 0.66125 0.674025; 0.66125 0.714602 0.581433; 0.674025 0.581433 0.784324]),,Char[],Float64[],-Inf,Inf)\n\n\n\n\n\nWe then run the MM algorithm, starting from the Fisher scoring answer. MM finds an improved solution with objective value 8.955397e+03.\n\n\n# trait59_model contains the fitted model by Fisher scoring now\n\n\n@time\n \nmle_mm!\n(\ntrait57_model\n,\n \ntrait57_data\n;\n \nverbose\n=\ntrue\n)\n\n\ntrait57_model\n\n\n\n\n\n\n     MM Algorithm\n  Iter      Objective  \n--------  -------------\n       0  -1.470099e+04\n       1  -1.470099e+04\n\n  0.363096 seconds (539.27 k allocations: 20.482 MB, 2.19% gc time)\n\n\n\n\n\nVarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}(,(\n[0.281163 0.280014 0.232384; 0.280014 0.284899 0.220285; 0.232384 0.220285 0.212687],\n\n[0.716875 0.66125 0.674025; 0.66125 0.714602 0.581433; 0.674025 0.581433 0.784324]),,Char[],Float64[],-Inf,Inf)\n\n\n\n\n\nDo another run of MM algorithm from default starting point. It leads to a slightly better local optimum -1.470104e+04, slighly worse than the Fisher scoring result. Follow up anlaysis should use the Fisher scoring result.\n\n\n# default starting point\n\n\ntrait57_model\n \n=\n \nVarianceComponentModel\n(\ntrait57_data\n)\n\n\n@time\n \n_\n,\n \n_\n,\n \n_\n,\n \n\u03a3cov\n,\n \n=\n \nmle_mm!\n(\ntrait57_model\n,\n \ntrait57_data\n;\n \nverbose\n=\ntrue\n)\n\n\ntrait57_model\n\n\n\n\n\n\n     MM Algorithm\n  Iter      Objective  \n--------  -------------\n       0  -3.024751e+04\n       1  -2.040338e+04\n       2  -1.656127e+04\n       3  -1.528591e+04\n       4  -1.491049e+04\n       5  -1.480699e+04\n       6  -1.477870e+04\n       7  -1.477026e+04\n       8  -1.476696e+04\n       9  -1.476499e+04\n      10  -1.476339e+04\n      20  -1.475040e+04\n      30  -1.474042e+04\n      40  -1.473272e+04\n      50  -1.472677e+04\n      60  -1.472215e+04\n      70  -1.471852e+04\n      80  -1.471565e+04\n      90  -1.471336e+04\n     100  -1.471152e+04\n     110  -1.471002e+04\n     120  -1.470879e+04\n     130  -1.470778e+04\n     140  -1.470694e+04\n     150  -1.470623e+04\n     160  -1.470563e+04\n     170  -1.470513e+04\n     180  -1.470469e+04\n     190  -1.470432e+04\n     200  -1.470400e+04\n     210  -1.470372e+04\n     220  -1.470347e+04\n     230  -1.470326e+04\n     240  -1.470307e+04\n     250  -1.470290e+04\n     260  -1.470275e+04\n     270  -1.470262e+04\n     280  -1.470250e+04\n     290  -1.470239e+04\n     300  -1.470229e+04\n     310  -1.470220e+04\n     320  -1.470213e+04\n     330  -1.470205e+04\n     340  -1.470199e+04\n     350  -1.470193e+04\n     360  -1.470187e+04\n     370  -1.470182e+04\n     380  -1.470177e+04\n     390  -1.470173e+04\n     400  -1.470169e+04\n     410  -1.470165e+04\n     420  -1.470162e+04\n     430  -1.470159e+04\n     440  -1.470156e+04\n     450  -1.470153e+04\n     460  -1.470150e+04\n     470  -1.470148e+04\n     480  -1.470146e+04\n     490  -1.470143e+04\n     500  -1.470141e+04\n     510  -1.470140e+04\n     520  -1.470138e+04\n     530  -1.470136e+04\n     540  -1.470134e+04\n     550  -1.470133e+04\n     560  -1.470132e+04\n     570  -1.470130e+04\n     580  -1.470129e+04\n     590  -1.470128e+04\n     600  -1.470127e+04\n     610  -1.470125e+04\n     620  -1.470124e+04\n     630  -1.470123e+04\n     640  -1.470122e+04\n     650  -1.470122e+04\n     660  -1.470121e+04\n     670  -1.470120e+04\n     680  -1.470119e+04\n     690  -1.470118e+04\n     700  -1.470118e+04\n     710  -1.470117e+04\n     720  -1.470116e+04\n     730  -1.470116e+04\n     740  -1.470115e+04\n     750  -1.470114e+04\n     760  -1.470114e+04\n     770  -1.470113e+04\n     780  -1.470113e+04\n     790  -1.470112e+04\n     800  -1.470112e+04\n     810  -1.470111e+04\n     820  -1.470111e+04\n     830  -1.470111e+04\n     840  -1.470110e+04\n     850  -1.470110e+04\n     860  -1.470109e+04\n     870  -1.470109e+04\n     880  -1.470109e+04\n     890  -1.470108e+04\n     900  -1.470108e+04\n     910  -1.470108e+04\n     920  -1.470108e+04\n     930  -1.470107e+04\n     940  -1.470107e+04\n     950  -1.470107e+04\n     960  -1.470106e+04\n     970  -1.470106e+04\n     980  -1.470106e+04\n     990  -1.470106e+04\n    1000  -1.470106e+04\n    1010  -1.470105e+04\n    1020  -1.470105e+04\n    1030  -1.470105e+04\n    1040  -1.470105e+04\n    1050  -1.470105e+04\n    1060  -1.470104e+04\n    1070  -1.470104e+04\n    1080  -1.470104e+04\n    1090  -1.470104e+04\n    1100  -1.470104e+04\n    1110  -1.470104e+04\n\n  0.846316 seconds (153.65 k allocations: 15.453 MB, 0.89% gc time)\n\n\n\n\n\nVarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}(,(\n[0.281188 0.280032 0.232439; 0.280032 0.284979 0.220432; 0.232439 0.220432 0.212922],\n\n[0.71685 0.661232 0.67397; 0.661232 0.71452 0.581287; 0.67397 0.581287 0.784092]),,Char[],Float64[],-Inf,Inf)\n\n\n\n\n\nHeritability from 3-variate estimate and their standard errors.\n\n\nh\n,\n \nhse\n \n=\n \nheritability\n(\ntrait57_model\n.\n\u03a3\n,\n \n\u03a3cov\n)\n\n\n[\nh\n;\n \nhse\n]\n\n\n\n\n\n\n2\u00d73 Array{Float64,2}:\n 0.281741   0.285122   0.21356  \n 0.0778033  0.0773313  0.0841103\n\n\n\n\n\n\n\n13-trait joint analysis\n\n\nIn some situations, such as studying the genetic covariance, we need to jointly analyze 13 traits. We first try the \nFisher scoring algorithm\n.\n\n\n# initialize model parameters\n\n\ntraitall_model\n \n=\n \nVarianceComponentModel\n(\ncg10kdata_rotated\n)\n\n\n# estimate variance components using Fisher scoring algorithm\n\n\n@time\n \nmle_fs!\n(\ntraitall_model\n,\n \ncg10kdata_rotated\n;\n \nsolver\n=:\nIpopt\n,\n \nverbose\n=\ntrue\n)\n\n\n\n\n\n\nThis is Ipopt version 3.12.4, running with linear solver mumps.\nNOTE: Other linear solvers might be more efficient (see Ipopt documentation).\n\nNumber of nonzeros in equality constraint Jacobian...:        0\nNumber of nonzeros in inequality constraint Jacobian.:        0\nNumber of nonzeros in Lagrangian Hessian.............:    16653\n\nTotal number of variables............................:      182\n                     variables with only lower bounds:        0\n                variables with lower and upper bounds:        0\n                     variables with only upper bounds:        0\nTotal number of equality constraints.................:        0\nTotal number of inequality constraints...............:        0\n        inequality constraints with only lower bounds:        0\n   inequality constraints with lower and upper bounds:        0\n        inequality constraints with only upper bounds:        0\n\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n   0  1.3113368e+05 0.00e+00 1.00e+02   0.0 0.00e+00    -  0.00e+00 0.00e+00   0 \n   5  8.2237394e+04 0.00e+00 6.04e+02 -11.0 2.53e+00    -  1.00e+00 1.00e+00f  1 MaxS\n  10  1.2380115e+05 0.00e+00 1.03e+03 -11.0 6.31e+01  -5.4 1.00e+00 1.00e+00h  1 MaxS\n  15  1.4133320e+05 0.00e+00 1.99e+02 -11.0 4.54e+02  -7.8 1.00e+00 1.00e+00h  1 MaxS\n\n\n\nBase.LinAlg.PosDefException(25)\n\n\n\n in chkposdef at ./linalg/lapack.jl:44 [inlined]\n\n in sygvd!(::Int64, ::Char, ::Char, ::Array{Float64,2}, ::Array{Float64,2}) at ./linalg/lapack.jl:4908\n\n in eigfact!(::Symmetric{Float64,Array{Float64,2}}, ::Symmetric{Float64,Array{Float64,2}}) at ./linalg/symmetric.jl:224\n\n in eigfact(::Symmetric{Float64,Array{Float64,2}}, ::Symmetric{Float64,Array{Float64,2}}) at ./linalg/eigen.jl:274\n\n in VarianceComponentModels.TwoVarCompModelRotate{T\n:AbstractFloat,BT\n:Union{AbstractArray{T,1},AbstractArray{T,2}}}(::VarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}) at /Users/huazhou/.julia/v0.5/VarianceComponentModels/src/VarianceComponentModels.jl:121\n\n in eval_f(::VarianceComponentModels.TwoVarCompOptProb{VarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}},VarianceComponentModels.TwoVarCompVariateRotate{Float64,Array{Float64,2},Array{Float64,2}},Array{Float64,2},Array{Float64,1},VarianceComponentModels.VarianceComponentAuxData{Array{Float64,2},Array{Float64,1}}}, ::Array{Float64,1}) at /Users/huazhou/.julia/v0.5/VarianceComponentModels/src/two_variance_component.jl:683\n\n in (::Ipopt.#eval_f_cb#4{VarianceComponentModels.TwoVarCompOptProb{VarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}},VarianceComponentModels.TwoVarCompVariateRotate{Float64,Array{Float64,2},Array{Float64,2}},Array{Float64,2},Array{Float64,1},VarianceComponentModels.VarianceComponentAuxData{Array{Float64,2},Array{Float64,1}}}})(::Array{Float64,1}) at /Users/huazhou/.julia/v0.5/Ipopt/src/IpoptSolverInterface.jl:53\n\n in eval_f_wrapper(::Int32, ::Ptr{Float64}, ::Int32, ::Ptr{Float64}, ::Ptr{Void}) at /Users/huazhou/.julia/v0.5/Ipopt/src/Ipopt.jl:89\n\n in solveProblem(::Ipopt.IpoptProblem) at /Users/huazhou/.julia/v0.5/Ipopt/src/Ipopt.jl:304\n\n in optimize!(::Ipopt.IpoptMathProgModel) at /Users/huazhou/.julia/v0.5/Ipopt/src/IpoptSolverInterface.jl:120\n\n in #mle_fs!#29(::Int64, ::Symbol, ::Symbol, ::Bool, ::Function, ::VarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}, ::VarianceComponentModels.TwoVarCompVariateRotate{Float64,Array{Float64,2},Array{Float64,2}}) at /Users/huazhou/.julia/v0.5/VarianceComponentModels/src/two_variance_component.jl:893\n\n in (::VarianceComponentModels.#kw##mle_fs!)(::Array{Any,1}, ::VarianceComponentModels.#mle_fs!, ::VarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}, ::VarianceComponentModels.TwoVarCompVariateRotate{Float64,Array{Float64,2},Array{Float64,2}}) at ./\nmissing\n:0\n\n\n\n\n\nFrom the output we can see the Fisher scoring algorithm ran into some numerical issues. Let's try the \nMM algorithm\n.\n\n\n# reset model parameters\n\n\ntraitall_model\n \n=\n \nVarianceComponentModel\n(\ncg10kdata_rotated\n)\n\n\n# estimate variance components using Fisher scoring algorithm\n\n\n@time\n \nmle_mm!\n(\ntraitall_model\n,\n \ncg10kdata_rotated\n;\n \nverbose\n=\ntrue\n)\n\n\n\n\n\n\n     MM Algorithm\n  Iter      Objective  \n--------  -------------\n       0  -1.311337e+05\n       1  -8.002195e+04\n       2  -5.807051e+04\n       3  -4.926234e+04\n       4  -4.611182e+04\n       5  -4.511727e+04\n       6  -4.482798e+04\n       7  -4.474410e+04\n       8  -4.471610e+04\n       9  -4.470285e+04\n      10  -4.469355e+04\n      20  -4.462331e+04\n      30  -4.456960e+04\n      40  -4.452834e+04\n      50  -4.449652e+04\n      60  -4.447178e+04\n      70  -4.445237e+04\n      80  -4.443699e+04\n      90  -4.442467e+04\n     100  -4.441470e+04\n     110  -4.440656e+04\n     120  -4.439985e+04\n     130  -4.439427e+04\n     140  -4.438959e+04\n     150  -4.438564e+04\n     160  -4.438229e+04\n     170  -4.437941e+04\n     180  -4.437694e+04\n     190  -4.437480e+04\n     200  -4.437294e+04\n     210  -4.437131e+04\n     220  -4.436989e+04\n     230  -4.436863e+04\n     240  -4.436751e+04\n     250  -4.436652e+04\n     260  -4.436564e+04\n     270  -4.436485e+04\n     280  -4.436414e+04\n     290  -4.436351e+04\n     300  -4.436293e+04\n     310  -4.436242e+04\n     320  -4.436195e+04\n     330  -4.436152e+04\n     340  -4.436113e+04\n     350  -4.436078e+04\n     360  -4.436046e+04\n     370  -4.436016e+04\n     380  -4.435989e+04\n     390  -4.435965e+04\n     400  -4.435942e+04\n     410  -4.435921e+04\n     420  -4.435902e+04\n     430  -4.435884e+04\n     440  -4.435867e+04\n     450  -4.435852e+04\n     460  -4.435838e+04\n     470  -4.435825e+04\n     480  -4.435813e+04\n     490  -4.435802e+04\n     500  -4.435791e+04\n     510  -4.435781e+04\n     520  -4.435772e+04\n     530  -4.435764e+04\n     540  -4.435756e+04\n     550  -4.435748e+04\n     560  -4.435741e+04\n     570  -4.435735e+04\n     580  -4.435729e+04\n     590  -4.435723e+04\n     600  -4.435718e+04\n     610  -4.435713e+04\n     620  -4.435708e+04\n     630  -4.435704e+04\n     640  -4.435700e+04\n     650  -4.435696e+04\n     660  -4.435692e+04\n     670  -4.435688e+04\n     680  -4.435685e+04\n     690  -4.435682e+04\n     700  -4.435679e+04\n     710  -4.435676e+04\n     720  -4.435674e+04\n     730  -4.435671e+04\n     740  -4.435669e+04\n     750  -4.435667e+04\n     760  -4.435665e+04\n     770  -4.435663e+04\n     780  -4.435661e+04\n     790  -4.435659e+04\n     800  -4.435657e+04\n     810  -4.435656e+04\n     820  -4.435654e+04\n     830  -4.435653e+04\n     840  -4.435651e+04\n     850  -4.435650e+04\n     860  -4.435649e+04\n     870  -4.435648e+04\n     880  -4.435647e+04\n     890  -4.435646e+04\n     900  -4.435645e+04\n     910  -4.435644e+04\n     920  -4.435643e+04\n     930  -4.435642e+04\n     940  -4.435641e+04\n     950  -4.435640e+04\n     960  -4.435639e+04\n     970  -4.435639e+04\n     980  -4.435638e+04\n     990  -4.435637e+04\n    1000  -4.435637e+04\n    1010  -4.435636e+04\n    1020  -4.435635e+04\n    1030  -4.435635e+04\n    1040  -4.435634e+04\n    1050  -4.435634e+04\n    1060  -4.435633e+04\n    1070  -4.435633e+04\n    1080  -4.435632e+04\n\n  3.696088 seconds (156.73 k allocations: 69.792 MB, 0.32% gc time)\n\n\n\n\n\n(-44356.32043185692,VarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}(,(\n[0.273498 0.192141 \u2026 -0.128643 -0.098307; 0.192141 0.219573 \u2026 -0.0687355 -0.0433724; \u2026 ; -0.128643 -0.0687355 \u2026 0.117185 0.0899824; -0.098307 -0.0433724 \u2026 0.0899824 0.106354],\n\n[0.723441 0.568135 \u2026 -0.0586259 -0.12469; 0.568135 0.77999 \u2026 0.0236098 0.0464835; \u2026 ; -0.0586259 0.0236098 \u2026 0.881707 0.551818; -0.12469 0.0464835 \u2026 0.551818 0.893023]),,Char[],Float64[],-Inf,Inf),(\n[0.0111652 0.0131052 \u2026 0.012894 0.0127596; 0.0131077 0.0151572 \u2026 0.0171615 0.0171431; \u2026 ; 0.0128941 0.0171614 \u2026 0.0174141 0.0182039; 0.0127597 0.0171428 \u2026 0.0182038 0.0187958],\n\n[0.011228 0.0133093 \u2026 0.0130109 0.012784; 0.0133113 0.0158091 \u2026 0.0178685 0.0177999; \u2026 ; 0.0130107 0.0178687 \u2026 0.017957 0.0187655; 0.0127836 0.0177997 \u2026 0.0187654 0.0193582]),\n[0.000124662 7.24629e-5 \u2026 -3.61414e-7 -1.40373e-5; 7.24483e-5 0.000171812 \u2026 -2.03586e-5 -3.26457e-6; \u2026 ; -3.87301e-7 -2.03778e-5 \u2026 0.000352145 -1.48419e-5; -1.40438e-5 -3.27339e-6 \u2026 -1.48381e-5 0.000374741],\n\n,)\n\n\n\n\n\nIt converges after ~1000 iterations.\n\n\n\n\nSave analysis results\n\n\n#using JLD\n\n\n#@save \ncopd.jld\n\n\n#whos()", 
            "title": "Heritability"
        }, 
        {
            "location": "/man/heritability/#heritability-analysis", 
            "text": "As an application of the variance component model, this note demonstrates the workflow for heritability analysis in genetics, using a sample data set  cg10k  with  6,670  individuals and  630,860  SNPs. Person IDs and phenotype names are masked for privacy.  cg10k.bed ,  cg10k.bim , and  cg10k.fam  is a set of Plink files in binary format.  cg10k_traits.txt  contains 13 phenotypes of the 6,670 individuals.  ; ls   cg10k *.*   cg10k.bed\ncg10k.bim\ncg10k.fam\ncg10k_traits.txt  Machine information:  versioninfo ()   Julia Version 0.5.1\nCommit 6445c82 (2017-03-05 13:25 UTC)\nPlatform Info:\n  OS: macOS (x86_64-apple-darwin13.4.0)\n  CPU: Intel(R) Core(TM) i7-4790K CPU @ 4.00GHz\n  WORD_SIZE: 64\n  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Haswell)\n  LAPACK: libopenblas64_\n  LIBM: libopenlibm\n  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)", 
            "title": "Heritability Analysis"
        }, 
        {
            "location": "/man/heritability/#read-in-binary-snp-data", 
            "text": "We will use the  SnpArrays.jl  package to read in binary SNP data and compute the empirical kinship matrix. Issue   Pkg . clone ( https://github.com/OpenMendel/SnpArrays.jl.git )   within  Julia  to install the  SnpArrays  package.  using   SnpArrays   # read in genotype data from Plink binary file (~50 secs on my laptop)  @time   cg10k   =   SnpArray ( cg10k )    33.883711 seconds (282.07 k allocations: 1015.002 MB, 0.23% gc time)\n\n\n\n\n\n6670\u00d7630860 SnpArrays.SnpArray{2}:\n (false,true)   (false,true)   (true,true)   \u2026  (true,true)    (true,true) \n (true,true)    (true,true)    (false,true)     (false,true)   (true,false)\n (true,true)    (true,true)    (false,true)     (true,true)    (true,true) \n (true,true)    (true,true)    (true,true)      (false,true)   (true,true) \n (true,true)    (true,true)    (true,true)      (true,true)    (false,true)\n (false,true)   (false,true)   (true,true)   \u2026  (true,true)    (true,true) \n (false,false)  (false,false)  (true,true)      (true,true)    (true,true) \n (true,true)    (true,true)    (true,true)      (true,true)    (false,true)\n (true,true)    (true,true)    (false,true)     (true,true)    (true,true) \n (true,true)    (true,true)    (true,true)      (false,true)   (true,true) \n (true,true)    (true,true)    (false,true)  \u2026  (true,true)    (true,true) \n (false,true)   (false,true)   (true,true)      (true,true)    (false,true)\n (true,true)    (true,true)    (true,true)      (true,true)    (false,true)\n \u22ee                                           \u22f1                             \n (false,true)   (false,true)   (true,true)      (false,true)   (false,true)\n (false,true)   (false,true)   (false,true)     (false,true)   (true,true) \n (true,true)    (true,true)    (false,true)  \u2026  (false,true)   (true,true) \n (false,true)   (false,true)   (true,true)      (true,true)    (false,true)\n (true,true)    (true,true)    (true,true)      (false,true)   (true,true) \n (true,true)    (true,true)    (true,false)     (false,false)  (false,true)\n (true,true)    (true,true)    (true,true)      (true,true)    (false,true)\n (true,true)    (true,true)    (false,true)  \u2026  (true,true)    (true,true) \n (true,true)    (true,true)    (true,true)      (false,true)   (true,true) \n (true,true)    (true,true)    (true,true)      (true,true)    (false,true)\n (false,true)   (false,true)   (true,true)      (true,true)    (true,true) \n (true,true)    (true,true)    (true,true)      (true,true)    (true,true)", 
            "title": "Read in binary SNP data"
        }, 
        {
            "location": "/man/heritability/#summary-statistics-of-snp-data", 
            "text": "people ,   snps   =   size ( cg10k )   (6670,630860)  # summary statistics (~50 secs on my laptop)  @time   maf ,   _ ,   missings_by_snp ,   =   summarize ( cg10k );    26.436140 seconds (33.44 k allocations: 10.920 MB)  # 5 number summary and average MAF (minor allele frequencies)  quantile ( maf ,   [ 0.0   . 25   . 5   . 75   1.0 ]),   mean ( maf )   (\n[0.00841726 0.124063 \u2026 0.364253 0.5],\n\n0.24536516625042462)  # Pkg.add( Plots )  # Pkg.add( PyPlot )  using   Plots  pyplot ()  histogram ( maf ,   xlab   =   Minor Allele Frequency (MAF) ,   label   =   MAF )    # proportion of missing genotypes  sum ( missings_by_snp )   /   length ( cg10k )   0.0013128198764010824  # proportion of rare SNPs with maf   0.05  countnz ( maf   .   0.05 )   /   length ( maf )   0.07228069619249913", 
            "title": "Summary statistics of SNP data"
        }, 
        {
            "location": "/man/heritability/#empirical-kinship-matrix", 
            "text": "We estimate empirical kinship based on all SNPs by the genetic relation matrix (GRM). Missing genotypes are imputed on the fly by drawing according to the minor allele frequencies.  # GRM using all SNPs (~10 mins on my laptop)  srand ( 123 )  @time   \u03a6grm   =   grm ( cg10k ;   method   =   : GRM )   2099.911248 seconds (29.50 G allocations: 441.429 GB, 1.50% gc time)\n\n\n\n\n\n6670\u00d76670 Array{Float64,2}:\n  0.502916      0.00329978   -0.000116213  \u2026  -6.46286e-5   -0.00281229 \n  0.00329978    0.49892      -0.00201992       0.000909871   0.00345573 \n -0.000116213  -0.00201992    0.493632         0.000294565  -0.000349854\n  0.000933977  -0.00320391   -0.0018611       -0.00241682   -0.00127078 \n -7.75429e-5   -0.0036075     0.00181442       0.00213976   -0.00158382 \n  0.00200371    0.000577386   0.0025455    \u2026   0.000943753  -1.82994e-6 \n  0.000558503   0.00241421   -0.0018782        0.001217     -0.00123924 \n -0.000659495   0.00319987   -0.00101496       0.00353646   -0.00024093 \n -0.00102619   -0.00120448   -0.00055462       0.00175586    0.00181899 \n -0.00136838    0.00211996    0.000119128     -0.00147305   -0.00105239 \n -0.00206144    0.000148818  -0.000475177  \u2026  -0.000265522  -0.00106123 \n  0.000951016   0.00167042    0.00183545      -0.000703658  -0.00313334 \n  0.000330442  -0.000904147   0.00301478       0.000754772  -0.00127413 \n  \u22ee                                        \u22f1                            \n  0.00301137    0.00116042    0.00100426       6.67254e-6    0.00307069 \n -0.00214008    0.00270925   -0.00185054      -0.00109935    0.00366816 \n  0.000546739  -0.00242646   -0.00305264   \u2026  -0.000629014   0.00210779 \n -0.00422553   -0.0020713    -0.00109052      -0.000705804  -0.000508055\n -0.00318405   -0.00075385    0.00312377       0.00052883   -3.60969e-5 \n  0.000430196  -0.00197163    0.00268545      -0.00633175   -0.00520337 \n  0.00221429    0.000849792  -0.00101111      -0.000943129  -0.000624419\n -0.00229025   -0.000130598   0.000101853  \u2026   0.000840136  -0.00230224 \n -0.00202917    0.00233007   -0.00131006       0.00197798   -0.000513771\n -0.000964907  -0.000872326  -7.06722e-5       0.00124702   -0.00295844 \n -6.46286e-5    0.000909871   0.000294565      0.500983      0.000525615\n -0.00281229    0.00345573   -0.000349854      0.000525615   0.500792", 
            "title": "Empirical kinship matrix"
        }, 
        {
            "location": "/man/heritability/#phenotypes", 
            "text": "Read in the phenotype data and compute descriptive statistics.  # Pkg.add( DataFrames )  using   DataFrames  cg10k_trait   =   readtable ( \n     cg10k_traits.txt ;  \n     separator   =     , \n     names   =   [ : FID ;   : IID ;   : Trait1 ;   : Trait2 ;   : Trait3 ;   : Trait4 ;   : Trait5 ;   : Trait6 ;  \n              : Trait7 ;   : Trait8 ;   : Trait9 ;   : Trait10 ;   : Trait11 ;   : Trait12 ;   : Trait13 ],   \n     eltypes   =   [ String ;   String ;   Float64 ;   Float64 ;   Float64 ;   Float64 ;   Float64 ;  \n                Float64 ;   Float64 ;   Float64 ;   Float64 ;   Float64 ;   Float64 ;   Float64 ;   Float64 ] \n     )  # do not display FID and IID for privacy  cg10k_trait [ : ,   3 : end ]   Trait1 Trait2 Trait3 Trait4 Trait5 Trait6 Trait7 Trait8 Trait9 Trait10 Trait11 Trait12 Trait13 1 -1.81573145026234 -0.94615046147283 1.11363077580442 -2.09867121119159 0.744416614111748 0.00139171884080131 0.934732480409667 -1.22677315418103 1.1160784277875 -0.4436280335029 0.824465656443384 -1.02852542216546 -0.394049201727681 2 -1.24440094378729 0.109659992547179 0.467119394241789 -1.62131304097589 1.0566758355683 0.978946979419181 1.00014633946047 0.32487427140228 1.16232175219696 2.6922706948705 3.08263672461047 1.09064954786013 0.0256616415357438 3 1.45566914502305 1.53866932923243 1.09402959376555 0.586655272226893 -0.32796454430367 -0.30337709778827 -0.0334354881314741 -0.464463064285437 -0.3319396273436 -0.486839089635991 -1.10648681564373 -1.42015780427231 -0.687463456644413 4 -0.768809276698548 0.513490885514249 0.244263028382142 -1.31740254475691 1.19393774326845 1.17344127734288 1.08737426675232 0.536022583732261 0.802759240762068 0.234159411749815 0.394174866891074 -0.767365892476029 0.0635385761884935 5 -0.264415132547719 -0.348240421825694 -0.0239065083413606 0.00473915802244948 1.25619191712193 1.2038883667631 1.29800739042627 0.310113660247311 0.626159861059352 0.899289129831224 0.54996783350812 0.540687809542048 0.179675416046033 6 -1.37617270917293 -1.47191967744564 0.291179894254146 -0.803110740704731 -0.264239977442213 -0.260573027836772 -0.165372266287781 -0.219257294118362 1.04702422290318 -0.0985815534616482 0.947393438068448 0.594014812031438 0.245407436348479 7 0.1009416296374 -0.191615722103455 -0.567421321596677 0.378571487240382 -0.246656179817904 -0.608810750053858 0.189081058215596 -1.27077787326519 -0.452476199143965 0.702562877297724 0.332636218957179 0.0026916503626181 0.317117176705358 8 -0.319818276367464 1.35774480657283 0.818689545938528 -1.15565531644352 0.63448368102259 0.291461908634679 0.933323714954726 -0.741083289682492 0.647477683507572 -0.970877627077966 0.220861165411304 0.852512250237764 -0.225904624283945 9 -0.288334173342032 0.566082538090752 0.254958336116175 -0.652578302869714 0.668921559277347 0.978309199170558 0.122862966041938 1.4790926378214 0.0672132424173449 0.0795903917527827 0.167532455243232 0.246915579442139 0.539932616458363 10 -1.15759732583991 -0.781198583545165 -0.595807759833517 -1.00554980260402 0.789828885933321 0.571058413379044 0.951304176233755 -0.295962982984816 0.99042002479707 0.561309366988983 0.733100030623233 -1.73467772245684 -1.35278484330654 11 0.740569150459031 1.40873846755415 0.734689999440088 0.0208322841295094 -0.337440968561619 -0.458304040611395 -0.142582512772326 -0.580392297464107 -0.684684998101516 -0.00785381461893456 -0.712244337518008 -0.313345561230878 -0.345419463162219 12 -0.675892486454995 0.279892613829682 0.267915996308248 -1.04103665392985 0.910741715645888 0.866027618513171 1.07414431702005 0.0381751003538302 0.766355377018601 -0.340118016143495 -0.809013958505059 0.548521663785885 -0.0201828675962336 13 -0.795410435603455 -0.699989939762738 0.3991295030063 -0.510476261900736 1.51552245416844 1.28743032939467 1.53772393250903 0.133989160117702 1.02025736886037 0.499018733899186 -0.36948273277931 -1.10153460436318 -0.598132438886619 14 -0.193483122930324 -0.286021160323518 -0.691494225262995 0.0131581678700699 1.52337470686782 1.4010638072262 1.53114620451896 0.333066483478075 1.04372480381099 0.163206783570466 -0.422883765001728 -0.383527976713573 -0.489221907788158 15 0.151246203379718 2.09185108993614 2.03800472474384 -1.12474717143531 1.66557024390713 1.62535675109576 1.58751070483655 0.635852186043776 0.842577784605979 0.450761870778952 -1.39479033623028 -0.560984107567768 0.289349776549287 16 -0.464608740812712 0.36127694772303 1.2327673928287 -0.826033731086383 1.43475224709983 1.74451823818846 0.211096887484638 2.64816425140548 1.02511433146096 0.11975731603184 0.0596832073448267 -0.631231612661616 -0.207878671782927 17 -0.732977488012215 -0.526223425889779 0.61657871336593 -0.55447974332593 0.947484859025104 0.936833214138173 0.972516806335524 0.290251013865227 1.01285359725723 0.516207422283291 -0.0300689171988194 0.8787322524583 0.450254629309513 18 -0.167326459622119 0.175327165487237 0.287467725892572 -0.402652532084246 0.551181509418056 0.522204743290975 0.436837660094653 0.299564933845579 0.583109520896067 -0.704415820005353 -0.730810367994577 -1.95140580379896 -0.933504665700164 19 1.41159485787418 1.78722407901017 0.84397639585364 0.481278083772991 -0.0887673728508268 -0.49957757426858 0.304195897924847 -1.23884208383369 -0.153475724036624 -0.870486102788329 0.0955473331150403 -0.983708050882817 -0.3563445644514 20 -1.42997091652825 -0.490147045034213 0.272730237607695 -1.61029992954153 0.990787817197748 0.711687532608184 1.1885836012715 -0.371229188075638 1.24703459239952 -0.0389162332271516 0.883495749072872 2.58988026321017 3.33539552370368 21 -0.147247288176765 0.12328430415652 0.617549051912237 -0.18713077178262 0.256438107586694 0.17794983735083 0.412611806463263 -0.244809124559737 0.0947624806136492 0.723017223849532 -0.683948354633436 0.0873751276309269 -0.262209652750371 22 -0.187112676773894 -0.270777264595619 -1.01556818551606 0.0602850568600233 0.272419757757978 0.869133161879197 -0.657519461414234 2.32388522018189 -0.999936011525034 1.44671844178306 0.971157886040772 -0.358747904241515 -0.439657942096136 23 -1.82434047163768 -0.933480446068067 1.29474003766977 -1.94545221151036 0.33584651189654 0.359201654302844 0.513652924365886 -0.073197696696958 1.57139042812005 1.53329371326728 1.82076821859528 2.22740301867829 1.50063347195857 24 -2.29344084351335 -2.49161842344418 0.40383988742336 -2.36488074752948 1.4105254831956 1.42244117147792 1.17024166272172 0.84476650176855 1.79026875432495 0.648181858970515 -0.0857231057403538 -1.02789535292617 0.491288088952859 25 -0.434135932888305 0.740881989034652 0.699576357578518 -1.02405543187775 0.759529223983713 0.956656110895288 0.633299568656589 0.770733932268516 0.824988511714526 1.84287437634769 1.91045942063443 -0.502317207869366 0.132670133448219 26 -2.1920969546557 -2.49465664272271 0.354854763893431 -1.93155848635714 0.941979400289938 0.978917101414106 0.894860097289736 0.463239402831873 1.12537133317163 1.70528446191955 0.717792714479123 0.645888049108261 0.783968250169388 27 -1.46602269088422 -1.24921677101897 0.307977693653039 -1.55097364660989 0.618908494474798 0.662508171662042 0.475957173906078 0.484718674597707 0.401564892028249 0.55987973254026 -0.376938143754217 -0.933982629228218 0.390013151672955 28 -1.83317744236881 -1.53268787828701 2.55674262685865 -1.51827745783835 0.789409601746455 0.908747799728588 0.649971922941479 0.668373649931667 1.20058303519903 0.277963256075637 1.2504953198275 3.31370445071638 2.22035828885342 29 -0.784546628243178 0.276582579543931 3.01104958800057 -1.11978843206758 0.920823858422707 0.750217689886151 1.26153730009639 -0.403363882922417 0.400667296857811 -0.217597941303479 -0.724669537565068 -0.391945338467193 -0.650023936358253 30 0.464455916345135 1.3326356122229 -1.23059563374303 -0.357975958937414 1.18249746977104 1.54315938069757 -0.60339041154062 3.38308845958422 0.823740765148641 -0.129951318508883 -0.657979878422938 -0.499534924074273 -0.414476569095651  describe ( cg10k_trait [ : ,   3 : end ])   Trait1\nMin      -3.2041280147118\n1st Qu.  -0.645770976594801\nMedian   0.12500996951180798\nMean     0.0022113846331389903\n3rd Qu.  0.7233154897636109\nMax      3.47939787136478\nNAs      0\nNA%      0.0%\n\nTrait2\nMin      -3.51165862877157\n1st Qu.  -0.6426205239938769\nMedian   0.0335172506981786\nMean     0.0013525291443179934\n3rd Qu.  0.6574666174104795\nMax      4.91342267449592\nNAs      0\nNA%      0.0%\n\nTrait3\nMin      -3.93843646263987\n1st Qu.  -0.6409067201835312\nMedian   -0.000782161570259152\nMean     -0.0012959062525954158\n3rd Qu.  0.6371084235689337\nMax      7.91629946619107\nNAs      0\nNA%      0.0%\n\nTrait4\nMin      -3.60840330795393\n1st Qu.  -0.5460856267376792\nMedian   0.228165419346029\nMean     0.0023089259432067487\n3rd Qu.  0.7152907338009037\nMax      3.12768818152017\nNAs      0\nNA%      0.0%\n\nTrait5\nMin      -4.14874907974159\n1st Qu.  -0.6907651815712424\nMedian   0.03103429560265845\nMean     -0.0017903947913742396\n3rd Qu.  0.7349158784775832\nMax      2.71718436484651\nNAs      0\nNA%      0.0%\n\nTrait6\nMin      -3.82479174931095\n1st Qu.  -0.66279630694076\nMedian   0.03624198629403995\nMean     -0.001195980597331062\n3rd Qu.  0.7411755243742835\nMax      2.58972802240228\nNAs      0\nNA%      0.0%\n\nTrait7\nMin      -4.27245540828955\n1st Qu.  -0.6389232588654877\nMedian   0.0698010018021233\nMean     -0.0019890555724116853\n3rd Qu.  0.7104228734967848\nMax      2.65377857124275\nNAs      0\nNA%      0.0%\n\nTrait8\nMin      -5.62548796912517\n1st Qu.  -0.6015747053036895\nMedian   -0.0386301401797661\nMean     0.0006140754882985941\n3rd Qu.  0.5273417705306229\nMax      5.8057022359485\nNAs      0\nNA%      0.0%\n\nTrait9\nMin      -5.38196778211456\n1st Qu.  -0.6014287731518225\nMedian   0.10657100636146799\nMean     -0.0018096522573535152\n3rd Qu.  0.6985667613073132\nMax      2.57193558386964\nNAs      0\nNA%      0.0%\n\nTrait10\nMin      -3.54850550601412\n1st Qu.  -0.6336406665339003\nMedian   -0.0966507436079331\nMean     -0.0004370294352533275\n3rd Qu.  0.498610364573902\nMax      6.53782005410551\nNAs      0\nNA%      0.0%\n\nTrait11\nMin      -3.26491021902041\n1st Qu.  -0.6736846396608624\nMedian   -0.0680437088585371\nMean     -0.0006159181111862523\n3rd Qu.  0.6554864114250585\nMax      4.26240968462615\nNAs      0\nNA%      0.0%\n\nTrait12\nMin      -8.85190902714652\n1st Qu.  -0.5396855871831098\nMedian   -0.1410985990171995\nMean     -0.0005887830910961934\n3rd Qu.  0.35077884186653374\nMax      13.2114017261714\nNAs      0\nNA%      0.0%\n\nTrait13\nMin      -5.59210353493304\n1st Qu.  -0.49228904714392474\nMedian   -0.14102175804213551\nMean     -0.0001512383146454028\n3rd Qu.  0.32480412746681697\nMax      24.174436145414\nNAs      0\nNA%      0.0%  Y   =   convert ( Matrix { Float64 },   cg10k_trait [ : ,   3 : 15 ])  histogram ( Y ,   layout   =   13 )", 
            "title": "Phenotypes"
        }, 
        {
            "location": "/man/heritability/#pre-processing-data-for-heritability-analysis", 
            "text": "To prepare variance component model fitting, we form an instance of  VarianceComponentVariate . The two variance components are $(2\\Phi, I)$.  using   VarianceComponentModels  # form data as VarianceComponentVariate  cg10kdata   =   VarianceComponentVariate ( Y ,   ( 2 \u03a6grm ,   eye ( size ( Y ,   1 ))))  fieldnames ( cg10kdata )   3-element Array{Symbol,1}:\n :Y\n :X\n :V  cg10kdata   VarianceComponentModels.VarianceComponentVariate{Float64,2,Array{Float64,2},Array{Float64,2},Array{Float64,2}}([-1.81573 -0.94615 \u2026 -1.02853 -0.394049; -1.2444 0.10966 \u2026 1.09065 0.0256616; \u2026 ; 0.886626 0.487408 \u2026 -0.636874 -0.439825; -1.24394 0.213697 \u2026 0.299931 0.392809],,(\n[1.00583 0.00659955 \u2026 -0.000129257 -0.00562459; 0.00659955 0.99784 \u2026 0.00181974 0.00691145; \u2026 ; -0.000129257 0.00181974 \u2026 1.00197 0.00105123; -0.00562459 0.00691145 \u2026 0.00105123 1.00158],\n\n[1.0 0.0 \u2026 0.0 0.0; 0.0 1.0 \u2026 0.0 0.0; \u2026 ; 0.0 0.0 \u2026 1.0 0.0; 0.0 0.0 \u2026 0.0 1.0]))  Before fitting the variance component model, we pre-compute the eigen-decomposition of $2\\Phi_{\\text{GRM}}$, the rotated responses, and the constant part in log-likelihood, and store them as a  TwoVarCompVariateRotate  instance, which is re-used in various variane component estimation procedures.  # pre-compute eigen-decomposition (~50 secs on my laptop)  @time   cg10kdata_rotated   =   TwoVarCompVariateRotate ( cg10kdata )  fieldnames ( cg10kdata_rotated )    46.051433 seconds (726.58 k allocations: 1.024 GB, 0.45% gc time)\n\n\n\n\n\n5-element Array{Symbol,1}:\n :Yrot    \n :Xrot    \n :eigval  \n :eigvec  \n :logdetV2", 
            "title": "Pre-processing data for heritability analysis"
        }, 
        {
            "location": "/man/heritability/#save-intermediate-results", 
            "text": "We don't want to re-compute SnpArray and empirical kinship matrices again and again for heritibility analysis.  # Pkg.add( JLD )  #using JLD  #@save  cg10k.jld  #whos()                             Base  47039 KB     Module\n                       BinDeps    218 KB     Module\n                         Blosc     59 KB     Module\n                    ColorTypes   8492 KB     Module\n                        Colors   8414 KB     Module\n                        Compat   8100 KB     Module\n                         Conda   8491 KB     Module\n                          Core  19714 KB     Module\n                    DataArrays   8444 KB     Module\n                    DataFrames   8940 KB     Module\n                DataStructures    234 KB     Module\n                        FileIO   9044 KB     Module\n             FixedPointNumbers   8573 KB     Module\n               FixedSizeArrays   8222 KB     Module\n                          GZip   8003 KB     Module\n                          HDF5   8809 KB     Module\n                        IJulia 2082578 KB     Module\n                         Ipopt     32 KB     Module\n              IterativeSolvers    333 KB     Module\n                     Iterators     44 KB     Module\n                           JLD   9092 KB     Module\n                          JSON   8124 KB     Module\n                        KNITRO    218 KB     Module\n                  LaTeXStrings   4622 bytes  Module\n                 LegacyStrings     67 KB     Module\n                    MacroTools   8190 KB     Module\n                          Main 2498588 KB     Module\n                  MathProgBase    272 KB     Module\n                      Measures     24 KB     Module\n                        Nettle   8048 KB     Module\n                    PlotThemes   7999 KB     Module\n                     PlotUtils   8264 KB     Module\n                         Plots  12383 KB     Module\n                        PyCall  10435 KB     Module\n                        PyPlot   9938 KB     Module\n                   RecipesBase   8158 KB     Module\n                      Reexport   6178 bytes  Module\n                           SHA     71 KB     Module\n                       Showoff   8024 KB     Module\n                     SnpArrays   8191 KB     Module\n             SortingAlgorithms     28 KB     Module\n              SpecialFunctions   8257 KB     Module\n                     StatsBase   8662 KB     Module\n                     URIParser   8064 KB     Module\n       VarianceComponentModels    189 KB     Module\n                             Y    677 KB     6670\u00d713 Array{Float64,2}\n                           ZMQ   8058 KB     Module\n                             _     77 KB     630860-element BitArray{1}\n                         cg10k 1027303 KB     6670\u00d7630860 SnpArrays.SnpArray{2}\n                   cg10k_trait    978 KB     6670\u00d715 DataFrames.DataFrame\n                     cg10kdata 695816 KB     VarianceComponentModels.VarianceCo\u2026\n             cg10kdata_rotated 348299 KB     VarianceComponentModels.TwoVarComp\u2026\n                           maf   4928 KB     630860-element Array{Float64,1}\n               missings_by_snp   4928 KB     630860-element Array{Int64,1}\n                        people      8 bytes  Int64\n                          snps      8 bytes  Int64\n                          \u03a6grm 347569 KB     6670\u00d76670 Array{Float64,2}  To load workspace  using   SnpArrays ,   JLD ,   DataFrames ,   VarianceComponentModels ,   Plots  pyplot ()  @load   cg10k.jld  whos ()                             Base  39406 KB     Module\n                       BinDeps    218 KB     Module\n                         Blosc     59 KB     Module\n                    ColorTypes   6371 KB     Module\n                        Colors   6378 KB     Module\n                        Compat   6099 KB     Module\n                         Conda   6460 KB     Module\n                          Core  15532 KB     Module\n                    DataArrays   6388 KB     Module\n                    DataFrames    649 KB     Module\n                DataStructures    234 KB     Module\n                        FileIO   6870 KB     Module\n             FixedPointNumbers   6579 KB     Module\n               FixedSizeArrays   6226 KB     Module\n                          GZip   6004 KB     Module\n                          HDF5   6657 KB     Module\n                        IJulia   7125 KB     Module\n                         Ipopt     32 KB     Module\n              IterativeSolvers    333 KB     Module\n                     Iterators     44 KB     Module\n                           JLD   6839 KB     Module\n                          JSON   6132 KB     Module\n                        KNITRO    218 KB     Module\n                  LaTeXStrings   4622 bytes  Module\n                 LegacyStrings     68 KB     Module\n                    MacroTools   6196 KB     Module\n                          Main 2489170 KB     Module\n                  MathProgBase    272 KB     Module\n                      Measures     21 KB     Module\n                        Nettle   6068 KB     Module\n                    PlotThemes   6016 KB     Module\n                     PlotUtils   6174 KB     Module\n                         Plots   9209 KB     Module\n                        PyCall   7377 KB     Module\n                        PyPlot   7686 KB     Module\n                   RecipesBase    179 KB     Module\n                      Reexport   6178 bytes  Module\n                           SHA     71 KB     Module\n                       Showoff     27 KB     Module\n                     SnpArrays     98 KB     Module\n             SortingAlgorithms     28 KB     Module\n              SpecialFunctions   6277 KB     Module\n                     StatsBase    573 KB     Module\n                     URIParser   6085 KB     Module\n       VarianceComponentModels    189 KB     Module\n                             Y    677 KB     6670\u00d713 Array{Float64,2}\n                           ZMQ   6076 KB     Module\n                         cg10k 1027303 KB     6670\u00d7630860 SnpArrays.SnpArray{2}\n                   cg10k_trait    978 KB     6670\u00d715 DataFrames.DataFrame\n                     cg10kdata 695816 KB     VarianceComponentModels.VarianceCo\u2026\n             cg10kdata_rotated 348299 KB     VarianceComponentModels.TwoVarComp\u2026\n                           maf   4928 KB     630860-element Array{Float64,1}\n               missings_by_snp   4928 KB     630860-element Array{Int64,1}\n                        people      8 bytes  Int64\n                          snps      8 bytes  Int64\n                          \u03a6grm 347569 KB     6670\u00d76670 Array{Float64,2}", 
            "title": "Save intermediate results"
        }, 
        {
            "location": "/man/heritability/#heritability-of-single-traits", 
            "text": "We use Fisher scoring algorithm to fit variance component model for each single trait.  # heritability from single trait analysis  hST   =   zeros ( 13 )  # standard errors of estimated heritability  hST_se   =   zeros ( 13 )  # additive genetic effects  \u03c32a   =   zeros ( 13 )  # enviromental effects  \u03c32e   =   zeros ( 13 )  tic ()  for   trait   in   1 : 13 \n     println ( names ( cg10k_trait )[ trait   +   2 ]) \n     # form data set for trait j \n     traitj_data   =   TwoVarCompVariateRotate ( cg10kdata_rotated . Yrot [ : ,   trait ],   cg10kdata_rotated . Xrot ,  \n         cg10kdata_rotated . eigval ,   cg10kdata_rotated . eigvec ,   cg10kdata_rotated . logdetV2 ) \n     # initialize model parameters \n     traitj_model   =   VarianceComponentModel ( traitj_data ) \n     # estimate variance components \n     _ ,   _ ,   _ ,   \u03a3cov ,   _ ,   _   =   mle_fs! ( traitj_model ,   traitj_data ;   solver =: Ipopt ,   verbose = false ) \n     \u03c32a [ trait ]   =   traitj_model . \u03a3 [ 1 ][ 1 ] \n     \u03c32e [ trait ]   =   traitj_model . \u03a3 [ 2 ][ 1 ] \n     @show   \u03c32a [ trait ],   \u03c32e [ trait ] \n     h ,   hse   =   heritability ( traitj_model . \u03a3 ,   \u03a3cov ) \n     hST [ trait ]   =   h [ 1 ] \n     hST_se [ trait ]   =   hse [ 1 ]  end  toc ()   Trait1\n(\u03c32a[trait],\u03c32e[trait]) = (0.26104123217397623,0.7356884432614108)\nTrait2\n(\u03c32a[trait],\u03c32e[trait]) = (0.18874147380283665,0.8106899991616688)\nTrait3\n(\u03c32a[trait],\u03c32e[trait]) = (0.3185719276547346,0.6801458862875847)\nTrait4\n(\u03c32a[trait],\u03c32e[trait]) = (0.26556901333953487,0.7303588364945325)\nTrait5\n(\u03c32a[trait],\u03c32e[trait]) = (0.28123321193922013,0.7167989047155017)\nTrait6\n(\u03c32a[trait],\u03c32e[trait]) = (0.2829461149704479,0.7165629534396428)\nTrait7\n(\u03c32a[trait],\u03c32e[trait]) = (0.21543856403949083,0.7816211121585646)\nTrait8\n(\u03c32a[trait],\u03c32e[trait]) = (0.19412648732666096,0.8055277649986139)\nTrait9\n(\u03c32a[trait],\u03c32e[trait]) = (0.24789561127296741,0.7504615853619878)\nTrait10\n(\u03c32a[trait],\u03c32e[trait]) = (0.10007455815561886,0.899815277360586)\nTrait11\n(\u03c32a[trait],\u03c32e[trait]) = (0.16486778169300415,0.8338002257315682)\nTrait12\n(\u03c32a[trait],\u03c32e[trait]) = (0.08298660416198149,0.9158035668415443)\nTrait13\n(\u03c32a[trait],\u03c32e[trait]) = (0.05684248094794614,0.9423653381325947)\nelapsed time: 0.19565668 seconds\n\n\n\n\n\n0.19565668  # heritability and standard errors  [ hST ;   hST_se ]   2\u00d713 Array{Float64,2}:\n 0.261898  0.188849   0.318981   \u2026  0.165088   0.0830871  0.0568875\n 0.079869  0.0867203  0.0741462     0.0887725  0.0944835  0.0953863", 
            "title": "Heritability of single traits"
        }, 
        {
            "location": "/man/heritability/#pairwise-traits", 
            "text": "Joint analysis of multiple traits is subject to intensive research recently. Following code snippet does joint analysis of all pairs of traits, a total of 78 bivariate variane component models.  # additive genetic effects (2x2 psd matrices) from bavariate trait analysis;  \u03a3a   =   Array { Matrix { Float64 }}( 13 ,   13 )  # environmental effects (2x2 psd matrices) from bavariate trait analysis;  \u03a3e   =   Array { Matrix { Float64 }}( 13 ,   13 )  tic ()  for   i   in   1 : 13 \n     for   j   in   ( i + 1 ) : 13 \n         println ( names ( cg10k_trait )[ i   +   2 ],   names ( cg10k_trait )[ j   +   2 ]) \n         # form data set for (trait1, trait2) \n         traitij_data   =   TwoVarCompVariateRotate ( cg10kdata_rotated . Yrot [ : ,   [ i ; j ]],   cg10kdata_rotated . Xrot ,  \n             cg10kdata_rotated . eigval ,   cg10kdata_rotated . eigvec ,   cg10kdata_rotated . logdetV2 ) \n         # initialize model parameters \n         traitij_model   =   VarianceComponentModel ( traitij_data ) \n         # estimate variance components \n         mle_fs! ( traitij_model ,   traitij_data ;   solver =: Ipopt ,   verbose = false ) \n         \u03a3a [ i ,   j ]   =   traitij_model . \u03a3 [ 1 ] \n         \u03a3e [ i ,   j ]   =   traitij_model . \u03a3 [ 2 ] \n         @show   \u03a3a [ i ,   j ],   \u03a3e [ i ,   j ] \n     end  end  toc ()   Trait1Trait2\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.260119 0.176216; 0.176216 0.187376],\n\n[0.736589 0.583892; 0.583892 0.812033])\nTrait1Trait3\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.261564 -0.0131268; -0.0131268 0.319057],\n\n[0.73518 -0.121127; -0.121127 0.679679])\nTrait1Trait4\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.26088 0.222614; 0.222614 0.265581],\n\n[0.735846 0.599435; 0.599435 0.730347])\nTrait1Trait5\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.260783 -0.147012; -0.147012 0.281877],\n\n[0.735937 -0.254584; -0.254584 0.716176])\nTrait1Trait6\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.260707 -0.129356; -0.129356 0.283188],\n\n[0.736013 -0.231361; -0.231361 0.716329])\nTrait1Trait7\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.260308 -0.140258; -0.140258 0.215081],\n\n[0.736406 -0.197805; -0.197805 0.781985])\nTrait1Trait8\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.261035 -0.0335296; -0.0335296 0.194143],\n\n[0.735695 -0.126272; -0.126272 0.805512])\nTrait1Trait9\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.263016 -0.204865; -0.204865 0.246796],\n\n[0.733794 -0.30745; -0.30745 0.751544])\nTrait1Trait10\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.260898 -0.0998176; -0.0998176 0.0970233],\n\n[0.735828 -0.303609; -0.303609 0.902853])\nTrait1Trait11\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.26074 -0.138983; -0.138983 0.163063],\n\n[0.735982 -0.359175; -0.359175 0.835595])\nTrait1Trait12\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.263069 -0.145536; -0.145536 0.0805136],\n\n[0.733781 -0.0416975; -0.0416975 0.918359])\nTrait1Trait13\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.262344 -0.108896; -0.108896 0.051294],\n\n[0.73445 -0.113996; -0.113996 0.947942])\nTrait2Trait3\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.189015 0.146157; 0.146157 0.320529],\n\n[0.810418 0.0974992; 0.0974992 0.678271])\nTrait2Trait4\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.188395 0.0752146; 0.0752146 0.265558],\n\n[0.81103 0.220495; 0.220495 0.730369])\nTrait2Trait5\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.188716 -0.011314; -0.011314 0.281247],\n\n[0.810715 -0.0370105; -0.0370105 0.716786])\nTrait2Trait6\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.188774 -0.0031066; -0.0031066 0.283013],\n\n[0.810658 -0.0211827; -0.0211827 0.716499])\nTrait2Trait7\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.188352 -0.0299579; -0.0299579 0.215189],\n\n[0.811072 -0.00136939; -0.00136939 0.781868])\nTrait2Trait8\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.189262 0.0331423; 0.0331423 0.194666],\n\n[0.810182 -0.0326003; -0.0326003 0.805005])\nTrait2Trait9\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.187285 -0.0854146; -0.0854146 0.246719],\n\n[0.812133 -0.0808791; -0.0808791 0.751617])\nTrait2Trait10\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.188965 -0.125319; -0.125319 0.100121],\n\n[0.810498 -0.271071; -0.271071 0.899849])\nTrait2Trait11\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.187762 -0.118479; -0.118479 0.166273],\n\n[0.811653 -0.295549; -0.295549 0.832437])\nTrait2Trait12\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.188191 -0.0905383; -0.0905383 0.0822634],\n\n[0.811272 0.045422; 0.045422 0.916586])\nTrait2Trait13\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.18826 -0.0707041; -0.0707041 0.0547239],\n\n[0.811217 0.0737977; 0.0737977 0.944521])\nTrait3Trait4\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.31852 -0.154339; -0.154339 0.264754],\n\n[0.680196 -0.30344; -0.30344 0.731152])\nTrait3Trait5\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.31897 0.184354; 0.184354 0.2825],\n\n[0.67976 0.336411; 0.336411 0.715567])\nTrait3Trait6\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.319566 0.16664; 0.16664 0.285031],\n\n[0.679183 0.297698; 0.297698 0.714536])\nTrait3Trait7\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.318576 0.166852; 0.166852 0.215232],\n\n[0.680142 0.347139; 0.347139 0.781823])\nTrait3Trait8\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.320499 0.0575319; 0.0575319 0.197245],\n\n[0.678283 0.0442597; 0.0442597 0.802474])\nTrait3Trait9\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.318719 0.137292; 0.137292 0.246976],\n\n[0.680004 0.267105; 0.267105 0.751357])\nTrait3Trait10\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.318915 -0.0786338; -0.0786338 0.101103],\n\n[0.679815 -0.140789; -0.140789 0.898798])\nTrait3Trait11\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.317822 -0.017984; -0.017984 0.164743],\n\n[0.680871 -0.114166; -0.114166 0.833923])\nTrait3Trait12\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.320888 0.0845248; 0.0845248 0.0869868],\n\n[0.677914 0.0340133; 0.0340133 0.911841])\nTrait3Trait13\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.323009 0.110681; 0.110681 0.0611739],\n\n[0.675901 -0.00729662; -0.00729662 0.938072])\nTrait4Trait5\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.265667 -0.215848; -0.215848 0.282919],\n\n[0.730254 -0.376675; -0.376675 0.715164])\nTrait4Trait6\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.266143 -0.200634; -0.200634 0.284442],\n\n[0.729794 -0.346804; -0.346804 0.715112])\nTrait4Trait7\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.26449 -0.182752; -0.182752 0.214117],\n\n[0.731415 -0.326172; -0.326172 0.782935])\nTrait4Trait8\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.266694 -0.0976354; -0.0976354 0.196126],\n\n[0.729266 -0.15036; -0.15036 0.803571])\nTrait4Trait9\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.270037 -0.227407; -0.227407 0.248046],\n\n[0.726025 -0.415601; -0.415601 0.750298])\nTrait4Trait10\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.265543 -0.0338107; -0.0338107 0.0996098],\n\n[0.730395 -0.227725; -0.227725 0.900275])\nTrait4Trait11\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.265628 -0.09674; -0.09674 0.163274],\n\n[0.730302 -0.272611; -0.272611 0.835371])\nTrait4Trait12\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.268164 -0.141613; -0.141613 0.0803968],\n\n[0.727883 -0.0828465; -0.0828465 0.918446])\nTrait4Trait13\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.266171 -0.0980731; -0.0980731 0.0540102],\n\n[0.729775 -0.22506; -0.22506 0.945204])\nTrait5Trait6\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.281592 0.280898; 0.280898 0.282298],\n\n[0.716455 0.660368; 0.660368 0.717195])\nTrait5Trait7\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.280814 0.2321; 0.2321 0.211662],\n\n[0.717218 0.674304; 0.674304 0.785343])\nTrait5Trait8\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.28134 0.163948; 0.163948 0.192703],\n\n[0.716701 0.221032; 0.221032 0.806922])\nTrait5Trait9\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.283878 0.244532; 0.244532 0.241293],\n\n[0.714244 0.508417; 0.508417 0.756894])\nTrait5Trait10\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.281813 -0.0462114; -0.0462114 0.101481],\n\n[0.716238 -0.0572111; -0.0572111 0.898424])\nTrait5Trait11\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.280424 0.0202495; 0.0202495 0.164003],\n\n[0.717586 -0.0352436; -0.0352436 0.834649])\nTrait5Trait12\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.281427 0.0616131; 0.0616131 0.0827166],\n\n[0.716615 0.0529286; 0.0529286 0.916074])\nTrait5Trait13\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.282292 0.0704221; 0.0704221 0.0569463],\n\n[0.715782 0.0528374; 0.0528374 0.942268])\nTrait6Trait7\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.282961 0.220656; 0.220656 0.213856],\n\n[0.716549 0.581083; 0.581083 0.783179])\nTrait6Trait8\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.282961 0.18408; 0.18408 0.192379],\n\n[0.716549 0.436597; 0.436597 0.807246])\nTrait6Trait9\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.284978 0.234436; 0.234436 0.243207],\n\n[0.714601 0.476826; 0.476826 0.755028])\nTrait6Trait10\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.283655 -0.0435485; -0.0435485 0.102025],\n\n[0.715877 -0.0591681; -0.0591681 0.897886])\nTrait6Trait11\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.281522 0.0279992; 0.0279992 0.16343],\n\n[0.717946 -0.0524106; -0.0524106 0.835213])\nTrait6Trait12\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.283114 0.0571399; 0.0571399 0.0826789],\n\n[0.716403 0.0479199; 0.0479199 0.916112])\nTrait6Trait13\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.28382 0.0611212; 0.0611212 0.0570817],\n\n[0.715722 0.0532698; 0.0532698 0.942133])\nTrait7Trait8\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.213857 0.0884555; 0.0884555 0.192373],\n\n[0.783178 -0.0568331; -0.0568331 0.807252])\nTrait7Trait9\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.218756 0.217042; 0.217042 0.244144],\n\n[0.778433 0.462901; 0.462901 0.754123])\nTrait7Trait10\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.216273 -0.0421146; -0.0421146 0.10209],\n\n[0.780807 -0.0859075; -0.0859075 0.897822])\nTrait7Trait11\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.214069 0.0206967; 0.0206967 0.163474],\n\n[0.782961 -0.048148; -0.048148 0.83517])\nTrait7Trait12\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.214932 0.0757874; 0.0757874 0.0808731],\n\n[0.782132 0.0346956; 0.0346956 0.917915])\nTrait7Trait13\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.215959 0.0749373; 0.0749373 0.0545934],\n\n[0.781139 0.0389058; 0.0389058 0.944622])\nTrait8Trait9\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.194555 0.112816; 0.112816 0.247244],\n\n[0.805124 0.184778; 0.184778 0.751098])\nTrait8Trait10\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.194441 -0.0156342; -0.0156342 0.100426],\n\n[0.805222 0.0119826; 0.0119826 0.899468])\nTrait8Trait11\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.193853 0.0225325; 0.0225325 0.164669],\n\n[0.805796 -0.0272747; -0.0272747 0.833997])\nTrait8Trait12\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.193951 -0.00287601; -0.00287601 0.0828573],\n\n[0.8057 0.0336128; 0.0336128 0.915932])\nTrait8Trait13\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.19398 0.00407867; 0.00407867 0.0569071],\n\n[0.805672 0.0378817; 0.0378817 0.942301])\nTrait9Trait10\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.247294 -0.00230836; -0.00230836 0.0998264],\n\n[0.751051 0.0740729; 0.0740729 0.900061])\nTrait9Trait11\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.247823 0.0318235; 0.0318235 0.16489],\n\n[0.750532 0.152285; 0.152285 0.833778])\nTrait9Trait12\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.250335 0.0845714; 0.0845714 0.0887587],\n\n[0.748091 0.107756; 0.107756 0.910108])\nTrait9Trait13\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.249442 0.0934845; 0.0934845 0.057932],\n\n[0.748975 0.0982191; 0.0982191 0.941335])\nTrait10Trait11\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.0931397 0.100034; 0.100034 0.164955],\n\n[0.906703 0.474427; 0.474427 0.833715])\nTrait10Trait12\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.0967247 0.0564043; 0.0564043 0.0794528],\n\n[0.90315 0.0853232; 0.0853232 0.919334])\nTrait10Trait13\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.100985 -0.0279916; -0.0279916 0.0578369],\n\n[0.898937 0.166051; 0.166051 0.94139])\nTrait11Trait12\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.163841 0.0570318; 0.0570318 0.0792144],\n\n[0.834814 0.145597; 0.145597 0.919552])\nTrait11Trait13\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.164883 -0.00158414; -0.00158414 0.0574968],\n\n[0.833798 0.200612; 0.200612 0.941715])\nTrait12Trait13\n(\u03a3a[i,j],\u03a3e[i,j]) = (\n[0.0845946 0.0685052; 0.0685052 0.0554759],\n\n[0.914214 0.573152; 0.573152 0.943735])\nelapsed time: 7.752196517 seconds\n\n\n\n\n\n7.752196517", 
            "title": "Pairwise traits"
        }, 
        {
            "location": "/man/heritability/#3-trait-analysis", 
            "text": "Researchers want to jointly analyze traits 5-7. Our strategy is to try both Fisher scoring and MM algorithm with different starting point, and choose the best local optimum. We first form the data set and run Fisher scoring, which yields a final objective value -1.4700991+04.  traitidx   =   5 : 7  # form data set  trait57_data   =   TwoVarCompVariateRotate ( cg10kdata_rotated . Yrot [ : ,   traitidx ],   cg10kdata_rotated . Xrot ,  \n     cg10kdata_rotated . eigval ,   cg10kdata_rotated . eigvec ,   cg10kdata_rotated . logdetV2 )  # initialize model parameters  trait57_model   =   VarianceComponentModel ( trait57_data )  # estimate variance components  @time   mle_fs! ( trait57_model ,   trait57_data ;   solver =: Ipopt ,   verbose = true )  trait57_model   This is Ipopt version 3.12.4, running with linear solver mumps.\nNOTE: Other linear solvers might be more efficient (see Ipopt documentation).\n\nNumber of nonzeros in equality constraint Jacobian...:        0\nNumber of nonzeros in inequality constraint Jacobian.:        0\nNumber of nonzeros in Lagrangian Hessian.............:       78\n\nTotal number of variables............................:       12\n                     variables with only lower bounds:        0\n                variables with lower and upper bounds:        0\n                     variables with only upper bounds:        0\nTotal number of equality constraints.................:        0\nTotal number of inequality constraints...............:        0\n        inequality constraints with only lower bounds:        0\n   inequality constraints with lower and upper bounds:        0\n        inequality constraints with only upper bounds:        0\n\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n   0  3.0247512e+04 0.00e+00 1.00e+02   0.0 0.00e+00    -  0.00e+00 0.00e+00   0 \n   5  1.6834796e+04 0.00e+00 4.07e+02 -11.0 3.66e-01    -  1.00e+00 1.00e+00f  1 MaxS\n  10  1.4744497e+04 0.00e+00 1.12e+02 -11.0 2.45e-01    -  1.00e+00 1.00e+00f  1 MaxS\n  15  1.4701497e+04 0.00e+00 1.30e+01 -11.0 1.15e-01  -4.5 1.00e+00 1.00e+00f  1 MaxS\n  20  1.4700992e+04 0.00e+00 6.65e-01 -11.0 1.74e-04  -6.9 1.00e+00 1.00e+00f  1 MaxS\n  25  1.4700991e+04 0.00e+00 2.77e-02 -11.0 7.36e-06  -9.2 1.00e+00 1.00e+00f  1 MaxS\n  30  1.4700991e+04 0.00e+00 1.15e-03 -11.0 3.06e-07 -11.6 1.00e+00 1.00e+00f  1 MaxS\n  35  1.4700991e+04 0.00e+00 4.76e-05 -11.0 1.27e-08 -14.0 1.00e+00 1.00e+00h  1 MaxS\n  40  1.4700991e+04 0.00e+00 1.97e-06 -11.0 5.26e-10 -16.4 1.00e+00 1.00e+00f  1 MaxSA\n  45  1.4700991e+04 0.00e+00 8.17e-08 -11.0 2.18e-11 -18.8 1.00e+00 1.00e+00h  1 MaxSA\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n\nNumber of Iterations....: 49\n\n                                   (scaled)                 (unscaled)\nObjective...............:   4.4724330090668286e+02    1.4700991028593347e+04\nDual infeasibility......:   6.4345872286001950e-09    2.1150637455850896e-07\nConstraint violation....:   0.0000000000000000e+00    0.0000000000000000e+00\nComplementarity.........:   0.0000000000000000e+00    0.0000000000000000e+00\nOverall NLP error.......:   6.4345872286001950e-09    2.1150637455850896e-07\n\n\nNumber of objective function evaluations             = 50\nNumber of objective gradient evaluations             = 50\nNumber of equality constraint evaluations            = 0\nNumber of inequality constraint evaluations          = 0\nNumber of equality constraint Jacobian evaluations   = 0\nNumber of inequality constraint Jacobian evaluations = 0\nNumber of Lagrangian Hessian evaluations             = 49\nTotal CPU secs in IPOPT (w/o function evaluations)   =      0.013\nTotal CPU secs in NLP function evaluations           =      0.085\n\nEXIT: Optimal Solution Found.\n  0.118428 seconds (66.82 k allocations: 6.250 MB)\n\n\n\n\n\nVarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}(,(\n[0.281163 0.280014 0.232384; 0.280014 0.284899 0.220285; 0.232384 0.220285 0.212687],\n\n[0.716875 0.66125 0.674025; 0.66125 0.714602 0.581433; 0.674025 0.581433 0.784324]),,Char[],Float64[],-Inf,Inf)  We then run the MM algorithm, starting from the Fisher scoring answer. MM finds an improved solution with objective value 8.955397e+03.  # trait59_model contains the fitted model by Fisher scoring now  @time   mle_mm! ( trait57_model ,   trait57_data ;   verbose = true )  trait57_model        MM Algorithm\n  Iter      Objective  \n--------  -------------\n       0  -1.470099e+04\n       1  -1.470099e+04\n\n  0.363096 seconds (539.27 k allocations: 20.482 MB, 2.19% gc time)\n\n\n\n\n\nVarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}(,(\n[0.281163 0.280014 0.232384; 0.280014 0.284899 0.220285; 0.232384 0.220285 0.212687],\n\n[0.716875 0.66125 0.674025; 0.66125 0.714602 0.581433; 0.674025 0.581433 0.784324]),,Char[],Float64[],-Inf,Inf)  Do another run of MM algorithm from default starting point. It leads to a slightly better local optimum -1.470104e+04, slighly worse than the Fisher scoring result. Follow up anlaysis should use the Fisher scoring result.  # default starting point  trait57_model   =   VarianceComponentModel ( trait57_data )  @time   _ ,   _ ,   _ ,   \u03a3cov ,   =   mle_mm! ( trait57_model ,   trait57_data ;   verbose = true )  trait57_model        MM Algorithm\n  Iter      Objective  \n--------  -------------\n       0  -3.024751e+04\n       1  -2.040338e+04\n       2  -1.656127e+04\n       3  -1.528591e+04\n       4  -1.491049e+04\n       5  -1.480699e+04\n       6  -1.477870e+04\n       7  -1.477026e+04\n       8  -1.476696e+04\n       9  -1.476499e+04\n      10  -1.476339e+04\n      20  -1.475040e+04\n      30  -1.474042e+04\n      40  -1.473272e+04\n      50  -1.472677e+04\n      60  -1.472215e+04\n      70  -1.471852e+04\n      80  -1.471565e+04\n      90  -1.471336e+04\n     100  -1.471152e+04\n     110  -1.471002e+04\n     120  -1.470879e+04\n     130  -1.470778e+04\n     140  -1.470694e+04\n     150  -1.470623e+04\n     160  -1.470563e+04\n     170  -1.470513e+04\n     180  -1.470469e+04\n     190  -1.470432e+04\n     200  -1.470400e+04\n     210  -1.470372e+04\n     220  -1.470347e+04\n     230  -1.470326e+04\n     240  -1.470307e+04\n     250  -1.470290e+04\n     260  -1.470275e+04\n     270  -1.470262e+04\n     280  -1.470250e+04\n     290  -1.470239e+04\n     300  -1.470229e+04\n     310  -1.470220e+04\n     320  -1.470213e+04\n     330  -1.470205e+04\n     340  -1.470199e+04\n     350  -1.470193e+04\n     360  -1.470187e+04\n     370  -1.470182e+04\n     380  -1.470177e+04\n     390  -1.470173e+04\n     400  -1.470169e+04\n     410  -1.470165e+04\n     420  -1.470162e+04\n     430  -1.470159e+04\n     440  -1.470156e+04\n     450  -1.470153e+04\n     460  -1.470150e+04\n     470  -1.470148e+04\n     480  -1.470146e+04\n     490  -1.470143e+04\n     500  -1.470141e+04\n     510  -1.470140e+04\n     520  -1.470138e+04\n     530  -1.470136e+04\n     540  -1.470134e+04\n     550  -1.470133e+04\n     560  -1.470132e+04\n     570  -1.470130e+04\n     580  -1.470129e+04\n     590  -1.470128e+04\n     600  -1.470127e+04\n     610  -1.470125e+04\n     620  -1.470124e+04\n     630  -1.470123e+04\n     640  -1.470122e+04\n     650  -1.470122e+04\n     660  -1.470121e+04\n     670  -1.470120e+04\n     680  -1.470119e+04\n     690  -1.470118e+04\n     700  -1.470118e+04\n     710  -1.470117e+04\n     720  -1.470116e+04\n     730  -1.470116e+04\n     740  -1.470115e+04\n     750  -1.470114e+04\n     760  -1.470114e+04\n     770  -1.470113e+04\n     780  -1.470113e+04\n     790  -1.470112e+04\n     800  -1.470112e+04\n     810  -1.470111e+04\n     820  -1.470111e+04\n     830  -1.470111e+04\n     840  -1.470110e+04\n     850  -1.470110e+04\n     860  -1.470109e+04\n     870  -1.470109e+04\n     880  -1.470109e+04\n     890  -1.470108e+04\n     900  -1.470108e+04\n     910  -1.470108e+04\n     920  -1.470108e+04\n     930  -1.470107e+04\n     940  -1.470107e+04\n     950  -1.470107e+04\n     960  -1.470106e+04\n     970  -1.470106e+04\n     980  -1.470106e+04\n     990  -1.470106e+04\n    1000  -1.470106e+04\n    1010  -1.470105e+04\n    1020  -1.470105e+04\n    1030  -1.470105e+04\n    1040  -1.470105e+04\n    1050  -1.470105e+04\n    1060  -1.470104e+04\n    1070  -1.470104e+04\n    1080  -1.470104e+04\n    1090  -1.470104e+04\n    1100  -1.470104e+04\n    1110  -1.470104e+04\n\n  0.846316 seconds (153.65 k allocations: 15.453 MB, 0.89% gc time)\n\n\n\n\n\nVarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}(,(\n[0.281188 0.280032 0.232439; 0.280032 0.284979 0.220432; 0.232439 0.220432 0.212922],\n\n[0.71685 0.661232 0.67397; 0.661232 0.71452 0.581287; 0.67397 0.581287 0.784092]),,Char[],Float64[],-Inf,Inf)  Heritability from 3-variate estimate and their standard errors.  h ,   hse   =   heritability ( trait57_model . \u03a3 ,   \u03a3cov )  [ h ;   hse ]   2\u00d73 Array{Float64,2}:\n 0.281741   0.285122   0.21356  \n 0.0778033  0.0773313  0.0841103", 
            "title": "3-trait analysis"
        }, 
        {
            "location": "/man/heritability/#13-trait-joint-analysis", 
            "text": "In some situations, such as studying the genetic covariance, we need to jointly analyze 13 traits. We first try the  Fisher scoring algorithm .  # initialize model parameters  traitall_model   =   VarianceComponentModel ( cg10kdata_rotated )  # estimate variance components using Fisher scoring algorithm  @time   mle_fs! ( traitall_model ,   cg10kdata_rotated ;   solver =: Ipopt ,   verbose = true )   This is Ipopt version 3.12.4, running with linear solver mumps.\nNOTE: Other linear solvers might be more efficient (see Ipopt documentation).\n\nNumber of nonzeros in equality constraint Jacobian...:        0\nNumber of nonzeros in inequality constraint Jacobian.:        0\nNumber of nonzeros in Lagrangian Hessian.............:    16653\n\nTotal number of variables............................:      182\n                     variables with only lower bounds:        0\n                variables with lower and upper bounds:        0\n                     variables with only upper bounds:        0\nTotal number of equality constraints.................:        0\nTotal number of inequality constraints...............:        0\n        inequality constraints with only lower bounds:        0\n   inequality constraints with lower and upper bounds:        0\n        inequality constraints with only upper bounds:        0\n\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n   0  1.3113368e+05 0.00e+00 1.00e+02   0.0 0.00e+00    -  0.00e+00 0.00e+00   0 \n   5  8.2237394e+04 0.00e+00 6.04e+02 -11.0 2.53e+00    -  1.00e+00 1.00e+00f  1 MaxS\n  10  1.2380115e+05 0.00e+00 1.03e+03 -11.0 6.31e+01  -5.4 1.00e+00 1.00e+00h  1 MaxS\n  15  1.4133320e+05 0.00e+00 1.99e+02 -11.0 4.54e+02  -7.8 1.00e+00 1.00e+00h  1 MaxS\n\n\n\nBase.LinAlg.PosDefException(25)\n\n\n\n in chkposdef at ./linalg/lapack.jl:44 [inlined]\n\n in sygvd!(::Int64, ::Char, ::Char, ::Array{Float64,2}, ::Array{Float64,2}) at ./linalg/lapack.jl:4908\n\n in eigfact!(::Symmetric{Float64,Array{Float64,2}}, ::Symmetric{Float64,Array{Float64,2}}) at ./linalg/symmetric.jl:224\n\n in eigfact(::Symmetric{Float64,Array{Float64,2}}, ::Symmetric{Float64,Array{Float64,2}}) at ./linalg/eigen.jl:274\n\n in VarianceComponentModels.TwoVarCompModelRotate{T :AbstractFloat,BT :Union{AbstractArray{T,1},AbstractArray{T,2}}}(::VarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}) at /Users/huazhou/.julia/v0.5/VarianceComponentModels/src/VarianceComponentModels.jl:121\n\n in eval_f(::VarianceComponentModels.TwoVarCompOptProb{VarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}},VarianceComponentModels.TwoVarCompVariateRotate{Float64,Array{Float64,2},Array{Float64,2}},Array{Float64,2},Array{Float64,1},VarianceComponentModels.VarianceComponentAuxData{Array{Float64,2},Array{Float64,1}}}, ::Array{Float64,1}) at /Users/huazhou/.julia/v0.5/VarianceComponentModels/src/two_variance_component.jl:683\n\n in (::Ipopt.#eval_f_cb#4{VarianceComponentModels.TwoVarCompOptProb{VarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}},VarianceComponentModels.TwoVarCompVariateRotate{Float64,Array{Float64,2},Array{Float64,2}},Array{Float64,2},Array{Float64,1},VarianceComponentModels.VarianceComponentAuxData{Array{Float64,2},Array{Float64,1}}}})(::Array{Float64,1}) at /Users/huazhou/.julia/v0.5/Ipopt/src/IpoptSolverInterface.jl:53\n\n in eval_f_wrapper(::Int32, ::Ptr{Float64}, ::Int32, ::Ptr{Float64}, ::Ptr{Void}) at /Users/huazhou/.julia/v0.5/Ipopt/src/Ipopt.jl:89\n\n in solveProblem(::Ipopt.IpoptProblem) at /Users/huazhou/.julia/v0.5/Ipopt/src/Ipopt.jl:304\n\n in optimize!(::Ipopt.IpoptMathProgModel) at /Users/huazhou/.julia/v0.5/Ipopt/src/IpoptSolverInterface.jl:120\n\n in #mle_fs!#29(::Int64, ::Symbol, ::Symbol, ::Bool, ::Function, ::VarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}, ::VarianceComponentModels.TwoVarCompVariateRotate{Float64,Array{Float64,2},Array{Float64,2}}) at /Users/huazhou/.julia/v0.5/VarianceComponentModels/src/two_variance_component.jl:893\n\n in (::VarianceComponentModels.#kw##mle_fs!)(::Array{Any,1}, ::VarianceComponentModels.#mle_fs!, ::VarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}, ::VarianceComponentModels.TwoVarCompVariateRotate{Float64,Array{Float64,2},Array{Float64,2}}) at ./ missing :0  From the output we can see the Fisher scoring algorithm ran into some numerical issues. Let's try the  MM algorithm .  # reset model parameters  traitall_model   =   VarianceComponentModel ( cg10kdata_rotated )  # estimate variance components using Fisher scoring algorithm  @time   mle_mm! ( traitall_model ,   cg10kdata_rotated ;   verbose = true )        MM Algorithm\n  Iter      Objective  \n--------  -------------\n       0  -1.311337e+05\n       1  -8.002195e+04\n       2  -5.807051e+04\n       3  -4.926234e+04\n       4  -4.611182e+04\n       5  -4.511727e+04\n       6  -4.482798e+04\n       7  -4.474410e+04\n       8  -4.471610e+04\n       9  -4.470285e+04\n      10  -4.469355e+04\n      20  -4.462331e+04\n      30  -4.456960e+04\n      40  -4.452834e+04\n      50  -4.449652e+04\n      60  -4.447178e+04\n      70  -4.445237e+04\n      80  -4.443699e+04\n      90  -4.442467e+04\n     100  -4.441470e+04\n     110  -4.440656e+04\n     120  -4.439985e+04\n     130  -4.439427e+04\n     140  -4.438959e+04\n     150  -4.438564e+04\n     160  -4.438229e+04\n     170  -4.437941e+04\n     180  -4.437694e+04\n     190  -4.437480e+04\n     200  -4.437294e+04\n     210  -4.437131e+04\n     220  -4.436989e+04\n     230  -4.436863e+04\n     240  -4.436751e+04\n     250  -4.436652e+04\n     260  -4.436564e+04\n     270  -4.436485e+04\n     280  -4.436414e+04\n     290  -4.436351e+04\n     300  -4.436293e+04\n     310  -4.436242e+04\n     320  -4.436195e+04\n     330  -4.436152e+04\n     340  -4.436113e+04\n     350  -4.436078e+04\n     360  -4.436046e+04\n     370  -4.436016e+04\n     380  -4.435989e+04\n     390  -4.435965e+04\n     400  -4.435942e+04\n     410  -4.435921e+04\n     420  -4.435902e+04\n     430  -4.435884e+04\n     440  -4.435867e+04\n     450  -4.435852e+04\n     460  -4.435838e+04\n     470  -4.435825e+04\n     480  -4.435813e+04\n     490  -4.435802e+04\n     500  -4.435791e+04\n     510  -4.435781e+04\n     520  -4.435772e+04\n     530  -4.435764e+04\n     540  -4.435756e+04\n     550  -4.435748e+04\n     560  -4.435741e+04\n     570  -4.435735e+04\n     580  -4.435729e+04\n     590  -4.435723e+04\n     600  -4.435718e+04\n     610  -4.435713e+04\n     620  -4.435708e+04\n     630  -4.435704e+04\n     640  -4.435700e+04\n     650  -4.435696e+04\n     660  -4.435692e+04\n     670  -4.435688e+04\n     680  -4.435685e+04\n     690  -4.435682e+04\n     700  -4.435679e+04\n     710  -4.435676e+04\n     720  -4.435674e+04\n     730  -4.435671e+04\n     740  -4.435669e+04\n     750  -4.435667e+04\n     760  -4.435665e+04\n     770  -4.435663e+04\n     780  -4.435661e+04\n     790  -4.435659e+04\n     800  -4.435657e+04\n     810  -4.435656e+04\n     820  -4.435654e+04\n     830  -4.435653e+04\n     840  -4.435651e+04\n     850  -4.435650e+04\n     860  -4.435649e+04\n     870  -4.435648e+04\n     880  -4.435647e+04\n     890  -4.435646e+04\n     900  -4.435645e+04\n     910  -4.435644e+04\n     920  -4.435643e+04\n     930  -4.435642e+04\n     940  -4.435641e+04\n     950  -4.435640e+04\n     960  -4.435639e+04\n     970  -4.435639e+04\n     980  -4.435638e+04\n     990  -4.435637e+04\n    1000  -4.435637e+04\n    1010  -4.435636e+04\n    1020  -4.435635e+04\n    1030  -4.435635e+04\n    1040  -4.435634e+04\n    1050  -4.435634e+04\n    1060  -4.435633e+04\n    1070  -4.435633e+04\n    1080  -4.435632e+04\n\n  3.696088 seconds (156.73 k allocations: 69.792 MB, 0.32% gc time)\n\n\n\n\n\n(-44356.32043185692,VarianceComponentModels.VarianceComponentModel{Float64,2,Array{Float64,2},Array{Float64,2}}(,(\n[0.273498 0.192141 \u2026 -0.128643 -0.098307; 0.192141 0.219573 \u2026 -0.0687355 -0.0433724; \u2026 ; -0.128643 -0.0687355 \u2026 0.117185 0.0899824; -0.098307 -0.0433724 \u2026 0.0899824 0.106354],\n\n[0.723441 0.568135 \u2026 -0.0586259 -0.12469; 0.568135 0.77999 \u2026 0.0236098 0.0464835; \u2026 ; -0.0586259 0.0236098 \u2026 0.881707 0.551818; -0.12469 0.0464835 \u2026 0.551818 0.893023]),,Char[],Float64[],-Inf,Inf),(\n[0.0111652 0.0131052 \u2026 0.012894 0.0127596; 0.0131077 0.0151572 \u2026 0.0171615 0.0171431; \u2026 ; 0.0128941 0.0171614 \u2026 0.0174141 0.0182039; 0.0127597 0.0171428 \u2026 0.0182038 0.0187958],\n\n[0.011228 0.0133093 \u2026 0.0130109 0.012784; 0.0133113 0.0158091 \u2026 0.0178685 0.0177999; \u2026 ; 0.0130107 0.0178687 \u2026 0.017957 0.0187655; 0.0127836 0.0177997 \u2026 0.0187654 0.0193582]),\n[0.000124662 7.24629e-5 \u2026 -3.61414e-7 -1.40373e-5; 7.24483e-5 0.000171812 \u2026 -2.03586e-5 -3.26457e-6; \u2026 ; -3.87301e-7 -2.03778e-5 \u2026 0.000352145 -1.48419e-5; -1.40438e-5 -3.27339e-6 \u2026 -1.48381e-5 0.000374741],\n\n,)  It converges after ~1000 iterations.", 
            "title": "13-trait joint analysis"
        }, 
        {
            "location": "/man/heritability/#save-analysis-results", 
            "text": "#using JLD  #@save  copd.jld  #whos()", 
            "title": "Save analysis results"
        }, 
        {
            "location": "/man/api/", 
            "text": "API\n\n\nDocumentation for \nVarianceComponentModels.jl\n's types and methods.\n\n\n\n\nIndex\n\n\n\n\nVarianceComponentModels.TwoVarCompModelRotate\n\n\nVarianceComponentModels.TwoVarCompVariateRotate\n\n\nVarianceComponentModels.VarianceComponentModel\n\n\nVarianceComponentModels.VarianceComponentVariate\n\n\nVarianceComponentModels.fit_mle!\n\n\nVarianceComponentModels.fit_reml!\n\n\nVarianceComponentModels.mle_fs!\n\n\nVarianceComponentModels.mle_mm!\n\n\n\n\n\n\nTypes\n\n\n#\n\n\nVarianceComponentModels.VarianceComponentModel\n \n \nType\n.\n\n\nVarianceComponentModel\n stores the model parameters of a variance component model.\n\n\nFields\n\n\n\n\nB\n: \np x d\n mean parameters\n\n\n\u03a3\n: tuple of \nd x d\n variance component parameters\n\n\nA\n: constraint matrix for \nvec(B)\n\n\nsense\n: vector of characters \n'='\n, \n'\n'\n or \n'\n'\n\n\nb\n: constraint vector for \nvec(B)\n\n\nlb\n: lower bounds for \nvec(B)\n\n\nub\n: upper bounds for \nvec(B)\n\n\n\n\nsource\n\n\n#\n\n\nVarianceComponentModels.VarianceComponentVariate\n \n \nType\n.\n\n\nVarianceComponentVariate\n stores the data of a variance component model.\n\n\nFeilds\n\n\n\n\nY\n: \nn x d\n responses\n\n\nX\n: \nn x p\n predictors\n\n\nV\n: tuple of \nn x n\n covariance matrices\n\n\n\n\nsource\n\n\n#\n\n\nVarianceComponentModels.TwoVarCompModelRotate\n \n \nType\n.\n\n\nTwoVarCompModelRotate\n stores the rotated two variance component model.\n\n\nFields\n\n\n\n\nBrot\n: rotated mean parameters \nB * eigvec\n\n\neigval\n: eigenvalues of \neig(\u03a3[1], \u03a3[2])\n\n\neigvec\n: eigenvectors of \neig(\u03a3[1], \u03a3[2])\n\n\nlogdet\u03a32\n: log-determinant of \n\u03a3[2]\n\n\n\n\nsource\n\n\n#\n\n\nVarianceComponentModels.TwoVarCompVariateRotate\n \n \nType\n.\n\n\nTwoVarCompVariateRotate\n stores the rotated two variance component data.\n\n\nFields\n\n\n\n\nYrot\n: rotated responses \neigvec * Y\n\n\nXrot\n: rotated covariates \neigvec * X\n\n\neigval\n: eigenvalues of \neig(V[1], V[2])\n\n\neigvec\n: eigenvectors of \neig(V[1], V[2])\n\n\nlogdetV2\n: log-determinant of \nV[2]\n\n\n\n\nsource\n\n\n\n\nFunctions\n\n\n#\n\n\nVarianceComponentModels.mle_fs!\n \n \nFunction\n.\n\n\nmle_fs!(vcmodel, vcdatarot; maxiter, solver, qpsolver, verbose)\n\n\n\n\n\nFind MLE by Fisher scoring algorithm.\n\n\nInput\n\n\n\n\nvcmodel\n: two variane component model \nVarianceComponentModel\n, with\n\n\n\n\nvcmodel.B\n and \nvcmodel.\u03a3\n used as starting point\n\n\n\n\nvcdatarot\n: rotated two varianec component data \nTwoVarCompVariateRotate\n\n\n\n\nKeyword\n\n\n\n\nmaxiter::Int\n: maximum number of iterations, default is 1000\n\n\nsolver::Symbol\n: backend nonlinear programming solver, \n:Ipopt\n (default) or \n:Knitro\n\n\nqpsolver::Symbol\n: backend quadratic programming solver, \n:Ipopt\n (default) or \n:Gurobi\n or \nMosek\n\n\nverbose::Bool\n: display information\n\n\n\n\nOutput\n\n\n\n\nmaxlogl\n: log-likelihood at solution\n\n\nvcmodel\n: \nVarianceComponentModel\n with updated model parameters\n\n\n\u03a3se=(\u03a3se[1],\u03a3se[2])\n: standard errors of estimate \n\u03a3=(\u03a3[1],\u03a3[2])\n\n\n\u03a3cov\n: covariance matrix of estimate \n\u03a3=(\u03a3[1],\u03a3[2])\n\n\nBse\n: standard errors of estimate \nB\n\n\nBcov\n: covariance of estimate \nB\n\n\n\n\nsource\n\n\n#\n\n\nVarianceComponentModels.mle_mm!\n \n \nFunction\n.\n\n\nmle_mm!(vcmodel, vcdatarot; maxiter, qpsolver, verbose)\n\n\n\n\n\nFind MLE by minorization-maximization (MM) algorithm.\n\n\nInput\n\n\n\n\nvcmodel\n: two variane component model \nVarianceComponentModel\n, with\n\n\n\n\nvcmodel.B\n and \nvcmodel.\u03a3\n used as starting point\n\n\n\n\nvcdatarot\n: rotated two varianec component data \nTwoVarCompVariateRotate\n\n\n\n\nKeyword\n\n\n\n\nmaxiter::Int\n: maximum number of iterations, default is 1000\n\n\nqpsolver::Symbol\n: backend quadratic programming solver, \n:Ipopt\n (default) or \n:Gurobi\n or \nMosek\n\n\nverbose::Bool\n: display information\n\n\n\n\nOutput\n\n\n\n\nmaxlogl\n: log-likelihood at solution\n\n\nvcmodel\n: \nVarianceComponentModel\n with updated model parameters\n\n\n\u03a3se=(\u03a3se[1],\u03a3se[2])\n: standard errors of estimate \n\u03a3=(\u03a3[1],\u03a3[2])\n\n\n\u03a3cov\n: covariance matrix of estimate \n\u03a3=(\u03a3[1],\u03a3[2])\n\n\nBse\n: standard errors of estimate \nB\n\n\nBcov\n: covariance of estimate \nB\n\n\n\n\nReference\n\n\n\n\nH. Zhou, L. Hu, J. Zhou, and K. Lange (2015) MM algorithms for variance components models. \nhttp://arxiv.org/abs/1509.07426\n\n\n\n\nsource\n\n\n#\n\n\nVarianceComponentModels.fit_mle!\n \n \nFunction\n.\n\n\nfit_mle!(vcmodel, vcdata; algo)\n\n\n\n\n\nFind MLE of variane component model.\n\n\nInput\n\n\n\n\nvcmodel\n: two variane component model \nVarianceComponentModel\n, with\n\n\n\n\nvcmodel.B\n and \nvcmodel.\u03a3\n used as starting point\n\n\n\n\nvcdata\n: two varianec component data \nVarianceComponentVariate\n\n\n\n\nKeyword\n\n\n\n\nalgo::Symbol\n: algorithm, \n:FS\n (Fisher scoring) for \n:MM\n\n\n\n\n(minorization-maximization algorithm)\n\n\nOutput\n\n\n\n\nmaxlogl\n: log-likelihood at solution\n\n\nvcmodel\n: \nVarianceComponentModel\n with updated model parameters\n\n\n\u03a3se=(\u03a3se[1],\u03a3se[2])\n: standard errors of estimate \n\u03a3=(\u03a3[1],\u03a3[2])\n\n\n\u03a3cov\n: covariance matrix of estimate \n\u03a3=(\u03a3[1],\u03a3[2])\n\n\nBse\n: standard errors of estimate \nB\n\n\nBcov\n: covariance of estimate \nB\n\n\n\n\nsource\n\n\n#\n\n\nVarianceComponentModels.fit_reml!\n \n \nFunction\n.\n\n\nfit_reml!(vcmodel, vcdata; algo)\n\n\n\n\n\nFind restricted MLE (REML) of variane component model.\n\n\nInput\n\n\n\n\nvcmodel\n: two variane component model \nVarianceComponentModel\n, with\n\n\n\n\nvcmodel.B\n and \nvcmodel.\u03a3\n used as starting point\n\n\n\n\nvcdata\n: two varianec component data \nVarianceComponentVariate\n\n\n\n\nKeyword\n\n\n\n\nalgo::Symbol\n: algorithm, \n:FS\n (Fisher scoring) for \n:MM\n\n\n\n\n(minorization-maximization algorithm)\n\n\nOutput\n\n\n\n\nmaxlogl\n: log-likelihood at solution\n\n\nvcmodel\n: \nVarianceComponentModel\n with updated model parameters\n\n\n\u03a3se=(\u03a3se[1],\u03a3se[2])\n: standard errors of estimate \n\u03a3=(\u03a3[1],\u03a3[2])\n\n\n\u03a3cov\n: covariance matrix of estimate \n\u03a3=(\u03a3[1],\u03a3[2])\n\n\nBse\n: standard errors of estimate \nB\n\n\nBcov\n: covariance of estimate \nB\n\n\n\n\nsource", 
            "title": "API"
        }, 
        {
            "location": "/man/api/#api", 
            "text": "Documentation for  VarianceComponentModels.jl 's types and methods.", 
            "title": "API"
        }, 
        {
            "location": "/man/api/#index", 
            "text": "VarianceComponentModels.TwoVarCompModelRotate  VarianceComponentModels.TwoVarCompVariateRotate  VarianceComponentModels.VarianceComponentModel  VarianceComponentModels.VarianceComponentVariate  VarianceComponentModels.fit_mle!  VarianceComponentModels.fit_reml!  VarianceComponentModels.mle_fs!  VarianceComponentModels.mle_mm!", 
            "title": "Index"
        }, 
        {
            "location": "/man/api/#types", 
            "text": "#  VarianceComponentModels.VarianceComponentModel     Type .  VarianceComponentModel  stores the model parameters of a variance component model.  Fields   B :  p x d  mean parameters  \u03a3 : tuple of  d x d  variance component parameters  A : constraint matrix for  vec(B)  sense : vector of characters  '=' ,  ' '  or  ' '  b : constraint vector for  vec(B)  lb : lower bounds for  vec(B)  ub : upper bounds for  vec(B)   source  #  VarianceComponentModels.VarianceComponentVariate     Type .  VarianceComponentVariate  stores the data of a variance component model.  Feilds   Y :  n x d  responses  X :  n x p  predictors  V : tuple of  n x n  covariance matrices   source  #  VarianceComponentModels.TwoVarCompModelRotate     Type .  TwoVarCompModelRotate  stores the rotated two variance component model.  Fields   Brot : rotated mean parameters  B * eigvec  eigval : eigenvalues of  eig(\u03a3[1], \u03a3[2])  eigvec : eigenvectors of  eig(\u03a3[1], \u03a3[2])  logdet\u03a32 : log-determinant of  \u03a3[2]   source  #  VarianceComponentModels.TwoVarCompVariateRotate     Type .  TwoVarCompVariateRotate  stores the rotated two variance component data.  Fields   Yrot : rotated responses  eigvec * Y  Xrot : rotated covariates  eigvec * X  eigval : eigenvalues of  eig(V[1], V[2])  eigvec : eigenvectors of  eig(V[1], V[2])  logdetV2 : log-determinant of  V[2]   source", 
            "title": "Types"
        }, 
        {
            "location": "/man/api/#functions", 
            "text": "#  VarianceComponentModels.mle_fs!     Function .  mle_fs!(vcmodel, vcdatarot; maxiter, solver, qpsolver, verbose)  Find MLE by Fisher scoring algorithm.  Input   vcmodel : two variane component model  VarianceComponentModel , with   vcmodel.B  and  vcmodel.\u03a3  used as starting point   vcdatarot : rotated two varianec component data  TwoVarCompVariateRotate   Keyword   maxiter::Int : maximum number of iterations, default is 1000  solver::Symbol : backend nonlinear programming solver,  :Ipopt  (default) or  :Knitro  qpsolver::Symbol : backend quadratic programming solver,  :Ipopt  (default) or  :Gurobi  or  Mosek  verbose::Bool : display information   Output   maxlogl : log-likelihood at solution  vcmodel :  VarianceComponentModel  with updated model parameters  \u03a3se=(\u03a3se[1],\u03a3se[2]) : standard errors of estimate  \u03a3=(\u03a3[1],\u03a3[2])  \u03a3cov : covariance matrix of estimate  \u03a3=(\u03a3[1],\u03a3[2])  Bse : standard errors of estimate  B  Bcov : covariance of estimate  B   source  #  VarianceComponentModels.mle_mm!     Function .  mle_mm!(vcmodel, vcdatarot; maxiter, qpsolver, verbose)  Find MLE by minorization-maximization (MM) algorithm.  Input   vcmodel : two variane component model  VarianceComponentModel , with   vcmodel.B  and  vcmodel.\u03a3  used as starting point   vcdatarot : rotated two varianec component data  TwoVarCompVariateRotate   Keyword   maxiter::Int : maximum number of iterations, default is 1000  qpsolver::Symbol : backend quadratic programming solver,  :Ipopt  (default) or  :Gurobi  or  Mosek  verbose::Bool : display information   Output   maxlogl : log-likelihood at solution  vcmodel :  VarianceComponentModel  with updated model parameters  \u03a3se=(\u03a3se[1],\u03a3se[2]) : standard errors of estimate  \u03a3=(\u03a3[1],\u03a3[2])  \u03a3cov : covariance matrix of estimate  \u03a3=(\u03a3[1],\u03a3[2])  Bse : standard errors of estimate  B  Bcov : covariance of estimate  B   Reference   H. Zhou, L. Hu, J. Zhou, and K. Lange (2015) MM algorithms for variance components models.  http://arxiv.org/abs/1509.07426   source  #  VarianceComponentModels.fit_mle!     Function .  fit_mle!(vcmodel, vcdata; algo)  Find MLE of variane component model.  Input   vcmodel : two variane component model  VarianceComponentModel , with   vcmodel.B  and  vcmodel.\u03a3  used as starting point   vcdata : two varianec component data  VarianceComponentVariate   Keyword   algo::Symbol : algorithm,  :FS  (Fisher scoring) for  :MM   (minorization-maximization algorithm)  Output   maxlogl : log-likelihood at solution  vcmodel :  VarianceComponentModel  with updated model parameters  \u03a3se=(\u03a3se[1],\u03a3se[2]) : standard errors of estimate  \u03a3=(\u03a3[1],\u03a3[2])  \u03a3cov : covariance matrix of estimate  \u03a3=(\u03a3[1],\u03a3[2])  Bse : standard errors of estimate  B  Bcov : covariance of estimate  B   source  #  VarianceComponentModels.fit_reml!     Function .  fit_reml!(vcmodel, vcdata; algo)  Find restricted MLE (REML) of variane component model.  Input   vcmodel : two variane component model  VarianceComponentModel , with   vcmodel.B  and  vcmodel.\u03a3  used as starting point   vcdata : two varianec component data  VarianceComponentVariate   Keyword   algo::Symbol : algorithm,  :FS  (Fisher scoring) for  :MM   (minorization-maximization algorithm)  Output   maxlogl : log-likelihood at solution  vcmodel :  VarianceComponentModel  with updated model parameters  \u03a3se=(\u03a3se[1],\u03a3se[2]) : standard errors of estimate  \u03a3=(\u03a3[1],\u03a3[2])  \u03a3cov : covariance matrix of estimate  \u03a3=(\u03a3[1],\u03a3[2])  Bse : standard errors of estimate  B  Bcov : covariance of estimate  B   source", 
            "title": "Functions"
        }
    ]
}